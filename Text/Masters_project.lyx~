#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[a4paper]{geometry}
\end_preamble
\use_default_options true
\begin_modules
theorems-std
theorems-sec
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Decision Theory
\end_layout

\begin_layout Author
Marko Sahan
\end_layout

\begin_layout Section
Introduction to Decision Theory
\end_layout

\begin_layout Standard
Decision process is complicated set of actions that living being makes for
 satisfying its needs.
 We want to apply same concept for inanimate thing such as computers.
 Before constructing some theory we must define some terms with which we
 will work.
 
\end_layout

\begin_layout Standard
Assuming that we want to solve classification problem.
 For simplicity we will work with binary classification problem.
 We want to find such solution that will assign for each input value its
 class.
 Moreover, we want to make classification error as small as possible.
 Considering that input data 
\begin_inset Formula $\mathbf{x}\in{\bf X}\subset\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\bf Y}$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is an input vectors of size 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula ${\bf y}$
\end_inset

 is its labels assigned to the data from 
\begin_inset Formula ${\bf X}$
\end_inset

.
 Each value from 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 is one or zero 
\begin_inset Formula $\mathbf{y}\in\{0,1\}$
\end_inset

 that represents first or second class.
\end_layout

\begin_layout Standard
In order to make a classification with respect to some input data we must
 provide an action or in other words decision.
 Let 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

 is an action and 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is an action space.
 Of-course we do not want to make random decisions.
 We want to make decisions with respect to some metrics that can tell us
 how good our decision is.
 Thus, we will introduce a loss function 
\begin_inset Formula $L$
\end_inset

.
 From the previous text it is obvious that loss function will be dependent
 on action 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

.
 Furthermore, from the definition of the loss function it must be also dependent
 on a parameter.
 Let 
\begin_inset Formula $\theta\in\varTheta$
\end_inset

 is parameters' vector and 
\begin_inset Formula $\varTheta$
\end_inset

 is parameters space.
 As a result loss function 
\begin_inset Formula $L$
\end_inset

 can be represented as 
\begin_inset Formula 
\begin{equation}
L=L(\theta,a).\label{eq:loss_func}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We will continue construction of the decision theory on the example of Support
 Vector Machine (SVM) method.
 For simplicity lets consider linearly separable dataset.
 From the theoretical perspective SVM constructs hyperplane in high dimensional
 space that separates two classes.
 In this case our decision is a hyperplane that will separate two classes
 from each other.
 Equation of the hyperplane can be written as 
\begin_inset Formula $f(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

 where 
\begin_inset Formula $\text{\ensuremath{\mathbf{w}}\ensuremath{\in\mathbb{R}^{n}} }$
\end_inset

 is a set of hyperplane parameters and 
\begin_inset Formula $b\in\mathbb{R}$
\end_inset

 is a bias .
 As a result, action space is represented as 
\begin_inset Formula $(\mathbb{R}^{n},\mathbb{R})\mathcal{=A}$
\end_inset

 and as a consequence tuple 
\begin_inset Formula $(\ensuremath{\mathbf{w}},b)\in\mathcal{A}$
\end_inset

.
 From this knowledge we can define 
\begin_inset Formula $\mathbf{X}$
\end_inset

 as a parameter space and 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 is the set of possible outcomes (
\shape slanted
sample space
\shape default
) where 
\begin_inset Formula $\mathbf{y}=\mathbf{y}(\mathbf{x})$
\end_inset

.
 Considering updated definitions loss function (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

) can be rewritten as
\begin_inset Formula 
\begin{equation}
L=L(\mathbf{x},\mathbf{w},b).\label{eq:SVM_thoer_loss}
\end{equation}

\end_inset

Following task is to understand how good is out action (hyperplane estimation)
 with respect to the dataset.
 We can choose different types of the loss functions such as cross entropy
 or hinge loss, etc.
 The most basic approach for SVM method is hinge loss function which is
 defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{w},b)=\max(0,1-{\bf y}(\mathbf{x})\hat{\mathbf{y}}(\mathbf{x},\mathbf{w},b))\label{eq:hinge_loss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{\mathbf{y}}(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

.
 
\end_layout

\begin_layout Standard
If we had available data 
\begin_inset Formula ${\bf X}$
\end_inset

 and it its labels 
\begin_inset Formula ${\bf Y}$
\end_inset

 we would not have to construct all this theory because all labels would
 be known and no classification problem must be solved.
 However, in most cases we would have little discrete subset 
\begin_inset Formula $\tilde{{\bf X}}\subset{\bf X}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}\subset{\bf Y}$
\end_inset

.
 On the basis of 
\begin_inset Formula $\mathbf{\tilde{X}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{\tilde{Y}}$
\end_inset

 we want to come up with a decision that will help us to label unlabeled
 data.
 In terms of SVM method we want to find such hyperplane that will label
 input values as a first class if it is above the hyperplane and as a second
 class if it is below the hyperplane.
 At this point very important assumption will be introduced.
 In order to find an optimal hyperplane we assume that the data 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and its labels 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 fully describe dataset 
\begin_inset Formula ${\bf X}$
\end_inset

 and 
\begin_inset Formula ${\bf Y}$
\end_inset

.
 Moreover we want to consider 
\begin_inset Formula $\mathbf{x}\in\mathbf{X}$
\end_inset

 as a random variable with probability distribution 
\begin_inset Formula $p({\bf x})$
\end_inset

.
 We will also assume that 
\begin_inset Formula $\forall i\in\{1,..,N\},\ \mathbf{x}_{i}\in\mathbf{\tilde{X}}$
\end_inset

 is independent identically distributed.
 With the usage of the data 
\begin_inset Formula $\mathbf{\tilde{X}}$
\end_inset

, probability distribution 
\begin_inset Formula $p({\bf x})$
\end_inset

 can be approximated as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x})=\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i})\label{eq:approx_of_joint_dist}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta(\mathbf{x}-\mathbf{x}_{i})$
\end_inset

 is Dirac delta function which is centered in 
\begin_inset Formula $\text{x}_{i}$
\end_inset

.
\end_layout

\begin_layout Definition
If 
\begin_inset Formula $\pi^{*}(\theta)$
\end_inset

 is believed probability distribution of 
\begin_inset Formula $\theta$
\end_inset

 at the time of decision making, the 
\shape slanted
Bayesian expected loss
\shape default
 of an action 
\begin_inset Formula $a$
\end_inset

 is 
\begin_inset Formula 
\begin{align}
\rho(\pi^{*},a)= & \mathbb{E}_{\pi^{*}}[L(\theta,a)],\label{eq:expected_loss}\\
= & \int_{\varTheta}L(\theta,a)dF^{\pi^{*}}(\theta)
\end{align}

\end_inset


\end_layout

\begin_layout Definition
Using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we can evaluate expected loss function for SVM as follows 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{\mathbf{X}}L(\mathbf{x},\mathbf{w},b)p(\mathbf{x})d(\mathbf{x}),\\
 & =\int_{\mathbf{X}}\max(0,1-{\bf y}(\mathbf{x})\hat{\mathbf{y}}(\mathbf{x},\mathbf{w},b))\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i})d(\mathbf{x}),\\
 & =\frac{1}{N}\sum_{i=1}^{N}\max(0,1-{\bf y}(\mathbf{x}_{i})\hat{\mathbf{y}}(\mathbf{x}_{i},\mathbf{w},b))
\end{align*}

\end_inset

where 
\begin_inset Formula ${\bf y}(\mathbf{x}_{i})=\mathbf{\tilde{y}}_{i}$
\end_inset

 and 
\begin_inset Formula $\hat{{\bf y}}(\mathbf{x}_{i})=\mathbf{\hat{\mathbf{y}}}_{i}.$
\end_inset

 Expect loss function for SVM can be written as 
\begin_inset Formula 
\begin{align}
\rho(\mathbf{x}_{i},\mathbf{w},b) & =\mathbb{E}_{\pi^{*}}L\nonumber \\
 & =\frac{1}{N}\sum_{i=1}^{N}\max(0,1-{\bf y}_{i}\mathbf{w}^{T}\mathbf{x}_{i}+b)\label{eq:expected_loss_svm}
\end{align}

\end_inset


\end_layout

\end_body
\end_document
