#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[a4paper]{geometry}
\end_preamble
\use_default_options true
\begin_modules
theorems-std
theorems-sec
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Decision Theory
\end_layout

\begin_layout Author
Marko Sahan
\end_layout

\begin_layout Part
Introduction
\end_layout

\begin_layout Standard
Decision process is complicated set of actions that living being makes for
 satisfying its needs.
 We want to apply same concept for inanimate thing such as computers.
 Before constructing some theory we must define some terms with which we
 will work.
 
\end_layout

\begin_layout Standard
Lets assume that we want to solve classification problem.
 For simplicity we will work with binary classification problem.
 We want to find such solution that will assign for each input value its
 class.
 Moreover, we want to make classification error as small as possible.
 Considering that exists data 
\begin_inset Formula $\mathbf{x}\in{\cal X}\subset\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 are vectors of size 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula ${\bf y}$
\end_inset

 are its labels assigned to the data from space 
\begin_inset Formula ${\cal X}$
\end_inset

.
 In this work we will use several definitions of the values from space 
\begin_inset Formula ${\cal Y}$
\end_inset

.
 Each value from space 
\begin_inset Formula ${\cal Y}$
\end_inset

 can be represented as a one hot representation that consist from ones and
 zeros 
\begin_inset Formula $\mathbf{y}\in\{(0,1)^{T},(1,0)^{T}\}$
\end_inset

 or as a scalars 
\begin_inset Formula $y\in\{0,1\}$
\end_inset

 (bold 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is a vector that is why if we talk about a scalar we have to redefine 
\begin_inset Formula $\mathbf{y}$
\end_inset

 as 
\begin_inset Formula $y$
\end_inset

).
 These representations can be different with respect to the overviewed methods.
 If we 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is defined as a one hot representation them first class is represented
 as 
\begin_inset Formula $\mathbf{y}=(1,0)^{T}$
\end_inset

 and second class is represented as 
\begin_inset Formula $\mathbf{y}=(0,1)^{T}$
\end_inset

.
 If 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is represented as scalar, then first class is 
\begin_inset Formula $y=1\Leftrightarrow\mathbf{y}=(1,0)^{T}$
\end_inset

 and second class is 
\begin_inset Formula $y=0\Leftrightarrow\mathbf{y}=(0,1)^{T}$
\end_inset

.
\end_layout

\begin_layout Standard
In order to make a classification with respect to some data we must provide
 an action or in other words decision.
 Let 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

 is an action and 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is an action space.
 Of-course we do not want to make random decisions.
 We want to make decisions with respect to some metrics that can tell us
 how good our decision is.
 Thus, we will introduce a loss function 
\begin_inset Formula $L$
\end_inset

.
 From the previous text it is obvious that loss function will be dependent
 on action 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

.
 Furthermore, from the definition of the loss function it must be also dependent
 on a parameter.
 Let 
\begin_inset Formula $\theta\in\varTheta$
\end_inset

 is parameters' vector and 
\begin_inset Formula $\varTheta$
\end_inset

 is parameters space.
 As a result loss function 
\begin_inset Formula $L$
\end_inset

 can be represented as 
\begin_inset Formula 
\begin{equation}
L=L(\theta,a).\label{eq:loss_func}
\end{equation}

\end_inset

However its impossible to have all the data 
\begin_inset Formula $\theta\in\varTheta$
\end_inset

 and 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

.
 As a result we will introduce following definition.
\end_layout

\begin_layout Definition
\begin_inset CommandInset label
LatexCommand label
name "def:expected_loss"

\end_inset

If 
\begin_inset Formula $\pi^{*}(\theta)$
\end_inset

 is believed probability distribution of 
\begin_inset Formula $\theta$
\end_inset

 at the time of decision making, the 
\shape slanted
Bayesian expected loss
\shape default
 of an action 
\begin_inset Formula $a$
\end_inset

 is 
\begin_inset Formula 
\begin{align}
\rho(\pi^{*},a)= & \mathbb{E}_{\pi^{*}}[L(\theta,a)],\label{eq:expected_loss-1}\\
= & \int_{\varTheta}L(\theta,a)dF^{\pi^{*}}(\theta)
\end{align}

\end_inset


\end_layout

\begin_layout Section
Datasets
\begin_inset CommandInset label
LatexCommand label
name "subsec:Datasets"

\end_inset


\end_layout

\begin_layout Standard
We have already defined spaces 
\begin_inset Formula ${\cal X}$
\end_inset

 and 
\begin_inset Formula ${\cal Y}$
\end_inset

 above.
 However, we must define the data with which we are going to work.
 In this project we work with tuples of variables 
\begin_inset Formula $(\mathbf{x},\mathbf{y})\in{\cal X\times Y}$
\end_inset

.
 For example 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can be a text document and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 can be its label that will show if the document is relevant or not.
\end_layout

\begin_layout Standard
Considering the data 
\begin_inset Formula $\tilde{\mathbf{X}}$
\end_inset

 and its labels 
\begin_inset Formula $\tilde{\mathbf{Y}}$
\end_inset

 are from spaces 
\begin_inset Formula ${\cal X}$
\end_inset

 and 
\begin_inset Formula ${\cal Y}$
\end_inset

 respectively.
 These are the data that form a training set 
\series bold

\begin_inset Formula $(\tilde{\mathbf{X}}\times\mathbf{\tilde{Y}})$
\end_inset


\series default
.
 After we train our model we can continue training our model with the data
 from 
\begin_inset Formula $\mathbf{X}\in{\cal X}$
\end_inset

.
 However it is important to understand that we have no labels for the set
 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 We can ask for a help from an annotator that can give us those labels.
 We assume that getting labels needs some time and is very expensive.
 For example we can think that our model is not good enough and we want
 to ask a group of lawyers to classify if some more unlabeled documents
 are relevant or not.
 Data labeling will take both some time and will be quite costly.
 
\end_layout

\begin_layout Standard
In this work, set 
\begin_inset Formula $\mathbf{X}$
\end_inset

 will be used for the active learning section.
 We want to show that if we choose the documents for labeling from the set
 
\begin_inset Formula $\mathbf{X}$
\end_inset

 smartly our models will start to show better scores faster then if we choose
 the data for labeling randomly.
 
\end_layout

\begin_layout Standard
Basing on the definition of the datasets above we can assume that 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in{\cal Y}}$
\end_inset

 are random variables.
 If both 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 are random variables than tuples 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

 must be samples from a joint probability density function 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

.
 With the usage of conditional probability rule 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x},y)=p(y|\mathbf{x})p(\mathbf{x})
\end{equation}

\end_inset

where 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

 is given and 
\begin_inset Formula $p(y|\mathbf{x})$
\end_inset

 is a pdf that we want to estimate.
 
\end_layout

\begin_layout Section
Decision Theory for Supervised Learning
\end_layout

\begin_layout Subsection
Decision Theory and Support Vector Machine Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_svm"

\end_inset


\end_layout

\begin_layout Standard
In this subsection we will continue construction of the decision theory
 on the example of Support Vector Machine (SVM) method.
 For simplicity lets consider linearly separable dataset.
 From the theoretical perspective SVM constructs hyperplane in high dimensional
 space that separates two classes.
 In this case our decision is a hyperplane that will separate two classes
 from each other.
 Equation of the hyperplane can be written as 
\begin_inset Formula $f(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

 where 
\begin_inset Formula $\text{\ensuremath{\mathbf{w}}\ensuremath{\in\mathbb{R}^{n}} }$
\end_inset

 is a set of hyperplane parameters and 
\begin_inset Formula $b\in\mathbb{R}$
\end_inset

 is a bias .
 As a result, action space is represented as 
\begin_inset Formula $(\mathbb{R}^{n},\mathbb{R})\mathcal{=A}$
\end_inset

 and as a consequence tuple 
\begin_inset Formula $(\ensuremath{\mathbf{w}},b)\in\mathcal{A}$
\end_inset

.
 From this knowledge we can define 
\begin_inset Formula $\theta=(\mathbf{x},y)$
\end_inset

 where tuple 
\begin_inset Formula $(\mathbf{x},y)\in{\cal X}\times{\cal Y}$
\end_inset

 is parameters and 
\begin_inset Formula $\varTheta={\cal X}\times{\cal Y}$
\end_inset

 is a parameters' space.
 In this case we used 
\begin_inset Formula $y\in\{0,1\}$
\end_inset

.
 Considering updated definitions, loss function (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

) can be rewritten as
\begin_inset Formula 
\begin{equation}
L=L(\mathbf{x},y,\mathbf{w},b).\label{eq:SVM_thoer_loss}
\end{equation}

\end_inset

Following task is to understand how good is our action (hyperplane estimation)
 with respect to the dataset.
 We can choose different types of the loss functions such as cross entropy
 or hinge loss, etc.
 The most basic approach for SVM method is hinge loss function which is
 defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},y,\mathbf{w},b)=\max(0,1-y\hat{y}(\mathbf{x},\mathbf{w},b))\label{eq:hinge_loss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{y}(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

.
 
\end_layout

\begin_layout Standard
On the basis of given data 
\begin_inset Formula $\mathbf{\tilde{X}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{\tilde{Y}}$
\end_inset

 we want to come up with a decision that will help us to predict labels
 of the data 
\begin_inset Formula ${\cal X}$
\end_inset

.
 In terms of SVM method we want to find such hyperplane that will label
 input values as a first class if it is 
\begin_inset Quotes eld
\end_inset

above
\begin_inset Quotes erd
\end_inset

 the hyperplane and as a second class if it is 
\begin_inset Quotes eld
\end_inset

below
\begin_inset Quotes erd
\end_inset

 the hyperplane.
 At this point very important assumption will be introduced.
 In order to find an optimal hyperplane we assume that the data 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and its labels 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 fully describe spaces 
\begin_inset Formula ${\cal X}$
\end_inset

 and 
\begin_inset Formula ${\cal Y}$
\end_inset

.
 Moreover we consider 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $y\in{\cal Y}$
\end_inset

 are random variables with joint probability density function 
\begin_inset Formula $p({\bf x},y)$
\end_inset

.
 We will also assume that 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},y_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed.
 Probability density function 
\begin_inset Formula $p(\mathbf{x},y)$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x},y)=p(y|\mathbf{x})p(\mathbf{x}).\label{eq:joint_prob_dist}
\end{equation}

\end_inset

If we had those pdfs from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we could easily derive its joint pdf.
 However, we have only 
\begin_inset Formula $\mathbf{\tilde{X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{\mathbf{Y}}$
\end_inset

.
 As a result, with the usage of those data, probability density function
 
\begin_inset Formula $p({\bf x},y)$
\end_inset

 can be approximated as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x},y)=\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},y-y_{i})\label{eq:approx_of_joint_dist}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta(\mathbf{x}-\mathbf{x}_{i},y-y_{i})$
\end_inset

 is Dirac delta function which is centered in 
\begin_inset Formula $(\text{x}_{i},y_{i})$
\end_inset

.
\end_layout

\begin_layout Standard
Using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we can evaluate expected loss function for SVM as follows 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},y,\mathbf{w},b)p(\mathbf{x},y)d(\mathbf{x},y),\\
 & =\int_{{\cal X}\times{\cal Y}}\max(0,1-y\hat{y}(\mathbf{x},\mathbf{w},b))\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},y-y_{i})d(\mathbf{x},y),\\
 & =\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{i}\hat{y}(\mathbf{x}_{i},\mathbf{w},b))
\end{align*}

\end_inset

where 
\begin_inset Formula $\hat{y}(\mathbf{x}_{i},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}_{i}+b.$
\end_inset

 Expect loss function for SVM can be written as 
\begin_inset Formula 
\begin{equation}
\rho(\mathbf{x}_{i},\mathbf{w},b)=\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{i}\mathbf{w}^{T}\mathbf{x}_{i}+b).\label{eq:expected_loss_SVM}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Decision Theory and Algorithm Based on Neural Network Function 
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_nn"

\end_inset


\end_layout

\begin_layout Standard
Decision Theory construction for the algorithm, based on a neural network
 function, is mostly the same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 However in this case our decision is to finds estimate 
\begin_inset Formula $\hat{y}=\hat{y}(\mathbf{x},\mathbf{W},\mathbf{b})$
\end_inset

 of the probability density function 
\begin_inset Formula $p(y|\mathbf{x})$
\end_inset

 where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the input data, 
\series bold

\begin_inset Formula $\mathbf{y}=y\in\{0,1\}$
\end_inset


\series default
 is the label assigned to 
\begin_inset Formula $\mathbf{x}$
\end_inset

, 
\begin_inset Formula $\mathbf{W}$
\end_inset

 is a set of neural network function parameters and 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is a vector of biases.
 Once again our action space 
\begin_inset Formula $\mathcal{A}$
\end_inset

 will be parameters' and biases' space of 
\begin_inset Formula $\hat{y}$
\end_inset

.
 Same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can define 
\begin_inset Formula $(\mathbf{x},y)$
\end_inset

 are parameters of the loss function and 
\begin_inset Formula ${\cal X}\times{\cal Y}$
\end_inset

 is a parameters' space.
 For better understanding of the variety of loss functions we will use cross
 entropy loss function that is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},y,\mathbf{w},b)=-y\ln\big(\hat{y}(\mathbf{x},\mathbf{w},b)\big)-(1-y)\ln\big((1-\hat{y}(\mathbf{x},\mathbf{w},b)\big).\label{eq:cross_entropy_loss}
\end{equation}

\end_inset

Continuing using assumptions from 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Datasets"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we consider 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

 are random variables with joint probability density function 
\begin_inset Formula $p({\bf x},y)$
\end_inset

.
 With the usage of the given dataset where 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},y_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed we can approximate 
\begin_inset Formula $p(\mathbf{x},y)$
\end_inset

 as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Applying definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:expected_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

, expected loss for the algorithm based on a neural network function is
 evaluated as 
\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},y,\mathbf{w},b)p(\mathbf{x},y)d(\mathbf{x},y),\nonumber \\
 & =-\int_{\mathbf{{\cal X}}\times{\cal Y}}\Big(y\ln\big(\hat{y}(\mathbf{x},\mathbf{w},b)\big)+(1-y)\ln\big((1-\hat{y}(\mathbf{x},\mathbf{w},b)\big)\Big)\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},y-y_{i})d(\mathbf{x},y),\nonumber \\
 & =-\frac{1}{N}\sum_{i=1}^{N}\Big(y_{i}\ln\big(\hat{y}_{i}(\mathbf{x}_{i},\mathbf{w},b)\big)+(1-y_{i})\ln\big((1-\hat{y}_{i}(\mathbf{x}_{i},\mathbf{w},b)\big)\Big).\label{eq:expected_loss_cross_entropy}
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Decision Theory and Naive Bayes Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_nb"

\end_inset


\end_layout

\begin_layout Standard
Naive Bayes algorithm is a bit different to the algorithm based on Neural
 Networks and SVM.
 In the case of Naive Bayes we want to estimate 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 where 
\begin_inset Formula $\mathbf{W}\in{\cal W}$
\end_inset

 is set of parameters of a specific probability density function of 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

, 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the input data, 
\series bold

\begin_inset Formula $\mathbf{y}$
\end_inset

 
\series default
is the label in a one hot representation assigned to 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 The reason why we look for an estimate of the 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 but not 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{w})$
\end_inset

 is due to the fact that in the case of 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{W})$
\end_inset

 we would get an expression where normalization constant would be dependent
 on the set of parameters 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 That fact would make our computations very complicated.
 Before continuing with a loss function construction we would like to go
 trough Naive Bayes (NB) method.
 Assuming binary classification problem.
 With the usage of Bayes rule we can rewrite
\series bold
 
\series default

\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 as follows 
\begin_inset Formula 
\begin{equation}
p(\mathbf{W}|\mathbf{x},\mathbf{y})=\frac{p(\mathbf{y})p(\mathbf{x}|\mathbf{y},\mathbf{W})p(\mathbf{W})}{\int_{\mathbf{{\cal W}}}p(\mathbf{x},\mathbf{y}|\mathbf{W})p(\mathbf{W})d\mathbf{W}}\label{eq:bayes_rule}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\cal W}$
\end_inset

 is a space of 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Naive Bayes method introduces very strong assumption in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes_rule"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 This assumption says that features of vector 
\begin_inset Formula $\mathbf{x}=(x_{1},x_{2},...,x_{n})^{T}$
\end_inset

 are conditionally independent.
 As a result estimation of 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 can be approximated as
\begin_inset Formula 
\begin{equation}
\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y})=\frac{1}{Z}p(\mathbf{y})p(\mathbf{W})\prod_{i=1}^{n}\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big),\label{eq:naive_bayes_eq}
\end{equation}

\end_inset

where 
\series bold

\begin_inset Formula $\mathbf{y}=(y_{1},y_{2})$
\end_inset


\series default
 is a one-hot representation of 
\begin_inset Formula $\mathbf{y}$
\end_inset

, 
\begin_inset Formula $\mathbf{W}=(\mathbf{w}_{1},\mathbf{w}_{2},...,\mathbf{w}_{n})$
\end_inset

, 
\begin_inset Formula $Z$
\end_inset

 is a normalizing constant.
 We want to maximize probability 
\begin_inset Formula $\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

.
 As a result, using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:naive_bayes_eq"
plural "false"
caps "false"
noprefix "false"

\end_inset

, loss function 
\begin_inset Formula $L$
\end_inset

 will be represented as 
\begin_inset Formula 
\begin{align}
L(\mathbf{y},\mathbf{x},\mathbf{w}) & =-\log\big(\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y}),\label{eq:nb_loss}\\
 & =\log(Z)-\log\big(p(\mathbf{y})\big)-\log\big(p(\mathbf{W})\big)-\sum_{i=1}^{n}\log\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big).
\end{align}

\end_inset

 Same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_nn"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we will assume that we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 From this moment everything is ready for deriving expected loss function.
 Expected loss function for Naive Bayes method is derived as 
\series bold

\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi^{*}}L & =\int_{\mathbf{{\cal X}}\times\mathbf{{\cal Y}}}L(\mathbf{x},\mathbf{y},\mathbf{w})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & \int_{\mathbf{{\cal X}}\times\mathbf{{\cal Y}}}\Big(\xi(\mathbf{W},\mathbf{y})-\sum_{i=1}^{n}\log\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big)\frac{1}{N}\sum_{j=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{j},\mathbf{y}-\mathbf{y}_{j})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & \frac{1}{N}\sum_{j=1}^{N}\Big(\xi_{j}(\mathbf{W},\mathbf{y}_{j})-\sum_{i=1}^{n}\log\big(p(x_{i,j}|y_{1,j},\mathbf{w}_{i})^{y_{1,j}}p(x_{i,j}|y_{2,j},\mathbf{w}_{i})^{y_{2,j}}\big)\Big)\label{eq:expected_loss_mle_or_map}
\end{align}

\end_inset

where 
\begin_inset Formula $\xi(\mathbf{W},\text{y})=\log(Z)-\log\big(p(\mathbf{y})p(\mathbf{W})\big)$
\end_inset

 and 
\begin_inset Formula $\xi_{j}(\mathbf{W},\mathbf{y}_{j})=\log(Z)-\log\big(p(\mathbf{y}_{j})p(\mathbf{W})\big)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Due to the fact that NB method theory is slightly different with 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_nn"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we introduce here and example when 
\begin_inset Formula 
\begin{equation}
p(x_{i}|y_{k},\mathbf{w}_{i})={\cal N}(w_{1,i},w_{2,i})\label{eq:gaussian_conditioned_density}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{w}_{i}=(w_{1,i},w_{2,i})$
\end_inset

, 
\begin_inset Formula $k\in\{0,1\}$
\end_inset

 and 
\begin_inset Formula $p(\mathbf{W})$
\end_inset

 is a uniform distribution.
 Expected loss (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss_mle_or_map"
plural "false"
caps "false"
noprefix "false"

\end_inset

) for Gaussian likelihood case can be written as 
\begin_inset Formula 
\[
\mathbb{E}_{\pi^{*}}L=\frac{1}{N}\sum_{j=1}^{N}\Big(\xi_{j}(\mathbf{W},\mathbf{y}_{j})+\sum_{i=1}^{n}\big(y_{1,j}\big(\log(\frac{1}{\sqrt{2\pi}w_{2,i}})+\frac{(x_{i,j}-w_{1,i})^{2}}{2w_{2,i}^{2}}\big)+y_{2,j}\big(\log(\frac{1}{\sqrt{2\pi}w_{2,i}})+\frac{(x_{i,j}-w_{1,i})^{2}}{2w_{2,i}^{2}}\big)\big)\Big)
\]

\end_inset

where 
\begin_inset Formula $\tilde{\xi}_{j,k}(\mathbf{w})$
\end_inset

 is normalizing parameter calculated from 
\begin_inset Formula $\xi_{j,k}(\mathbf{w})$
\end_inset

 and additional values of Gaussian pdf.
 In this case 
\begin_inset Formula 
\[
w_{1,i}=\frac{1}{N}\sum_{j=1}^{N}x_{i,j}
\]

\end_inset

 and 
\begin_inset Formula 
\[
w_{2,i}=\frac{1}{N}\sum_{j=1}^{N}(x_{i,j}-w_{1,i})^{2}.
\]

\end_inset


\end_layout

\begin_layout Subsection
Decision Theory and Random Forest Algorithm
\end_layout

\begin_layout Standard
In order to work with random forests we must precisely define decision trees
 and only then construct theory for random forests.
\end_layout

\begin_layout Subsubsection
Decision Tree
\end_layout

\begin_layout Standard
In this section we expect our decision three to give us an estimate 
\begin_inset Formula $\hat{\mathbf{y}}(\mathbf{x},\mathbf{w})\in\{(0,1)^{T},(1,0)^{T}\}$
\end_inset

 where 
\begin_inset Formula $\mathbf{w}$
\end_inset

 is a vector that describes tree (depth, branches, etc.), 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

.
 It is very important to mention that for different trees 
\begin_inset Formula $\mathbf{w}$
\end_inset

 can be different.
 Thus, for consistency we will assume that for all 
\begin_inset Formula $\mathbf{w}\in{\cal W}$
\end_inset

 exists upper bound, where 
\begin_inset Formula ${\cal W}$
\end_inset

 is redefined as a space of tree parameters.
 As a result we will make all 
\begin_inset Formula $\mathbf{w}$
\end_inset

 same length.
 If 
\begin_inset Formula $\mathbf{w}$
\end_inset

 has spare slots, they will be filled with zeros.
 Parameter space will be same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

-
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_nb"
plural "false"
caps "false"
noprefix "false"

\end_inset

, whereas action 
\begin_inset Formula $a\in{\cal A}$
\end_inset

 will be represented as 
\begin_inset Formula ${\cal A}={\cal W}$
\end_inset

.
 In order to understand when our tree is optimal we can use zero-one loss
 function.
 Zero-one loss function is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\mathbf{w})=\begin{cases}
1, & \mathbf{y\neq\hat{y}}(\mathbf{x},\mathbf{w})\\
0, & \mathbf{y=\hat{y}}(\mathbf{x},\mathbf{w})
\end{cases}.\label{eq:zero-one_loss_function}
\end{equation}

\end_inset

With the usage of the given data where 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 As a result, expected loss function for decision tree can be derived as
 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\\
 & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w})\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\\
 & =\frac{1}{N}\sum_{i=1}^{N}L(\mathbf{x}_{i},\mathbf{y}_{i},\mathbf{w})\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})
\end{align*}

\end_inset

where 
\begin_inset Formula $\sum_{i=1}^{N}L(\mathbf{x}_{i},\mathbf{y}_{i},\mathbf{w})$
\end_inset

 is (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:zero-one_loss_function"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
If we want to minimize expected loss we have to follow next steps.
 While construction decision tree we choose such feature 
\begin_inset Formula $x_{i}\in(x_{1},...,x_{n})^{T}=\mathbf{x}$
\end_inset

 that will bring as the highest information about the system.
 This feature will form first layer, then we add another feature with the
 highest informational gain and construct second layer.
 Basing of this method we construct nodes and add more and more layers (branches
).
\end_layout

\begin_layout Standard
In the following part we are going to work with a set of decision trees.
 For this purposes we will define our decision tree as 
\begin_inset Formula $\hat{\mathbf{y}}=\big(T_{1}(\mathbf{x},\mathbf{w}_{l}),T_{2}(\mathbf{x},\mathbf{w}_{l})\big)^{T}$
\end_inset

 where index 
\begin_inset Formula $l$
\end_inset

 represents set of parameters for 
\begin_inset Formula $l-$
\end_inset

th three and indices 
\begin_inset Formula $1,2$
\end_inset

 represent first and second value of the one-hot vector.
 
\end_layout

\begin_layout Subsubsection
Random Forest
\end_layout

\begin_layout Standard
With the usage of the given data 
\begin_inset Formula $\mathbf{\tilde{X}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{\tilde{Y}}$
\end_inset

 we want to come up with a decision that will help us to predict labels
 of the data 
\begin_inset Formula ${\cal X}$
\end_inset

0.
 We want to consider 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

 are random variables with joint probability density function 
\begin_inset Formula $p({\bf x},\mathbf{y})$
\end_inset

.
 We will also assume that 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed.
 Probability density function 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 can be written as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:joint_prob_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
With the usage of 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 we will make 
\begin_inset Formula $\{1,...,L\},$
\end_inset

 
\begin_inset Formula $L\in\mathbb{N}$
\end_inset

 sets where 
\begin_inset Formula $\forall l\in L$
\end_inset

, 
\begin_inset Formula $\tilde{\mathbf{X}}_{l}\subset\tilde{\mathbf{X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}_{l}\subset\tilde{{\bf Y}}$
\end_inset

.
 The data 
\begin_inset Formula $\tilde{\mathbf{X}}_{l}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}_{l}$
\end_inset

 are created with random uniform sampling from 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

.
 We also want each subset to contain strictly 
\begin_inset Formula $60\%$
\end_inset

 of the data from 
\begin_inset Formula $\mathbf{\tilde{{\bf X}}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

.
 As a result parameter space for random forests will form tuples of sets
 
\begin_inset Formula $(\tilde{{\bf X}}_{l},\tilde{{\bf Y}}_{l})$
\end_inset

.
 Basing on this theory we will construct 
\begin_inset Formula $L$
\end_inset

 decision trees 
\begin_inset Formula $\hat{y}=T(\mathbf{x},\mathbf{w}_{l})$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}\in\tilde{{\bf X}}_{l}$
\end_inset

.
 As a result for 
\begin_inset Formula $l-$
\end_inset

th decision tree 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi^{*}}L=\frac{1}{N_{l}}\sum_{i=1}^{N_{l}}L(\mathbf{x}_{i,l},\mathbf{y}_{i,l},\mathbf{w}_{l})\delta(\mathbf{x}-\mathbf{x}_{i,l},\mathbf{y}-\mathbf{y}_{i,l})\label{eq:decision_tree_for_random_forest}
\end{equation}

\end_inset

where 
\begin_inset Formula $(\mathbf{x}_{i,l},\mathbf{y}_{i,l})\in(\tilde{{\bf X}}_{l},\tilde{{\bf Y}}_{l})$
\end_inset

 and 
\begin_inset Formula $N_{l}$
\end_inset

 is a number of the data in 
\begin_inset Formula $\tilde{{\bf X}}_{l}$
\end_inset

 and 
\begin_inset Formula $\tilde{\mathbf{Y}}_{l}$
\end_inset

.
 If we assume 
\begin_inset Formula $\mathbf{w}_{l}$
\end_inset

 as a random variable then 
\begin_inset Formula $L$
\end_inset

 decision trees form samples from probability density function 
\begin_inset Formula $p(\text{y}|\mathbf{x},\mathbf{w})$
\end_inset

.
 In other words 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x},\mathbf{w}_{l})=T_{1}(\mathbf{x},\mathbf{w}_{l})^{y_{1}}T_{2}(\mathbf{x},\mathbf{w}_{l})^{y_{2}}\label{eq:decision_tree_as_random_variable}
\end{equation}

\end_inset

where label 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is written as a one-hot representation 
\begin_inset Formula $\mathbf{y}=(y_{1},y_{2})^{T}$
\end_inset

.
 Thus, we can say that classification probability 
\begin_inset Formula $p(\text{\textbf{y}}|\mathbf{x})$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x})=\int_{\mathbf{w}\in\mathbf{{\cal A}}}p(\mathbf{y}|\mathbf{x},\mathbf{w})p(\mathbf{w})d\mathbf{w}.\label{eq:classification_prob_for_random_forest}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\cal A}$
\end_inset

 is an action space.
 With the usage of samples 
\begin_inset Formula $\mathbf{w}_{l}$
\end_inset

 we can approximate 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x})=\frac{1}{L}\sum_{l=1}^{L}T_{1}(\mathbf{x},\mathbf{w}_{l})^{y_{1}}T_{2}(\mathbf{x},\mathbf{w}_{l})^{y_{2}}\label{eq:approx_class_prob_for_random_forest}
\end{equation}

\end_inset

where each decision tree 
\begin_inset Formula $\big(T_{1}(\mathbf{x},\mathbf{w}_{l}),T_{2}(\mathbf{x},\mathbf{w}_{l})\big)^{T}$
\end_inset

 is constructed with the usage of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decision_tree_for_random_forest"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\end_body
\end_document
