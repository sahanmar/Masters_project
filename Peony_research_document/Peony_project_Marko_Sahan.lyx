#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 0.8cm
\headsep 1cm
\footskip 0.5cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
documentdate{July 8, 2019}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%
\backslash
def
\backslash
documentdate{
\backslash
today}
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{empty}
\end_layout

\begin_layout Plain Layout

{
\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align block
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/cvut.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "60line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\align center

\shape smallcaps
\size large
Czech Technical University in Prague
\shape default

\begin_inset Newline newline
\end_inset

Faculty of Nuclear Sciences and Physical Engineering
\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/fjfi.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 3cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Active Learning for Text Classification
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Aktivní učení pro klasifikaci textů
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\size large
Masters's Degree Project
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
ends the centered part (the required new paragraph before "}" is inserted
 by \SpecialChar LyX
 as "}" is on a separate line.)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Author: 
\series bold
Marko Sahan
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Supervisor: 
\series bold
doc.
 Ing.
 Václav Šmídl, Ph.D.
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Academic
\begin_inset space ~
\end_inset

year: 2019/2020
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Final dummy paragraph.
 Its function is to bear the page break flag
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\align center
- Zadání práce -
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\align center
- Zadání práce (zadní strana) -
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Acknowledgment:
\end_layout

\begin_layout Standard
\noindent
I would like to thank ............................................
 for (his/her expert guidance) and express my gratitude to ..........................................
 for (his/her language assistance).
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Author's declaration:
\end_layout

\begin_layout Standard
\noindent
I declare that this Masters's Degree Project is entirely my own work and
 I have listed all the used sources in the bibliography.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
Prague, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
documentdate
\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset

Jméno Autora
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
\lang czech
Název práce:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\series bold
\lang czech
Aktivní učení pro klasifikaci textů
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Autor:
\emph default
 Marko Sahan
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Obor:
\emph default
 Aplikované matematicko-stochastické metody
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Druh práce:
\emph default
 Diplomová práce
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Vedoucí práce:
\emph default
 
\series bold
\lang american
doc.
 Ing.
 Václav Šmídl, Ph.D.
\series default
\lang czech
, Ústav teorie informace a automatizace
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Abstrakt:
\emph default
 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang czech
WRITE AN ABSTRACT IN CZECH
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Klíčová slova:
\emph default
 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang czech
FILL IN THE KEYWORDS IN CZECH
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
Title:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\series bold
Active learning for text classification
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Author:
\emph default
 Marko Sahan
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Abstract:
\emph default
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
WRITE AND ABSTRACT IN ENGLISH
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Key words:
\emph default
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
FILL IN THE KEYWORDS IN ENGLISH
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter*
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Introduction}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
WRITE IN AN INTRODUCTION
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Introduction to Decision Theory
\begin_inset CommandInset label
LatexCommand label
name "chap:Introduction-to-Decision"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Easily speaking, decision can be defined as a set of actions.
 We are going to work with text data and solve text classification problem.
 Thus, our main purpose is to introduce precise mathematical abstractions
 for an automatic and optimal decision making.
\end_layout

\begin_layout Standard
Considering binary classification problem.
 We are seeking to find such set of actions that will assign each input
 value to its class.
 
\end_layout

\begin_layout Standard
We would like to commence our formal definition with the data.
 Let 
\begin_inset Formula $\mathbf{x}\in{\cal X}\subset\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}=\{(0,1)^{T},(1,0)^{T}\}$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 are feature vectors of size 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula ${\bf y}$
\end_inset

 are its labels assigned to the data from space 
\begin_inset Formula ${\cal X}$
\end_inset

.
 Each value from space 
\begin_inset Formula ${\cal Y}$
\end_inset

 can be represented as a one hot representation that consist from ones and
 zeros 
\begin_inset Formula $\mathbf{y}\in\{(0,1)^{T},(1,0)^{T}\}$
\end_inset

.
 Therefore, first class is represented as 
\begin_inset Formula $\mathbf{y}=(1,0)^{T}$
\end_inset

 and second class is represented as 
\begin_inset Formula $\mathbf{y}=(0,1)^{T}$
\end_inset

.
 As a good example of previous definition, 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can be a text document (represented in a mathematical form in order to
 meet a definition above) and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 can be its label that will show if the document is relevant or not.
 As seen from an example, label and text are forming a tuple.
 In this work we are also considering our data as tuples of variables 
\begin_inset Formula $(\mathbf{x},\mathbf{y})\in{\cal X\times Y}$
\end_inset

.
 
\end_layout

\begin_layout Section
Decision Theory
\end_layout

\begin_layout Standard
In this section we would like to define solution of a classification problem
 as finding such set of actions (decision) that minimizes loss.
 By loss we can understand classification error, entropy, etc..
 This problem is still set up vaguely because neither decision nor loss
 was properly defined.
 
\end_layout

\begin_layout Standard
Each classification problem can be summarized as finding the optimal boundary
 that will split the dataset with respect to labels.
 The boundary can be defined as an action that is done with respect to some
 conditions.
 Let 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

 is an action and 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is an action space.
 As mentioned previously, we would like to make a decision (action) that
 will split the dataset in two sets and we want this classification to be
 as good as possible.
 Quality of the classification can be measured with specific metrics which
 will be introduced in further sections.
 In our case we would like to minimize losses (error of classification).
 As a result, tandem of an action and loss minimization metric lets us to
 understand how good the action is.
 Subsequent step is an introduction of a loss function 
\begin_inset Formula $L$
\end_inset

.
 Loss function is dependent on action 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

.
 Formal definition of the loss function is shown next.
 
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "Berger:1327974"
literal "false"

\end_inset

 loss function 
\begin_inset Formula $L$
\end_inset

 can be defines as 
\begin_inset Formula 
\begin{equation}
L=L(\theta,a),\label{eq:loss_func}
\end{equation}

\end_inset

where 
\begin_inset Formula $\theta\in\varTheta$
\end_inset

 is parameters' vector where 
\begin_inset Formula $\varTheta$
\end_inset

 is parameters space and 
\begin_inset Formula $a\in\mathcal{A}$
\end_inset

 is an action where 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is an action space.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "Berger:1327974"
literal "false"

\end_inset

, it is also said that parameter 
\begin_inset Formula $\theta$
\end_inset

 can be understand as the true state of nature.
 Definition above shows us loss with respect to one action and one true
 state of nature.
 Thus, we would like to define expected loss function.
\end_layout

\begin_layout Definition
\begin_inset CommandInset citation
LatexCommand cite
key "Berger:1327974"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "def:expected_loss"

\end_inset

If 
\begin_inset Formula $\pi^{*}(\theta)$
\end_inset

 is believed probability distribution of 
\begin_inset Formula $\theta$
\end_inset

 at the time of decision making, the 
\shape slanted
Bayesian expected loss
\shape default
 of an action 
\begin_inset Formula $a$
\end_inset

 is 
\begin_inset Formula 
\begin{align}
\rho(\pi^{*},a)= & \mathbb{E}_{\pi^{*}}[L(\theta,a)],\label{eq:expected_loss-1}\\
= & \int_{\varTheta}L(\theta,a)\pi^{*}d\theta.
\end{align}

\end_inset

Basing on definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the optimal action is the one that minimizes expected loss, which is defined
 as 
\end_layout

\begin_layout Definition
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
{argmax}
\end_inset


\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
{argmin}
\end_inset


\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
a^{*}=\argmin_{a\in{\cal A}}\mathbb{E}_{\pi^{*}}[L(\theta,a)].\label{eq:optimal_action}
\end{equation}

\end_inset

However, how can we connect optimal action 
\begin_inset Formula $a^{*}$
\end_inset

 with given data? Basing on the data definitions from previous part, we
 can assume that 
\begin_inset Formula $\mathcal{X\times Y}$
\end_inset

 is an infinite set and 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

 are samples from this set.
 Considering 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in{\cal Y}}$
\end_inset

 are random variables.
 If both 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 are random variables then tuple 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

 is a sample from joint probability density function 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

.
 With the usage of conditional probability rule 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x},\mathbf{y})=p(\mathbf{y}|\mathbf{x})p(\mathbf{x}).\label{eq: data_bayes_rule}
\end{equation}

\end_inset

At this part we are able to show which meaning this optimal action 
\begin_inset Formula $a^{*}$
\end_inset

 has.
 In equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: data_bayes_rule"
plural "false"
caps "false"
noprefix "false"

\end_inset

 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

 is given and 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 is an unknown pdf that we want to estimate.
 The optimal action will lead us to an estimate of 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

.
\end_layout

\begin_layout Section
Decision Theory for Supervised Learning
\end_layout

\begin_layout Standard
Supervised learning requires validation set of the data.
 Validation set is splitted on training and testing sets.
 Thus, we have to extend data definition.
\end_layout

\begin_layout Standard
Considering the data 
\begin_inset Formula $\tilde{\mathbf{X}}\subset{\cal X}$
\end_inset

 and its labels 
\begin_inset Formula $\tilde{\mathbf{Y}}\subset{\cal Y}$
\end_inset

.
 We define cartesian product 
\series bold

\begin_inset Formula $\tilde{\mathbf{X}}\times\mathbf{\tilde{Y}}$
\end_inset


\series default
 as a validation set.
\end_layout

\begin_layout Subsection
Decision Theory and Support Vector Machine Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_svm"

\end_inset


\end_layout

\begin_layout Standard
In this subsection we will continue construction of the decision theory
 on the example of Support Vector Machine (SVM) method.
 For simplicity lets consider linearly separable dataset.
 From the theoretical perspective SVM constructs hyperplane in high dimensional
 space that separates two classes.
 In this case our decision (action) is a hyperplane that will separate two
 classes.
 Equation of the hyperplane can be written as 
\begin_inset Formula $f(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

 where 
\begin_inset Formula $\text{\ensuremath{\mathbf{w}}\ensuremath{\in\mathbb{R}^{n}} }$
\end_inset

 is a set of hyperplane parameters and 
\begin_inset Formula $b\in\mathbb{R}$
\end_inset

 is a bias .
 As a result, action space is represented as 
\begin_inset Formula $(\mathbb{R}^{n},\mathbb{R})\mathcal{=A}$
\end_inset

 and as a consequence tuple 
\begin_inset Formula $(\ensuremath{\mathbf{w}},b)\in\mathcal{A}$
\end_inset

.
 From this knowledge we can define 
\begin_inset Formula $\theta=(\mathbf{x},\mathbf{y})$
\end_inset

 where tuple 
\begin_inset Formula $(\mathbf{\mathbf{x}},\mathbf{y})\in{\cal X}\times{\cal Y}$
\end_inset

 is parameters and 
\begin_inset Formula $\varTheta={\cal X}\times{\cal Y}$
\end_inset

 is a parameters' space.
 However, for now we are limited on 
\begin_inset Formula $\varTheta=\tilde{\mathbf{X}}\times\mathbf{\tilde{Y}}$
\end_inset

.
 Considering loss function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

 that can be written as
\begin_inset Formula 
\begin{equation}
L=L(\mathbf{x},\mathbf{y},\mathbf{w},b).\label{eq:SVM_thoer_loss}
\end{equation}

\end_inset

Following task is to understand how good is our action (hyperplane estimation)
 with respect to the dataset.
 We can choose different types of loss functions such as cross entropy,
 hinge loss, etc..
 The most basic approach for SVM method is hinge loss function 
\begin_inset CommandInset citation
LatexCommand cite
key "rosasco2004loss"
literal "false"

\end_inset

 which is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\mathbf{w},b)=\max(0,1-y\hat{y}(\mathbf{x},\mathbf{w},b))\label{eq:hinge_loss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{y}(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

 and 
\begin_inset Formula $y=\mathbf{y}_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
In terms of SVM method we want to find such hyperplane that will label input
 values as a first class if it is 
\begin_inset Quotes eld
\end_inset

above
\begin_inset Quotes erd
\end_inset

 the hyperplane and as a second class if it is 
\begin_inset Quotes eld
\end_inset

below
\begin_inset Quotes erd
\end_inset

 the hyperplane.
 At this point very important assumption will be introduced.
 In order to find an optimal hyperplane we assume that the data 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and its labels 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 fully describe spaces 
\begin_inset Formula ${\cal X}$
\end_inset

 and 
\begin_inset Formula ${\cal Y}$
\end_inset

.
 Moreover, as mentioned earlier, we consider 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}\in{\cal Y}$
\end_inset

 are random variables with joint probability density function 
\begin_inset Formula $p({\bf x},\mathbf{y})$
\end_inset

.
 We will also assume that 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed.
 As a result, with the usage of those data, probability density function
 
\begin_inset Formula $p({\bf x},\mathbf{y})$
\end_inset

 can be estimated as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x},\mathbf{y})=\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})\label{eq:approx_of_joint_dist}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})$
\end_inset

 is Dirac delta function which is centered in 
\begin_inset Formula $(\mathbf{x}_{i},\mathbf{y}_{i})$
\end_inset

.
\end_layout

\begin_layout Standard
Using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we can evaluate expected loss function for SVM as follows 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w},b)p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\\
 & =\int_{{\cal X}\times{\cal Y}}\max(0,1-y_{1}\hat{y}(\mathbf{x},\mathbf{w},b))\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\\
 & =\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{1,i}\hat{y}(\mathbf{x}_{i},\mathbf{w},b))
\end{align*}

\end_inset

where 
\begin_inset Formula $\hat{y}(\mathbf{x}_{i},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}_{i}+b$
\end_inset

 and 
\begin_inset Formula $y_{1,i}$
\end_inset

 is first component of 
\begin_inset Formula $i-\mathrm{th}$
\end_inset

 vector 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 Expect loss function for SVM can be written as 
\begin_inset Formula 
\begin{equation}
\rho(\mathbf{x}_{i},\mathbf{w},b)=\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{i}(\mathbf{w}^{T}\mathbf{x}_{i}+b)).\label{eq:expected_loss_SVM}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Decision Theory and Algorithm Based on Neural Network Function 
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_nn"

\end_inset


\end_layout

\begin_layout Standard
Decision Theory construction for the algorithm, based on a neural network
 function, is mostly the same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 However in this case our decision is to find estimate 
\begin_inset Formula $\hat{\mathbf{y}}=\hat{\mathbf{y}}(\mathbf{x},\mathbf{W},\mathbf{b})$
\end_inset

 of the probability density function 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the input data, 
\begin_inset Formula $\mathbf{W}$
\end_inset

 is a set of all neural network function parameters and 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is a vector of biases.
 Action space 
\begin_inset Formula $\mathcal{A}$
\end_inset

 will be parameters' and biases' space of 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

.
 Same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can define 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

 are parameters of the loss function and 
\begin_inset Formula ${\cal X}\times{\cal Y}$
\end_inset

 is a parameters' space.
 Another example of loss functions that we will use is cross entropy loss
 function, which is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\mathbf{W},\mathbf{b})=-y_{1}\ln\big(\hat{y}_{1}(\mathbf{x},\mathbf{W},\mathbf{b})\big)-y_{2}\ln\big((\hat{y}_{2}(\mathbf{x},\mathbf{W},\mathbf{b})\big),\label{eq:cross_entropy_loss}
\end{equation}

\end_inset

where 
\series bold

\begin_inset Formula $\mathbf{y}=(y_{1},y_{2})^{T}$
\end_inset

 and 
\begin_inset Formula $\hat{\mathbf{y}}=(\hat{y}_{1},\hat{y}_{2})^{T}$
\end_inset

.
 
\series default
We consider 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

 are random variables with joint probability density function 
\begin_inset Formula $p({\bf x},\mathbf{y})$
\end_inset

.
 With the usage of the given dataset where 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Applying definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "def:expected_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

, expected loss for the algorithm based on a neural network function is
 evaluated as 
\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},y,\mathbf{W},\mathbf{b})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & =-\int_{\mathbf{{\cal X}}\times{\cal Y}}\Big(y_{1}\ln\big(\hat{y}_{1}(\mathbf{x},\mathbf{W},\mathbf{b})\big)+y_{2}\ln\big((\hat{y}_{2}(\mathbf{x},\mathbf{W},\mathbf{b})\big)\Big)\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & =-\frac{1}{N}\sum_{i=1}^{N}\Big(y_{1}\ln\big(\hat{y}_{1}(\mathbf{x},\mathbf{W},\mathbf{b})\big)+y_{2}\ln\big((\hat{y}_{2}(\mathbf{x},\mathbf{W},\mathbf{b})\big)\Big),\label{eq:expected_loss_cross_entropy}
\end{align}

\end_inset

where 
\series bold

\begin_inset Formula $\mathbf{y}=(y_{1},y_{2})^{T}$
\end_inset

 and 
\begin_inset Formula $\hat{\mathbf{y}}=(\hat{y}_{1},\hat{y}_{2})^{T}$
\end_inset

.
\end_layout

\begin_layout Subsection
Decision Theory and Naive Bayes Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_nb"

\end_inset


\end_layout

\begin_layout Standard
Naive Bayes algorithm is a bit different to the algorithm based on Neural
 Networks and SVM.
 In the case of Naive Bayes we want to estimate 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 where 
\begin_inset Formula $\mathbf{W}=(\mathbf{w}_{1},\mathbf{w}_{2},...,\mathbf{w}_{n})\subset{\cal W}$
\end_inset

 is an action (
\begin_inset Formula $a=\mathbf{W},\ a\in\mathcal{A}$
\end_inset

).
 The reason why we look for an estimate of the 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 but not 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{w})$
\end_inset

 is due to the fact that in case of estimating 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{W})$
\end_inset

 we would have to work with normalization constant would be dependent on
 the set of parameters 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 That fact would make our computations very complicated.
 Before continuing with a loss function construction we would like to go
 trough Naive Bayes (NB) method.
\end_layout

\begin_layout Subsubsection
Naive Bayes
\end_layout

\begin_layout Standard
Assuming binary classification problem.
 With the usage of Bayes rule we can rewrite
\series bold
 
\series default

\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 as follows 
\begin_inset Formula 
\begin{equation}
p(\mathbf{W}|\mathbf{x},\mathbf{y})=\frac{p(\mathbf{y})p(\mathbf{x}|\mathbf{y},\mathbf{W})p(\mathbf{W})}{\int_{\mathbf{{\cal W}}}p(\mathbf{x},\mathbf{y}|\mathbf{W})p(\mathbf{W})d\mathbf{W}}\label{eq:bayes_rule}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\cal W}$
\end_inset

 is assumed as a space of 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Naive Bayes method introduces very strong assumption in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes_rule"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 This assumption says that features of vector 
\begin_inset Formula $\mathbf{x}=(x_{1},x_{2},...,x_{n})^{T}$
\end_inset

 are conditionally independent.
 As a result estimation of 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 can be estimated as
\begin_inset Formula 
\begin{equation}
\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y})=\frac{1}{Z}p(\mathbf{y})p(\mathbf{W})\prod_{i=1}^{n}\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big),\label{eq:naive_bayes_eq}
\end{equation}

\end_inset

where 
\series bold

\begin_inset Formula $\mathbf{y}=(y_{1},y_{2})$
\end_inset


\series default
, 
\begin_inset Formula $\mathbf{W}=(\mathbf{w}_{1},\mathbf{w}_{2},...,\mathbf{w}_{n})$
\end_inset

, 
\begin_inset Formula $Z$
\end_inset

 is a normalizing constant.
 
\end_layout

\begin_layout Subsubsection
Decision Theory
\end_layout

\begin_layout Standard
We want to maximize probability 
\begin_inset Formula $\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

.
 As a result, using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:naive_bayes_eq"
plural "false"
caps "false"
noprefix "false"

\end_inset

, loss function 
\begin_inset Formula $L$
\end_inset

 will be represented as 
\begin_inset Formula 
\begin{align}
L(\mathbf{y},\mathbf{x},\mathbf{w}) & =-\log\big(\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y}),\label{eq:nb_loss}\\
 & =\log(Z)-\log\big(p(\mathbf{y})\big)-\log\big(p(\mathbf{W})\big)-\sum_{i=1}^{n}\log\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big).
\end{align}

\end_inset

 Same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_nn"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we will assume that we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 From this moment everything is ready for deriving expected loss function.
 Expected loss function for Naive Bayes method is derived as 
\series bold

\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi^{*}}L & =\int_{\mathbf{{\cal X}}\times\mathbf{{\cal Y}}}L(\mathbf{x},\mathbf{y},\mathbf{w})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & \int_{\mathbf{{\cal X}}\times\mathbf{{\cal Y}}}\Big(\xi(\mathbf{W},\mathbf{y})-\sum_{i=1}^{n}\log\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big)\frac{1}{N}\sum_{j=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{j},\mathbf{y}-\mathbf{y}_{j})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & \frac{1}{N}\sum_{j=1}^{N}\Big(\xi_{j}(\mathbf{W},\mathbf{y}_{j})-\sum_{i=1}^{n}\log\big(p(x_{i,j}|y_{1,j},\mathbf{w}_{i})^{y_{1,j}}p(x_{i,j}|y_{2,j},\mathbf{w}_{i})^{y_{2,j}}\big)\Big)\label{eq:expected_loss_mle_or_map}
\end{align}

\end_inset

where 
\begin_inset Formula $\xi(\mathbf{W},\text{y})=\log(Z)-\log\big(p(\mathbf{y})\big)-\log\big(p(\mathbf{W})\big)$
\end_inset

 and 
\begin_inset Formula $\xi_{j}(\mathbf{W},\mathbf{y}_{j})=\log(Z)-\log\big(p(\mathbf{y}_{j})-\log\big(p(\mathbf{W})\big)$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Decision Theory and Random Forest Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:random_forest_supervised_learning"

\end_inset


\end_layout

\begin_layout Standard
In order to work with random forests we must precisely define decision trees
 and only then construct theory for random forests.
\end_layout

\begin_layout Subsubsection
Decision Tree
\end_layout

\begin_layout Standard
In this section we expect our decision three to give us an estimate 
\begin_inset Formula $\hat{\mathbf{y}}(\mathbf{x},\mathbf{w})\in\{(0,1)^{T},(1,0)^{T}\}$
\end_inset

 where 
\begin_inset Formula $\mathbf{w}$
\end_inset

 is a vector that describes tree (depth, branches, etc.), 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

.
 It is very important to mention that for different trees 
\begin_inset Formula $\mathbf{w}$
\end_inset

 can have different dimensionality.
 Thus, for consistency we will assume that for all 
\begin_inset Formula $\mathbf{w}\in{\cal W}$
\end_inset

 exists upper bound, where 
\begin_inset Formula ${\cal W}$
\end_inset

 is redefined as a space of tree parameters.
 As a result we will make all 
\begin_inset Formula $\mathbf{w}$
\end_inset

 same length.
 If 
\begin_inset Formula $\mathbf{w}$
\end_inset

 has spare elements, they will be filled with zeros.
 Parameter space will be same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

-
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_nb"
plural "false"
caps "false"
noprefix "false"

\end_inset

, whereas action 
\begin_inset Formula $a\in{\cal A}$
\end_inset

 will be represented as 
\begin_inset Formula ${\cal A}={\cal W}$
\end_inset

.
 In order to understand when our tree is optimal we can use zero-one loss
 function.
 Zero-one loss function is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\mathbf{w})=\begin{cases}
1, & \mathbf{y\neq\hat{y}}(\mathbf{x},\mathbf{w})\\
0, & \mathbf{y=\hat{y}}(\mathbf{x},\mathbf{w})
\end{cases}.\label{eq:zero-one_loss_function}
\end{equation}

\end_inset

With the usage of the given data where 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 As a result, expected loss function for decision tree can be derived as
 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\\
 & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w})\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\\
 & =\frac{1}{N}\sum_{i=1}^{N}L(\mathbf{x}_{i},\mathbf{y}_{i},\mathbf{w})
\end{align*}

\end_inset

where 
\begin_inset Formula $\sum_{i=1}^{N}L(\mathbf{x}_{i},\mathbf{y}_{i},\mathbf{w})$
\end_inset

 is (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:zero-one_loss_function"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
If we want to minimize expected loss we have to follow next steps.
 While construction decision tree we choose such feature 
\begin_inset Formula $x_{i}\in(x_{1},...,x_{n})^{T}=\mathbf{x}$
\end_inset

 that will bring as the highest information about the system.
 This feature will form first layer, then we add another feature with the
 highest informational gain and construct second layer.
 Basing of this method we construct nodes and add more and more layers (branches
).
\end_layout

\begin_layout Standard
In the following part we are going to work with a set of decision trees.
 For this purposes we will define our decision tree as 
\begin_inset Formula $\hat{\mathbf{y}}=\big(T_{1}(\mathbf{x},\mathbf{w}_{l}),T_{2}(\mathbf{x},\mathbf{w}_{l})\big)^{T}$
\end_inset

 where index 
\begin_inset Formula $l$
\end_inset

 represents set of parameters for 
\begin_inset Formula $l-$
\end_inset

th three and indices 
\begin_inset Formula $1,2$
\end_inset

 represent first and second value of the one-hot vector.
 
\end_layout

\begin_layout Subsubsection
Random Forest
\end_layout

\begin_layout Standard
Considering 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

 as random variables with joint probability density function 
\begin_inset Formula $p({\bf x},\mathbf{y})$
\end_inset

.
 We will also assume that 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed.
 
\end_layout

\begin_layout Standard
With the usage of 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 we will make 
\begin_inset Formula $\{1,...,L\},$
\end_inset

 
\begin_inset Formula $L\in\mathbb{N}$
\end_inset

 sets where 
\begin_inset Formula $\forall l\in L$
\end_inset

, 
\begin_inset Formula $\tilde{\mathbf{X}}_{l}\subset\tilde{\mathbf{X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}_{l}\subset\tilde{{\bf Y}}$
\end_inset

.
 The data 
\begin_inset Formula $\tilde{\mathbf{X}}_{l}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}_{l}$
\end_inset

 are created with random uniform sampling from 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

.
 We also want each subset to contain strictly 
\begin_inset Formula $60\%$
\end_inset

 of the data from 
\begin_inset Formula $\mathbf{\tilde{{\bf X}}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

.
 As a result parameter space for random forests will form tuples of sets
 
\begin_inset Formula $(\tilde{{\bf X}}_{l},\tilde{{\bf Y}}_{l})$
\end_inset

.
 Basing on this theory we will construct 
\begin_inset Formula $L$
\end_inset

 decision trees 
\begin_inset Formula $\hat{y}=T(\mathbf{x},\mathbf{w}_{l})$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}\in\tilde{{\bf X}}_{l}$
\end_inset

.
 As a result for 
\begin_inset Formula $l-\mathrm{th}$
\end_inset

 decision tree 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi^{*}}L=\frac{1}{N_{l}}\sum_{i=1}^{N_{l}}L(\mathbf{x}_{i,l},\mathbf{y}_{i,l},\mathbf{w}_{l})\delta(\mathbf{x}-\mathbf{x}_{i,l},\mathbf{y}-\mathbf{y}_{i,l})\label{eq:decision_tree_for_random_forest}
\end{equation}

\end_inset

where 
\begin_inset Formula $(\mathbf{x}_{i,l},\mathbf{y}_{i,l})\in(\tilde{{\bf X}}_{l},\tilde{{\bf Y}}_{l})$
\end_inset

 and 
\begin_inset Formula $N_{l}$
\end_inset

 is a number of the data in 
\begin_inset Formula $\tilde{{\bf X}}_{l}$
\end_inset

 and 
\begin_inset Formula $\tilde{\mathbf{Y}}_{l}$
\end_inset

.
 If we assume 
\begin_inset Formula $\mathbf{w}_{l}$
\end_inset

 as a random variable then 
\begin_inset Formula $L$
\end_inset

 decision trees form samples from probability density function 
\begin_inset Formula $p(\text{y}|\mathbf{x},\mathbf{w})$
\end_inset

.
 In other words 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x},\mathbf{w}_{l})=T_{1}(\mathbf{x},\mathbf{w}_{l})^{y_{1}}T_{2}(\mathbf{x},\mathbf{w}_{l})^{y_{2}}\label{eq:decision_tree_as_random_variable}
\end{equation}

\end_inset

where label 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is written as a one-hot representation 
\begin_inset Formula $\mathbf{y}=(y_{1},y_{2})^{T}$
\end_inset

.
 Thus, we can say that classification probability 
\begin_inset Formula $p(\text{\textbf{y}}|\mathbf{x})$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x})=\int_{\mathbf{w}\in\mathbf{{\cal A}}}p(\mathbf{y}|\mathbf{x},\mathbf{w})p(\mathbf{w})d\mathbf{w}.\label{eq:classification_prob_for_random_forest}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\cal A}$
\end_inset

 is an action space.
 With the usage of samples 
\begin_inset Formula $\mathbf{w}_{l}$
\end_inset

 we can approximate 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x})=\frac{1}{L}\sum_{l=1}^{L}T_{1}(\mathbf{x},\mathbf{w}_{l})^{y_{1}}T_{2}(\mathbf{x},\mathbf{w}_{l})^{y_{2}}\label{eq:approx_class_prob_for_random_forest}
\end{equation}

\end_inset

where each decision tree 
\begin_inset Formula $\big(T_{1}(\mathbf{x},\mathbf{w}_{l}),T_{2}(\mathbf{x},\mathbf{w}_{l})\big)^{T}$
\end_inset

 is constructed with the usage of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decision_tree_for_random_forest"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
Before continuing with further sections we define output of Random Forest
 as 
\begin_inset Formula $\hat{\mathbf{y}}=\hat{\mathbf{y}}(\mathbf{x},\mathbf{W})$
\end_inset

, where 
\begin_inset Formula $\mathbf{W}=(\mathbf{w}_{1},...,\mathbf{w}_{L})$
\end_inset

 is set of parameters of specific Random Forest algorithm.
 We define vector 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\hat{\mathbf{y}}=\frac{1}{L}\sum_{l=1}^{L}\big(T_{1}(\mathbf{x},\mathbf{w}_{l}),T_{2}(\mathbf{x},\mathbf{w}_{l})\big).\label{eq:Random_forest_output}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Decision Theory for Active Learning 
\end_layout

\begin_layout Standard
As mentioned in previous sections 
\series bold

\begin_inset Formula $\tilde{\mathbf{X}}\times\mathbf{\tilde{Y}}$
\end_inset


\series default
 is a validation set.
 When we finish a model training, we may think that we need more training
 data.
 Thus, we can choose the data from 
\begin_inset Formula $\mathbf{X}\subset{\cal X}\backslash\tilde{\mathbf{X}}$
\end_inset

.
 However it is important to understand that we have no labels for the set
 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 We can ask for a help from an annotator that can give us those labels.
 We assume that getting labels needs some time and is very expensive.
 
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\argmax}{\operatornamewithlimits{argmax}}
{argmax}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\argmin}{\operatornamewithlimits{argmin}}
{argmin}
\end_inset


\end_layout

\begin_layout Standard
Active learning problem is defined as a sequence of Supervised learning
 problems.
 Specifically, we assume that labels 
\begin_inset Formula $\mathbf{y}\in\mathbf{\tilde{Y}}$
\end_inset

 are available only for 
\begin_inset Formula $\mathbf{x}\in\tilde{\mathbf{X}}$
\end_inset

.
 We have the possibility to select an unlabeled element from 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and ask for its label.
 Since it is expensive, we aim to have such questions that will maximize
 scores as fast as possible.
 Formally, we denote 
\begin_inset Formula $J_{0}=\{j_{01},j_{02},\ldots j_{0N}\}=\{1,\ldots,N\}$
\end_inset

 the initial set of 
\begin_inset Formula $N$
\end_inset

 available labels.
 using only the labeled data, the supervised learning task is defined on
 sets 
\begin_inset Formula $\mathbf{X}_{0}=\tilde{\mathbf{X}}=\{\mathbf{x}_{i}\}_{i\in J_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Y}_{0}=\tilde{\mathbf{Y}}=\{\mathbf{y}_{i}\}_{i\in J_{0}}$
\end_inset

.
 This set is sequentially extended with new labels gained from 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 We consider a sequence of 
\begin_inset Formula $U$
\end_inset

 questions 
\begin_inset Formula $u=\{1,\ldots,U\}$
\end_inset

, in each question, we select an index 
\begin_inset Formula $j_{u}$
\end_inset

 and ask to obtain the label 
\begin_inset Formula $\mathbf{y}_{j_{u}}$
\end_inset

 for data record 
\begin_inset Formula $\mathbf{x}_{j_{u}}$
\end_inset

.
 The index set and the data sets are extended as follows
\begin_inset Formula 
\begin{align*}
J_{u} & =\{J_{u-1},j_{u}\}, & \mathbf{X}_{u} & =\{\mathbf{X}_{u-1},\mathbf{x}_{j_{u}}\}, & \mathbf{Y}_{u} & =\{\mathbf{Y}_{u-1},\mathbf{y}_{j_{u}}\}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The task of active learning is to optimize the selection of indices 
\begin_inset Formula $j_{u}$
\end_inset

 to reach as good classification metrics with as low number of questions
 as possible.
 As a result we have to define expected loss for each question 
\begin_inset Formula $u$
\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
rephrase!
\end_layout

\end_inset

 that will be dependent on the action and parameter spaces.
 In this case we can define our action space 
\begin_inset Formula ${\cal A}_{u}$
\end_inset

 as a space of the data indices with respect to the parameters space 
\begin_inset Formula $\varTheta_{u}$
\end_inset

 for each question 
\begin_inset Formula $u$
\end_inset

.
 Parameters space is defined as a set of possible parameters from a specific
 model.
 It is very important to understand that we will need not only one set of
 parameters but parameters distribution.
 We need parameters distribution because we will integrate over the parameters
 space.
 As an example, if we talk about SVM method, then parameters space for active
 learning problem will be defined as a set of weights that form a hyperplane.
 If we talk about the algorithm that is based on a Neural Network function,
 then parameters space of the active learning problem will form weights
 from neurons.
 We wanted to highlight that parameters space will be different for each
 problem but the idea for each algorithm is same 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
Rephrase
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
As a result our task can be written as 
\begin_inset Formula 
\begin{equation}
j_{u}^{*}=\argmin_{j\in J\backslash J_{u}}(\mathbb{E}_{\pi_{u}^{*}}L^{*})\label{eq:index_selection_active_learning}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbb{E}_{\pi_{u}^{*}}L^{*}$
\end_inset

 is expected loss that is dependent on an action given question 
\begin_inset Formula $u$
\end_inset

, and 
\begin_inset Formula $J$
\end_inset

 is space of all indices.
 Expected loss for the active learning problem is defined as 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi_{u}^{*}}L^{*}=\int_{\varTheta_{u}}L^{*}(a,\theta)\pi_{u}^{*}d\theta\label{eq:Expected_loss_active_learning}
\end{equation}

\end_inset

where 
\begin_inset Formula $a\in{\cal A}_{u}$
\end_inset

 and 
\begin_inset Formula $\theta\in\varTheta_{u}$
\end_inset

 and 
\begin_inset Formula $L^{*}$
\end_inset

 is a loss function for the active learning problem.
 Character 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $*$
\end_inset


\begin_inset Quotes erd
\end_inset

 is used only for distinguishing active learning loss from the loss function
 which is used for different models.
 We will specify action space because it will be the same for all models
 that are used in the active learning section.
 Action space 
\begin_inset Formula ${\cal A}$
\end_inset

 is a set of possible indices 
\begin_inset Formula $j_{u}\in J_{u}$
\end_inset

 where 
\begin_inset Formula $u$
\end_inset

 is a specific question.
 Thus, (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Expected_loss_active_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

) can be written as 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi_{u}^{*}}L^{*}=\int_{\varTheta_{u}}L^{*}(j_{u},\theta)\pi_{u}^{*}d\theta.\label{eq:Expected_loss_active_learning_with_action_space_as_indices}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Using this approach we will be able sequentially select indices from 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and ask for a label from 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 that will help us to get higher scores faster than in the case of random
 choice of indices.
 
\end_layout

\begin_layout Subsection
Bayesian Approach of Classifiers' Parameters Sampling
\end_layout

\begin_layout Standard
Considering that 
\begin_inset Formula $\mathbf{y}\in{\cal Y}$
\end_inset

.
 Let 
\begin_inset Formula $\hat{\mathbf{y}}=\hat{\mathbf{y}}(\mathbf{x}_{j_{u}},\theta_{u})$
\end_inset

 is an estimate of 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 However, in this case output estimate 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

 is represented as a vector of probabilities that 
\begin_inset Formula $\mathbf{x}_{j_{u}}$
\end_inset

 is assigned to different classes.
 As an example for well trained binary classifier, for specific 
\begin_inset Formula $\mathbf{x}$
\end_inset

 that is assigned to 
\begin_inset Formula $\mathbf{y}=(1,0)^{T},$
\end_inset

 classifiers estimate of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can be 
\begin_inset Formula $\mathbf{\hat{y}}=(0.95,0.05)^{T}$
\end_inset

.
 It is very interesting that before we can solve the optimization problem
 with choosing the index 
\begin_inset Formula $j_{u}$
\end_inset

 we have to solve the optimization problem of finding 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

.
 This leads us to supervised learning models that we have discussed in previous
 sections.
\end_layout

\begin_layout Standard
In this section we would like to construct theory around 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Expected_loss_active_learning_with_action_space_as_indices"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Mentioned distribution is a distribution of the models' parameters given
 the training data that can be written as 
\begin_inset Formula 
\begin{equation}
\pi_{u}^{*}=p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u}).\label{eq:prior_distribution_for_expected_entropy_loss}
\end{equation}

\end_inset

We do not have explicit form of the pdf.
 However, we assume that we have samples 
\begin_inset Formula $Q_{u}$
\end_inset

 samples 
\begin_inset Formula $\theta_{u,q}\in\{1_{u},...,Q_{u}\}$
\end_inset

 from 
\begin_inset Formula $p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

.
 As a result 
\begin_inset Formula $p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

 can be approximated as 
\begin_inset Formula 
\begin{equation}
\pi_{u}^{*}=\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\delta(\theta_{u}-\theta_{u,q}),\label{eq:prior_distribution_for_exp_loss_estimate}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta(\theta_{u}-\theta_{u,q})$
\end_inset

 is Dirac delta function centered in 
\begin_inset Formula $\theta_{u,q}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Parameters Sampling Based on Training Data Subsets
\end_layout

\begin_layout Standard
This method is quite general and can be applied to all types of classifiers
 in this work (Random Forest, SVM, Neural Network).
 The idea is very simple.
 We consider that some data samples in training dataset 
\begin_inset Formula $\tilde{\mathbf{X}}\times\tilde{\mathbf{Y}}$
\end_inset

 are noise corrupted.
 Thus, its obvious that we do not want our models to learn from noise corrupted
 data.
 As a result, we would like to randomly sample 
\begin_inset Formula $Q_{u}$
\end_inset

 subsets from 
\begin_inset Formula $\tilde{\mathbf{X}}$
\end_inset

 with their labels from 
\begin_inset Formula $\tilde{\mathbf{Y}}$
\end_inset

.
 Lets rewrite it in more mathematical form.
 
\end_layout

\begin_layout Standard
Assuming 
\begin_inset Formula $N_{u}$
\end_inset

 is amount of samples in 
\begin_inset Formula $\mathbf{X}_{u}$
\end_inset

.
 Let 
\begin_inset Formula $Z_{u}=\{z_{1},...,z_{N_{u}^{sub}}\}$
\end_inset

, where 
\begin_inset Formula $N_{u}^{sub}\subset N_{u}$
\end_inset

 and 
\begin_inset Formula $\forall k,l\in N_{u}^{sub},\ z_{k},z_{l}\in J_{u},\ z_{k}\neq z_{l}$
\end_inset

.
 Next step is selection of indices that will form set 
\begin_inset Formula $Z_{u}$
\end_inset

.
 Let 
\begin_inset Formula $\forall k\in N_{u}^{sub},\ z_{k}\sim U(J_{u})$
\end_inset

.
 under condition that we don't want to have duplicate indices in 
\begin_inset Formula $Z_{u}$
\end_inset

.
 Let 
\begin_inset Formula $\mathbf{X}_{Z_{u}},\ \mathbf{Y}_{Z_{u}}$
\end_inset

 are defined as restriction of sets 
\begin_inset Formula $\mathbf{X}_{u},\ \mathbf{Y}_{u}$
\end_inset

 on indices from 
\begin_inset Formula $Z_{u}$
\end_inset

.
 As a result we can approximate 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\pi_{u}^{*}=p_{u}(\theta_{u}|\mathbf{X}_{Z_{u}},\mathbf{Y}_{Z_{u}}).\label{eq:prior_distribution_approximation_training_set_sampling}
\end{equation}

\end_inset

Sampling from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_approximation_training_set_sampling"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is very simple.
 After training the model using 
\begin_inset Formula $\mathbf{X}_{Z_{u}}$
\end_inset

 and 
\begin_inset Formula $Y_{Z_{u}}$
\end_inset

 vector of model parameters will represent a single sample from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
SGLD
\end_layout

\begin_layout Standard
Unlike previous section method, SGLD sampling is designed only for neural
 networks based classifiers.
 SGLD modifies Neural Network learning algorithm by adding noise in Stochastic
 Gradient descent.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
FINISHED HERE
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
DENFI
\end_layout

\begin_layout Standard
TBD
\end_layout

\begin_layout Subsection
Active Learning Loss Function
\begin_inset CommandInset label
LatexCommand label
name "subsec:Active-Learning-Loss"

\end_inset


\end_layout

\begin_layout Subsubsection
Entropy Based Active Learning Loss
\end_layout

\begin_layout Standard
First approach of defining Active Learning loss function is negative entropy.
 Basing of the formal definition of the entropy we can write it as 
\begin_inset Formula 
\begin{equation}
-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u})=\sum_{r=1}^{R}\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u})\log\big(\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u})\big),\label{eq:entropy}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{y}_{r}$
\end_inset

 is 
\begin_inset Formula $r-$
\end_inset

th element of the output estimate 
\begin_inset Formula $\mathbf{\hat{y}}$
\end_inset

 and 
\begin_inset Formula $\theta$
\end_inset

 is a vector of parameters for specific model.
 As done in Passive Learning sections we want to find expected loss based
 on entropy function.
 
\end_layout

\begin_layout Standard
With the usage of previous knowledge we can derive expected entropy loss
 as 
\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi_{u}^{*}}L^{*} & =\int_{\varTheta_{u}}-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u})p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})d\theta_{u}\nonumber \\
 & =\int_{\varTheta_{u}}-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u})\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\delta(\theta_{u}-\theta_{u,q})d\theta_{u}\nonumber \\
 & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u,q}))\nonumber \\
 & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\sum_{r=1}^{R}\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u,q})\log\big(\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u,q})\big).\label{eq:expected_entropy_loss}
\end{align}

\end_inset

As a result, minimization of given expected loss will lead us to a sample
 with the highest entropy.
\end_layout

\begin_layout Subsubsection
False Positive Sampling Loss
\end_layout

\begin_layout Standard
TBD
\end_layout

\begin_layout Subsection
Active Learning for Random Forests Algorithm
\end_layout

\begin_layout Standard
In Supervised Learning section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:random_forest_supervised_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we have derived that 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 for Random Forest (RF) algorithm is written as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_class_prob_for_random_forest"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Active Learning algorithm requires distribution over the parameters of
 the Random Forest algorithm.
 We will solve this problem the way that we will get samples from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 and then approximate probability distribution as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
In order to estimate samples from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 we define Ensemble Random Forest Algorithm.
 That means that we will use 
\begin_inset Formula $Q_{u}$
\end_inset

 Random Forests in each step of Active Learning algorithm.
 In this case parameters of each 
\begin_inset Formula $\mathbf{\hat{y}}_{q_{u}}$
\end_inset

, where 
\begin_inset Formula $\mathbf{\hat{y}}$
\end_inset

 is Random Forest one-hot represented output and 
\begin_inset Formula $q_{u}\in\{1_{u},...,Q_{u}\}$
\end_inset

, will be iid.
 Of-course RF algorithm is already ensemble of decision trees but in this
 case we create ensemble algorithm from ensemble algorithms.
 Previously we defined for each decision tree 
\begin_inset Formula $T_{l}$
\end_inset

 where 
\begin_inset Formula $l\in\{1,...,L\}$
\end_inset

, that 
\begin_inset Formula $\mathbf{w}_{l}$
\end_inset

 is a vector of parameters of 
\begin_inset Formula $l-$
\end_inset

th decision tree.
 Thus, let 
\begin_inset Formula $\mathbf{W}=(\mathbf{w}_{1},...,\mathbf{w}_{L})$
\end_inset

 is set of parameters of specific Random Forest algorithm.
 Thus, for 
\begin_inset Formula $\mathbf{\hat{y}}_{q_{u}}$
\end_inset

 we have 
\begin_inset Formula $\mathbf{W}_{q_{u}}=(\mathbf{w}_{q_{u},1},...,\mathbf{w}_{q_{u},L})$
\end_inset

.
 As a result we can approximate 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\pi_{u}^{*}=\frac{1}{Q_{u}}\sum_{q_{u}=1}^{Q_{u}}\delta(\mathbf{W}-\mathbf{W}_{q_{u}}).\label{eq:Random_forest_ensemble_parameters_distribution}
\end{equation}

\end_inset

Another important point is to define 
\begin_inset Formula $\hat{\mathbf{y}}_{q_{u}}=\hat{\mathbf{y}}(\mathbf{x}_{j_{u}},\mathbf{W}_{q_{u}})$
\end_inset

.
 We formally define each element 
\begin_inset Formula $\hat{y}_{r,q_{u}}$
\end_inset

 of vector 
\begin_inset Formula $\hat{\mathbf{y}}_{q_{u}}$
\end_inset

 as 
\begin_inset Formula 
\[
\hat{y}_{r,q_{u}}=\frac{1}{L}\sum_{l=1}^{L}T_{r}(\mathbf{x}_{j_{u}},\mathbf{w}_{q_{u},l}),
\]

\end_inset

where 
\begin_inset Formula $r$
\end_inset

 is 
\begin_inset Formula $r-$
\end_inset

th class 
\begin_inset Formula $T_{r}$
\end_inset

 is 
\begin_inset Formula $r-$
\end_inset

th element of a decision tree vector.
\end_layout

\begin_layout Standard
With the usage of the theory from section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Active-Learning-Loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can write expected loss for Active Learning Random Forest Algorithm
 as 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi_{u}^{*}}L_{u}^{*}=\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\sum_{r=1}^{R}\hat{y}_{r}(\mathbf{x}_{j_{u}},\mathbf{W}_{q_{u}})\log\big(\hat{y}_{r}(\mathbf{x}_{j_{u}},\mathbf{W}_{q_{u}})\big).\label{eq:Expected_entropy_loss_random_forest_active_learning}
\end{equation}

\end_inset


\end_layout

\begin_layout Chapter
Natural Language Processing Theory
\end_layout

\begin_layout Section
Text Representation
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "liddy2001natural"
literal "false"

\end_inset

, Natural Language Processing (NLP) is a theoretically motivated range of
 computational techniques for analyzing and representing naturally occurring
 texts at one or more levels of linguistic analysis for the purpose of achieving
 human-like language processing for a range of tasks or applications.
 
\end_layout

\begin_layout Standard
In this work we are focused on two techniques such as TF-IDF 
\begin_inset CommandInset citation
LatexCommand cite
key "rajaraman2011mining"
literal "false"

\end_inset

 and Fast Text Word Embeddings 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2018advances"
literal "false"

\end_inset

.
 These methods are used for representation of text in a mathematical form
 (vectors, matrices).
 Even though TF-IDF is quite old method for text representation, it is still
 widely used.
 However, primary method, that used in the theses is Fast Text Word Embeddings.
 In this project we are working with text documents (articles and tweets)
 and their labels.
 In the beginning of chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Introduction-to-Decision"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we defined value 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 as text features vector.
 By features vector we mean any kind of text encoding (TF-IDF, Fast Text
 Word Embedding, etc..).
 
\end_layout

\begin_layout Subsection
TF-IDF 
\end_layout

\begin_layout Standard
Term Frequency - Inverse Document Frequency (TF-IDF) is extremely powerful
 tool.
 This text encoding tool is quite simple and powerful.
 Method's advantage is its popularity.
 Plenty of packages in different programming languages have implementations
 of this algorithm.
 As mentioned in the name of this method, it is composed from two parts
 Term Frequency and Inverse Document Frequency.
 Term Frequency is defined as 
\begin_inset Formula 
\[
TF(t,d)=\frac{f_{t,d}}{\sum_{t^{\prime}}f_{t^{\prime},d},},
\]

\end_inset

where 
\begin_inset Formula $f_{t,d}$
\end_inset

 is number of times of word 
\begin_inset Formula $t$
\end_inset

 in a document 
\begin_inset Formula $d$
\end_inset

.
 Inverse Document Frequency is defined as 
\begin_inset Formula 
\[
IDF(t,d)=\log\frac{|D|}{|\{d\in D:t\in d\}|},
\]

\end_inset

where numerator stand for total number of documents in the corpus and denominato
r is number of documents where the term 
\begin_inset Formula $t$
\end_inset

 appears.
 We are assuming using only words that from corpus 
\begin_inset Formula $D$
\end_inset

.
 Thus, the denominator is always greater than zero.
 
\end_layout

\begin_layout Standard
Finally, 
\begin_inset Formula 
\[
TF-IDF(t,d)=TF(t,d)\cdot IDF(t,d).
\]

\end_inset


\series bold
N.B
\end_layout

\begin_layout Subsubsection
TF-IDF and Information Theory
\end_layout

\begin_layout Standard
In this part is shown the connection of TF-IDF to Information theory 
\begin_inset CommandInset citation
LatexCommand cite
key "aizawa2003information"
literal "false"

\end_inset

.
 Lets first take a look on documents' entropy given word 
\begin_inset Formula $t$
\end_inset

,
\begin_inset Formula 
\begin{align}
H(D|T=t) & =-\sum_{d}p(d|t)\log p(d|t)\nonumber \\
 & =\log\frac{1}{|\{d\in D:t\in D\}|}\nonumber \\
 & =-\log\frac{|\{d\in D:t\in D\}|}{D|}+\log|D|\nonumber \\
 & =-IDF(t,d)+\log|D|,\label{eq:Documents_entropy_given_word}
\end{align}

\end_inset

where 
\begin_inset Formula $D$
\end_inset

 is a documents' random variable and 
\begin_inset Formula $T$
\end_inset

 is words' random variable.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Documents_entropy_given_word"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is correct under condition that we have no duplicate documents in the text
 corpus.
 Next step is to derive an equation of mutual information of documents and
 words as follows 
\begin_inset Formula 
\begin{align}
M(D,T) & =H(D)-H(D|T)\nonumber \\
 & =-\sum_{d}p(d)\log p(d)-\sum H(D|T=t)\cdot p(t)_{t}\nonumber \\
 & =\sum_{t}p(t)\cdot\Big(\log\frac{1}{|D|}+IDF(t,d)-\log|D|\Big)\nonumber \\
 & =\sum_{t}p(t)\cdot IDF(t,d)\nonumber \\
 & =\sum_{t,d}p(t|d)\cdot p(d)\cdot IDF(t,d)\nonumber \\
 & =\frac{1}{|D|}\sum_{t,d}TF(t,d)\cdot IDF(t,d).\label{eq:tf_idf_mutual_information}
\end{align}

\end_inset

As seen from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:tf_idf_mutual_information"
plural "false"
caps "false"
noprefix "false"

\end_inset

 TF-IDF has really good explanatory definition based on Information Theory.
 As a result, it is one more advantage of this method usage.
 However, here is one big disadvantage that can be very crucial.
 The higher amount of words is, the bigger and sparser vectors, that represent
 each document, will be.
\end_layout

\begin_layout Subsection
Fast Text and CBOW Word Embeddings
\end_layout

\begin_layout Standard
Term word embedding means a set of language modeling and feature learning
 techniques in natural language processing where words or phrases from the
 vocabulary are mapped to vectors of real numbers.
 Nowadays exist plenty of word embedding methods based on neural networks
 and co-occurrence matrices.
 Word embeddings are used as pretrained models.
 Words' encoding is used in order to encode the text and then text encoding
 is used for different purposes such as classification, clustering, etc..
 
\end_layout

\begin_layout Standard
The principle of word embeddings based on neural networks is explained in
 this section.
 We decided to describe Continuous Bag of Words Model (CBOW), because Fast
 Text word embeddings model is a modification of this method and CBOW covers
 all main theoretical aspects.
\end_layout

\begin_layout Subsubsection
CBOW Word Embeddings
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
We would like to treat text {"The", "cat", ’over", "the’, "puddle"} as a
 context and from these words, be able to predict or generate the center
 word "jumped".
 This type of model is a Continuous Bag of Words (CBOW) Model.
 Before we continue with more theoretical part, it is good to mention that
 mathematical notation defined here is only used for this section and has
 no common with the same names of the variables that were defined in the
 beginning of this theses.
 First, we want to set up our known parameters.
 Let the known parameters in our model be the sentence represented by one-hot
 word vectors.
 The input one hot vectors or context we will represent with an 
\begin_inset Formula ${\bf x}^{(c)}$
\end_inset

.
 And the output as 
\begin_inset Formula ${\bf y}^{(c)}$
\end_inset

 and in the CBOW model, since we only have one output, so we just call this
 
\begin_inset Formula ${\bf y}$
\end_inset

 which is the one hot vector of the known center word.
 Now let’s define our unknowns in our model.
 We create two matrices, 
\begin_inset Formula ${\bf \mathcal{V}}\in\mathbb{R}^{n×|V|}$
\end_inset

 and 
\begin_inset Formula $\mathcal{U}\in\mathbb{R}^{|V|×n}$
\end_inset

.
 Where 
\begin_inset Formula $n$
\end_inset

 is an arbitrary size which defines the size of our embedding space.
 
\begin_inset Formula ${\bf \mathcal{V}}$
\end_inset

 is the input word matrix such that the 
\begin_inset Formula $i-$
\end_inset

th column of 
\begin_inset Formula ${\bf \mathcal{V}}$
\end_inset

 is the 
\begin_inset Formula $n-$
\end_inset

dimensional embedded vector for word 
\begin_inset Formula $w_{i}$
\end_inset

 when it is an input to this model.
 We denote this 
\begin_inset Formula $n×1$
\end_inset

 vector as 
\begin_inset Formula ${\bf v}_{i}$
\end_inset

.
 Similarly, 
\begin_inset Formula ${\cal U}$
\end_inset

 is the output word matrix.
 The 
\begin_inset Formula $j-$
\end_inset

th row of 
\begin_inset Formula ${\cal U}$
\end_inset

 is an 
\begin_inset Formula $n-$
\end_inset

dimensional embedded vector for word 
\begin_inset Formula $w_{j}$
\end_inset

 when it is an output of the model.
 We denote this row of 
\begin_inset Formula ${\cal U}$
\end_inset

 as 
\begin_inset Formula ${\bf u}_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
For this method sequence of actions can be written as follows:
\end_layout

\begin_layout Itemize
We generate our one hot word vectors 
\begin_inset Formula $({\bf {\bf x}}^{(c−m)},...,{\bf {\bf x}}^{(c−1)},{\bf {\bf x}}^{(c+1)},...,{\bf {\bf x}}^{(c+m)})$
\end_inset

 for the input context of size 
\begin_inset Formula $2m$
\end_inset

.
\end_layout

\begin_layout Itemize
We get our embedded word vectors for the context 
\begin_inset Formula $({\bf v}_{c−m}={\cal V}{\bf x}^{(c−m)},{\bf v}_{c−m+1}={\cal V}{\bf x}_{(c−m+1)},...,{\bf v}_{c+m}={\cal V}{\bf x}_{(c+m)})$
\end_inset


\end_layout

\begin_layout Itemize
Average these vectors to get 
\begin_inset Formula $\tilde{{\bf v}}=\frac{{\bf v}_{c\text{−}m}+{\bf v}_{c\text{−}m+1}+...+{\bf v}_{c\text{+}m}}{2m}$
\end_inset


\end_layout

\begin_layout Itemize
Generate a score vector 
\begin_inset Formula ${\bf z}={\cal U}\tilde{{\bf v}}$
\end_inset


\end_layout

\begin_layout Itemize
Turn the scores into probabilities 
\begin_inset Formula 
\begin{equation}
\hat{{\bf y}}=\mathrm{softmax}({\bf z})\label{eq:softmax_cbow}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
We desire our probabilities generated, 
\begin_inset Formula $\hat{{\bf y}}$
\end_inset

, to match the true probabilities, 
\begin_inset Formula ${\bf y}$
\end_inset

, which also happens to be the one hot vector of the actual word.
\end_layout

\begin_layout Standard
Described method can be interpreted as a feed forward neural network with
 one hidden layer that do not uses activation function.
 As a loss function for this algorithm can be chosen cross-entropy loss
 function 
\begin_inset Formula 
\begin{equation}
L=\sum_{i=1}^{|V|}{\bf y_{i}\log({\bf \hat{y}}_{i})}\label{eq:cross_entropy_cbow}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{\hat{y}}$
\end_inset

 is sofmatx (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:softmax_cbow"
plural "false"
caps "false"
noprefix "false"

\end_inset

) function.
\end_layout

\begin_layout Subsubsection
Fast Text Word Embeddings
\end_layout

\begin_layout Standard
As mentioned previously, Fast Text method is a CBOW modification.
 Main modification is that Fast Text is taking into account not only words
 but also suffixes of words.
 The words are splitted into suffixes and as a result they can handle understand
ing of the context better.
\backslash

\end_layout

\begin_layout Standard
In this theses we used pretrained Fast Text models 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2018advances"
literal "false"

\end_inset

 consisting of 1 million word vectors trained on Wikipedia 2017, UMBC webbase
 corpus and statmt.org news dataset (16B tokens).
\end_layout

\begin_layout Chapter
Results Validation Theory
\end_layout

\begin_layout Standard
When the models are implemented and trained we have to compare them.
 This part is very important because we want to define such metrics that
 will not be biased and which will have high discriminability.
 In this project experiments are separated into two parts.
 First part is supervised classification with big amount of data.
 This is done for understating what is the maximal upper bound of specific
 classifiers.
 These upper bounds are used as maximums which our active learning algorithms
 must be converging to.
 
\end_layout

\begin_layout Section
Receiving Operating Curve metric
\end_layout

\begin_layout Standard
In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Introduction-to-Decision"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we mentioned that we are limiting our problem only on binary classification.
 Plenty of metrics such as recall, accuracy, precision, etc.
 exists for binary classification.
 However we decided to find a metic that is able to unify all metrics discussed
 before and do not underperform each of them.
 For these purposes we chose Receiving Operating Curve metric.
 ROC visualizes the tradeoff between true positive rate (TPR) and false
 positive rate (FPR).
 This means that for every threshold, we are able to calculate TPR and FPR
 and plot it on one figure.
 
\end_layout

\begin_layout Standard
We are working with balanced datasets.
 Thus, there is no problem in using ROC metrics.
 ROC metric is also very good when we care equally about positive and negative
 class.
 Another advantage is that if we notice small changes in ROC it will not
 result in big changes in other binary classification metrics.
\end_layout

\begin_layout Standard
Receiving Operating Curve lets us to calculate area under the curve (AUC).
 The probabilistic interpretation of ROC score is that if a positive case
 and a negative case are chosen randomly, the probability that the positive
 case outranks the negative case according to the classifier is given by
 the AUC.
 
\end_layout

\begin_layout Section
Supervised Learning Results Validation
\end_layout

\begin_layout Standard
As mentioned above, supervised learning results are used as maximal upper
 bound of specific classifiers.
 In order to make results statistically valid we used k-fold cross validation.
 For each batch from k-fold cross validation we calculated both ROC and
 AUC.
 As an output result of a classifier performance we calculated mean value
 through all ROC and AUC results.
 All results are calculated with respect to balanced data classes.
\end_layout

\begin_layout Section
Active Learning Results Validation
\end_layout

\begin_layout Standard
Active learning model evolution is based on supervised learning algorithms
 that are sequentially retrained.
 Thus, we are not able to display ROC for active learning algorithms because
 amount of results is too big.
 We decided to aggregate results and display evolution of AUC metric for
 each step of active learning sequence.
 AUC sequences can be well compared between different classifiers.
 Another aspect of data validation is making the results statistically significa
nt.
 We are not able to use k-fold cross validation for active learning algorithms.
 Therefore we active learning algorithm 
\begin_inset Formula $H\in\mathbb{N}$
\end_inset

 times.
 Due to the fact of random initializations, we are able to determine uncertainty
 bounds that are calculated as standard deviations to mean value.
 
\end_layout

\begin_layout Chapter
Data
\end_layout

\begin_layout Chapter
Implementation Architecture
\end_layout

\begin_layout Chapter
Passive Learning Classification as Upper Bound Threshold
\end_layout

\begin_layout Chapter
Active Learning Classification
\end_layout

\begin_layout Chapter*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Text of the conclusion\SpecialChar ldots

\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
