#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 0.8cm
\headsep 1cm
\footskip 0.5cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
documentdate{July 8, 2019}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%
\backslash
def
\backslash
documentdate{
\backslash
today}
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{empty}
\end_layout

\begin_layout Plain Layout

{
\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align block
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/cvut.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "60line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\align center

\shape smallcaps
\size large
Czech Technical University in Prague
\shape default

\begin_inset Newline newline
\end_inset

Faculty of Nuclear Sciences and Physical Engineering
\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/fjfi.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 3cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Active Learning for Text Classification
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Aktivní učení pro klasifikaci textů
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\size large
Masters's Degree Project
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ends the centered part (the required new paragraph before "}" is inserted
 by \SpecialChar LyX
 as "}" is on a separate line.)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Author: 
\series bold
Marko Sahan
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Supervisor: 
\series bold
doc.
 Ing.
 Václav Šmídl, Ph.D.
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Academic
\begin_inset space ~
\end_inset

year: 2019/2020
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Final dummy paragraph.
 Its function is to bear the page break flag
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset Newpage newpage
\end_inset

zadani
\begin_inset Newpage newpage
\end_inset

zadani
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Acknowledgment:
\end_layout

\begin_layout Standard
\noindent
I would like to thank 
\series bold
doc.
 Ing.
 Václav Šmídl, Ph.D.

\series default
 for his expert guidance and express my gratitude for his language assistance.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Author's declaration:
\end_layout

\begin_layout Standard
\noindent
I declare that this Masters's Degree Project is entirely my own work and
 I have listed all the used sources in the bibliography.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
Prague, June 17, 2020
\begin_inset space \hfill{}
\end_inset

Marko Sahan
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
\lang czech
Název práce:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\series bold
\lang czech
Aktivní učení pro klasifikaci textů
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Autor:
\emph default
 Marko Sahan
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Obor:
\emph default
 Aplikované matematicko-stochastické metody
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Druh práce:
\emph default
 Diplomová práce
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Vedoucí práce:
\emph default
 
\series bold
doc.
 Ing.
 Václav Šmídl, Ph.D.
\series default
, Ústav teorie informace a automatizace
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Abstrakt:
\emph default
 Modely strojového učení pro klasifikaci jsou založené na učení parametrů
 black box modelu, které popisují vztah mezi vzorky dat a jejích třídou.
 Proces sběru dat a jejích labelů pro účely trénování modelu může být komplikova
ný a drahý.
 Množina dat je v mnoha případech větší než množina dostupných labelů, ale
 našim předpokladem je to, že nové labely můžou být obdržené prostřednictvím
 dotazu anotátorovi.
 Aktivní učení je proces výběru takových dat pro anotování, které povedou
 ke zvýšení diskriminability datasetu.
 Mnoho různých metod aktivního učení v mnoha různých odvětvích bylo navrženo
 pro úlohy v nichž se používá učení s učitelem.
 V tomto projektu jsou popsané a ukázané různé metody aktivního učení pro
 klasifikaci textů.
 Navíc jsou porovnávané už existující black box modely a jejích reprezentace
 neurčitosti.
 Modely aktivního učení jsou formalizované pomocí teorie rozhodování, kde
 rozhodnutím je výběr dat bez labelů pro získávání anotace a neurčitost
 je v parametrech klasifikátorů.
 Entropie predikce klasifikátoru je vybrána jako očekávaná ztrátová funkce
 pro rozhodovací úlohu.
 Modely hlubokého učení dosáhli state-of-the-art výsledků v různých odvětvích
 zpracování přirozeného jazyka a také v klasifikaci textu.
 Kombinace aktivního učení pro výběr dat a navržené reprezentace neurčitosti
 založené na ensemblech hlubokých neuronových sítí dosáhla výrazně lepších
 výsledků než strategie náhodného výběru nebo aktivní učení s alternativní
 reprezentací neurčitosti.
 
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Klíčová slova:
\emph default
 Aktivní učení, ensemble modely hlubokého učení, ensemble modely neuronových
 sítí, klasifikace textu, teorie rozhodování, zpracování přirozeného jazyka.
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
Title:
\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\series bold
Active learning for text classification
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Author:
\emph default
 Marko Sahan
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Abstract:
\emph default
 Machine learning approach to classification is based on learning parameters
 of black box model describing relation between the recorded data samples
 and their class labels.
 The process of data labels collection for the purposes of model training
 can be complex and costly.
 Therefore, the number of data record is often much higher than the number
 of labels, but the labels can be obtained by querying an annotator.
 Active learning is a process of selection of unlabeled data records for
 which knowledge of the label would bring the highest discriminability of
 the dataset.
 Various methods for active learning have been proposed in many different
 fields that use supervised learning models.
 In this project, we study suitability of various approaches for active
 learning of a text classification problem.
 We compare existing black box classifiers, and representations of their
 uncertainty.
 We formalize active learning using decision theory under uncertainty where
 the decision is which unlabeled data to select for annotation and the uncertain
ty is in the parameters of the classifier.
 The expected loss function of the decision making is chosen as entropy
 of the classifier predictions.
 Neural networks have showed state-of-the-art performance in different natural
 language processing tasks, including text classification.
 The proposed combination of active learning data selection and uncertainty
 representation, based on deep learning ensembles algorithm achieves significant
ly better results than random data selection strategy or active learning
 with other types of uncertainty represenattion.
 
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Key words:
\emph default
 Active learning, decision theory, deep learning ensembles, natural language
 processing, neural network ensembles, text classification.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Notation
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="23" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="left" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Symbol
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Definition
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{x}\in\mathcal{X}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vector features, i.e.
 the instance
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{y}\in\mathcal{Y}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
one hot encoded label of a specific instance
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{X}=\{\mathbf{x}_{1},\ldots\mathbf{x}_{M}\}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
set of available instances
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{Y}=\{\mathbf{y}_{1},\dots,\mathbf{y}_{M}\}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
set of labels that can be provided by an annotator
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\tilde{\mathbf{X}}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
set of training instances
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\tilde{\mathbf{Y}}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
set of training labels
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $[x_{1},x_{2},...,x_{S}]^{T}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
transposed vector 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tuple of elements 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\{\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{R}\}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
set of vectors 
\begin_inset Formula $\boldsymbol{x}_{1},\ldots\boldsymbol{x}_{R}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $a\in{\cal A}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
action from a set of all possible actions
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\theta\in\varTheta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
decision theory uncertainty parameter
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $L$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
loss function
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\pi^{*}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
probability density function of variable 
\begin_inset Formula $\theta$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
conditional probability density function of label 
\begin_inset Formula $\mathbf{y}$
\end_inset

 given instance 
\begin_inset Formula $\mathbf{x}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{\hat{y}}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
expected value of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 given 
\begin_inset Formula $\mathbf{x},\mathbf{X}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\mathbf{w}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vector of parameters of a classifier (e.g.
 SVM or decision tree)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $b$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SVM bias (scalar) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{W}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tuple of parameters of a classifier (e.g.
 Naive Bayes, random forest
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
or neural network (single layer))
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\mathbf{b}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vector of neural network single layer biases
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\Omega$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tuple of neural network parameters (all weights and biases)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\delta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dirac delta function
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Chapter*
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Introduction}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Active learning strategy lets the machine learning models iteratively and
 strategically query the labels of some instances for reducing human labeling
 efforts.
 This project shows how it is possible to connect active learning and text
 data.
 People have already been solving the same problem for anomaly detection
 
\begin_inset CommandInset citation
LatexCommand cite
key "das2018active"
literal "false"

\end_inset

, image processing 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

, etc.
 
\end_layout

\begin_layout Standard
If we take a look at the modern approach of automating the labeling process
 of a huge amount of unlabeled data, it is not optimal.
 People are randomly choosing unlabeled text data.
 These data are annotated by the subject matter experts, and used for training
 and testing the models.
 If the model performance is weak after the training, more text documents
 are selected and annotated.
 This approach is costly because nobody knows how many text documents must
 be selected to have good model scores.
 Our active learning strategy proposes a selection of unlabeled text data
 that the model is not certain about.
 Unlabeled text data are given to a subject matter expert to provide the
 labels.
 Discussed problem was introduced almost two decades ago.
 Some active learning approaches for text classification dates back to 2001
 
\begin_inset CommandInset citation
LatexCommand cite
key "tong2001support"
literal "false"

\end_inset

, where are shown different querying strategies, and superiority of results
 of the active learning over random sampling strategies is demonstarted.
 There is no doubt that active learning strategy brings a lot of advantages.
 First of all, we are able to start with lower amount of training data,
 and iteratively extend the dataset.
 The dataset is extended using the data, which the model is not certain
 about.
 In this work, we are extending our dataset with only one sample per active
 learning iteration.
 However, it was also shown that the strategies, which sample batches with
 more than one sample, also perform good results 
\begin_inset CommandInset citation
LatexCommand cite
key "an2018deep"
literal "false"

\end_inset

.
 Thus, based on the active learning approach, the model will get much more
 information from non-randomly chosen text samples.
\end_layout

\begin_layout Standard
The project describes different algorithms formulation with respect to decision
 theory, and then the connection of all the methods to active learning theory.
 The main focus of this work is on deep neural networks and ensemble models
 of their uncertainty.
 It was shown in 
\begin_inset CommandInset citation
LatexCommand cite
key "snoek2019can"
literal "false"

\end_inset

 that ensemble deep learning algorithms give the best performance both for
 text and image processing data.
 Plenty of alternative models for active learning for text classifications
 exits such 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

, with use of acquisition functions and dropout, for uncertainty representation
 e.g.
 named entity recognition 
\begin_inset CommandInset citation
LatexCommand cite
key "shen2017deep"
literal "false"

\end_inset

 and text classification 
\begin_inset CommandInset citation
LatexCommand cite
key "lowell2019practical"
literal "false"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "burkhardt2018semisupervised"
literal "false"

\end_inset

.
 We will investigate if the ensembles outperform dropout uncertainty representat
ion for text data, as it was shown in 
\begin_inset CommandInset citation
LatexCommand cite
key "lakshminarayanan2017simple"
literal "false"

\end_inset

 for image classification.
 
\end_layout

\begin_layout Standard
This project also provides the link to Python implementation of active learning
 algorithms and a comparison of different results gathered with respect
 to different data.
 We believe, that the active learning approach is able to significantly
 reduce the amount of time and expenses needed for automating the text labeling
 process.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Articles
\end_layout

\begin_layout Plain Layout
Ensembles
\end_layout

\begin_layout Plain Layout
1.
 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
 (Balaji Lakshminarayanan Alexander Pritzel Charles Blundell) - Nice approach
 of adversarial component in training ensembles.
 They showed how their approach of ensembles outperforms dropout technique.
 The results were shown on images MNIST, SVHN and ImageNet and toy problems.
\end_layout

\begin_layout Plain Layout
2.
 Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty
 Under Dataset Shift (Yaniv Ovadia Balaji Lakshminarayanan Sebastian Nowozin)
 - Different methods for uncertainty representation are compared between
 each other.
 All these methods were testes on text data and image processing data.
 It turned out that ensembles showed the best performace with repect to
 all other methods.
 
\end_layout

\begin_layout Plain Layout
Active Learning
\end_layout

\begin_layout Plain Layout
1.
 Active Anomaly Detection via Ensembles (Shubhomoy Das, Md Rakibul Islam,
 Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa) - Anomaly detection
 use case.
 They showed how it is possible to apply the weights for each ensemble where
 the weights are based on a feedback from an annotator.
 Nice approach of active learning on rescaling weights of each ensemble.
\end_layout

\begin_layout Plain Layout
2.
 Deep Bayesian Active Learning with Image Data (Yarin Gal, Riashat Islam,
 Zoubin Ghahramani ) - Active learning classification on images with respect
 to different acquisition functions.
 Good examples of which acquisition function can be used.
\end_layout

\begin_layout Plain Layout
Active Learning with texts
\end_layout

\begin_layout Plain Layout
1.
 Semi-Supervised Bayesian Active Learning for Text Classification (Sophie
 Burkhardt, Julia Siekiera, Stefan Kramer) - They are doing exactly the
 same thing that I do but with the usage of drop out based approach and
 ayes-by-Backprop (BBB) algorithm.
 The results are not cool at all.
 Mine are better
\end_layout

\begin_layout Plain Layout
2.
 Practical Obstacles to Deploying Active Learning (David Lowell, Zachary
 C.
 Lipton, Byron C.
 Wallace) - Nice try with active learning.
 Very poor results.
 They used LSTM, SVM and CNN for active learning.
 They also used dropout in their work.
\end_layout

\begin_layout Plain Layout
3.
 Deep active learning for named entity recognition (Yanyao Shen, Hyokun
 Yun, Zachary C.
 Lipton, Yakov Kronrod, Animashree Anandkumar) - Dropout based active learning.
 They are also trying to reduce amount of the training data for NER models.
 They also consider sampling according to the measure of uncertainty proposed
 by Gal et al.
 (2017).
\end_layout

\begin_layout Plain Layout
4.
 Support Vector Machine Active Learning with Applications to Text Classification
 (Simon Tong, Daphne Koller) - The approach of active learning method for
 text classification that comes from 2001.
 They go through three techniques that show different queuing strategy.
 The results are better than in case of random sampling.
 However, no uncertainty was measured there.
 (Non bayesian way of querying).
\end_layout

\begin_layout Plain Layout
5.
 Deep Active Learning for Text Classification (Bang An, Wenjun Wu, Huimin
 Han) - SVM and RNN (LSTM) multiclass text classification.
 No bayesian approach of neural networks.
 Trying to sample not 1 sample but batch.
 Their approach is only based on acquisition function.
 The uncertainty is measured only through output labels based on one set
 of parameters (point-wise estimate) 
\end_layout

\begin_layout Plain Layout
Techniques 
\end_layout

\begin_layout Plain Layout
1.
 Shannon, Claude Elwood.
 A mathematical theory of com- munication.
 Bell System Technical Journal, 27(3):379– 423, 1948.
 - Citation to entropy
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
{\text{argmax}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Chapter
Introduction to Decision Theory
\begin_inset CommandInset label
LatexCommand label
name "chap:Introduction-to-Decision"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The process of decision making is defined as a selection of the optimal
 action from a set of possibilities that can be applied at some operating
 conditions.
 The criteria of optimality are formalized by a loss function.
 The process of selection of the optimal action is, thus, formalized as
 the optimization problem.
 However, the operating conditions are often not known exactly, since we
 have incomplete information about them.
 Therefore, we will use the theory of decision making under uncertainty
 
\begin_inset CommandInset citation
LatexCommand cite
key "Berger:1327974"
literal "false"

\end_inset

, where the uncertainty is represented by probability density functions.
 We will now briefly review the theoretical background.
\end_layout

\begin_layout Section
Decision Theory
\end_layout

\begin_layout Standard
The theory of decision making has three basic elements: i) the set of possible
 actions 
\begin_inset Formula $\mathcal{A}$
\end_inset

 from which we should select an optimal action 
\begin_inset Formula $a^{*}\in\mathcal{A}$
\end_inset

, ii) the vector 
\begin_inset Formula $\theta\in\varTheta$
\end_inset

 defining operating conditions under which we make the decision, where 
\begin_inset Formula $\varTheta$
\end_inset

 is the parameter space, iii) the loss function 
\begin_inset Formula 
\begin{align}
L & =L(\theta,a),\label{eq:loss_func}
\end{align}

\end_inset

that defines our preference of the action, in the sense that action 
\begin_inset Formula $a$
\end_inset

 which has the lowest value from the action set of the loss function is
 preferred.
 For complete knowledge of the operating conditions 
\begin_inset Formula $\theta$
\end_inset

, the task is turned into simple optimization of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 However, with incomplete information, we have to consider a range of possible
 states 
\begin_inset Formula $\theta$
\end_inset

.
 The theory of decision making under uncertainty 
\begin_inset CommandInset citation
LatexCommand cite
key "Berger:1327974"
literal "false"

\end_inset

 defines the expected loss function that takes into account the uncertainty.
\end_layout

\begin_layout Definition
\begin_inset CommandInset citation
LatexCommand cite
key "Berger:1327974"
literal "false"

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "def:expected_loss"

\end_inset

If 
\begin_inset Formula $\pi^{*}(\theta)$
\end_inset

 is the probability distribution of 
\begin_inset Formula $\theta$
\end_inset

 at the time of decision making, the 
\shape slanted
Bayesian expected loss
\shape default
 of an action 
\begin_inset Formula $a$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\rho(\pi^{*},a)=\mathbb{E}_{\pi^{*}}[L(\theta,a)]=\int_{\varTheta}L(\theta,a)\pi^{*}d\theta.\label{eq:expected_loss-1}
\end{equation}

\end_inset

Based on definition 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the optimal action is defined as the one that minimizes the expected loss:
\end_layout

\begin_layout Definition
\begin_inset Formula 
\begin{equation}
a^{*}=\argmin_{a\in{\cal A}}\mathbb{E}_{\pi^{*}}[L(\theta,a)].\label{eq:optimal_action}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The key task of the application of decision theory is the choice of the
 action space, parameter space, loss function, and method of evaluating
 the probability measure.
 In the following sections, we discuss examples of the application of the
 theory to the problem of supervised learning and active learning, respectively.
\end_layout

\begin_layout Section
Decision Theory for Supervised Learning
\end_layout

\begin_layout Standard
Supervised learning is defined as learning from the data with a known target
 value.
 Specifically, for the classification problem, the target value is the class,
 where each data point belongs.
 
\end_layout

\begin_layout Standard
We would like to commence our formal definition with the data.
 Let 
\begin_inset Formula $\mathbf{x}\in{\cal X}\subset\mathbb{R}^{n}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}=\{[0,1]^{T},[1,0]^{T}\}$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the feature vector of size 
\begin_inset Formula $n$
\end_inset

, and 
\begin_inset Formula ${\bf y}$
\end_inset

 is its label assigned to the data instance 
\begin_inset Formula $\mathbf{x}$
\end_inset

 from space 
\begin_inset Formula ${\cal X}$
\end_inset

.
 Each value from space 
\begin_inset Formula ${\cal Y}$
\end_inset

 can be represented using one hot representation, which is a vector consisting
 of ones and zeros.
 In the case of binary classification 
\begin_inset Formula $\mathbf{y}\in\{[0,1]^{T},[1,0]^{T}\}$
\end_inset

, where the first class is represented as 
\begin_inset Formula $\mathbf{y}=[1,0]^{T}$
\end_inset

 and the second class is represented as 
\begin_inset Formula $\mathbf{y}=[0,1]^{T}$
\end_inset

.
 As a good example of the previous definition, 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can be a text document (represented as a vector in order to meet the definition
 above) and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 can be its category, such as sports or comedy.
 As seen from this example, the label and the text are forming a tuple.
 In this work we are considering our data as tuples of variables 
\begin_inset Formula $(\mathbf{x},\mathbf{y})\in{\cal X\times Y}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Basing on the data definitions from the previous part, we can assume that
 
\begin_inset Formula $\mathcal{X\times Y}$
\end_inset

 is an infinite set and 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

 is a sample from this set.
 We assume, that all available data tuples are sampled independently from
 a joint probability density function 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

.
 If 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 was known, the optimal classifier 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 can be obtained by the chain rule of probability 
\begin_inset Formula 
\begin{equation}
p(\mathbf{x},\mathbf{y})=p(\mathbf{y}|\mathbf{x})p(\mathbf{x}).\label{eq: data_bayes_rule}
\end{equation}

\end_inset

However, since we do not know the analytical form of the joint probability
 distribution, we aim at selecting the best possible approximation within
 a chosen class.
 Specifically, we choose a parametric form 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},a)$
\end_inset

, where 
\begin_inset Formula $a$
\end_inset

 is the parameter to be optimized.
 
\end_layout

\begin_layout Standard
It remains to choose the form of representation of the joint probability
 distribution.
 We will consider the uncertainty 
\begin_inset Formula $\theta$
\end_inset

 to be represented by empirical distribution: 
\begin_inset Formula 
\begin{equation}
\pi^{*}=p(\mathbf{x},\mathbf{y})=\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})\label{eq:approx_of_joint_dist-1}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{x}_{i},\mathbf{y}_{i}$
\end_inset

 are elements of training set 
\begin_inset Formula $(\tilde{\mathbf{X}}\subset{\cal X},\tilde{\mathbf{Y}}\subset{\cal Y})$
\end_inset

.
 The training set is usually a subset of all available data on which the
 optimization is performed, the rest of the data is used for validation
 
\begin_inset CommandInset citation
LatexCommand cite
key "vapnik2013nature"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Decision Theory and Support Vector Machine Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_svm"

\end_inset


\end_layout

\begin_layout Standard
In this subsection, we will continue the construction of the decision theory
 on the example of Support Vector Machine (SVM) method.
 For simplicity, let us consider a linearly separable dataset.
 From the theoretical perspective, SVM constructs a hyperplane in high dimension
al space.
 In this case, our decision (action) is a hyperplane that will separate
 two classes.
 Equation of the hyperplane can be written as 
\begin_inset Formula $f(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

, where 
\begin_inset Formula $\text{\ensuremath{\mathbf{w}}\ensuremath{\in\mathbb{R}^{n}} }$
\end_inset

 is a set of hyperplane parameters and 
\begin_inset Formula $b\in\mathbb{R}$
\end_inset

 is a bias.
 As a result, action space is represented as 
\begin_inset Formula $(\mathbb{R}^{n},\mathbb{R})\mathcal{=A}$
\end_inset

 and as a consequence tuple 
\begin_inset Formula $(\ensuremath{\mathbf{w}},b)\in\mathcal{A}$
\end_inset

.
 From this knowledge, we consider the uncertainty 
\begin_inset Formula $\theta$
\end_inset

 described with (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) that meets the condition of the limitation on 
\begin_inset Formula $\varTheta=(\tilde{\mathbf{X}}\subset{\cal X},\tilde{\mathbf{Y}}\subset{\cal Y})$
\end_inset

.
 Consider loss function (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss_func"
plural "false"
caps "false"
noprefix "false"

\end_inset

) that can be written as
\begin_inset Formula 
\begin{equation}
L=L(\mathbf{x},\mathbf{y},\mathbf{w},b).\label{eq:SVM_thoer_loss}
\end{equation}

\end_inset

The following task is to understand how good is our action (hyperplane estimatio
n) with respect to the dataset.
 We can choose different types of loss functions, such as cross entropy,
 hinge loss, etc..
 The most basic approach for SVM method is the hinge loss function 
\begin_inset CommandInset citation
LatexCommand cite
key "rosasco2004loss"
literal "false"

\end_inset

 which is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\mathbf{w},b)=\max(0,1-y\hat{y}(\mathbf{x},\mathbf{w},b)),\label{eq:hinge_loss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{y}(\mathbf{x},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}+b$
\end_inset

 and 
\begin_inset Formula $y=\mathbf{y}_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
In terms of the SVM method, we want to find such a hyperplane that will
 label input values as the first class, if it is 
\begin_inset Quotes eld
\end_inset

above
\begin_inset Quotes erd
\end_inset

 the hyperplane and as the second class, if it is 
\begin_inset Quotes eld
\end_inset

below
\begin_inset Quotes erd
\end_inset

 the hyperplane.
 At this point, a very important assumption will be introduced.
 In order to find an optimal hyperplane, we assume, that the data 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and its labels 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 fully describe spaces 
\begin_inset Formula ${\cal X}$
\end_inset

 and 
\begin_inset Formula ${\cal Y}$
\end_inset

.
 Thus, the uncertainty of the decision task can be defined as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
Using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:expected_loss-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we can evaluate expected loss function for SVM as follows 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w},b)p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\\
 & =\int_{{\cal X}\times{\cal Y}}\max(0,1-y_{1}\hat{y}(\mathbf{x},\mathbf{w},b))\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\\
 & =\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{1,i}\hat{y}(\mathbf{x}_{i},\mathbf{w},b)),
\end{align*}

\end_inset

where 
\begin_inset Formula $\hat{y}(\mathbf{x}_{i},\mathbf{w},b)=\mathbf{w}^{T}\mathbf{x}_{i}+b$
\end_inset

 and 
\begin_inset Formula $y_{1,i}$
\end_inset

 is first component of 
\begin_inset Formula $i-\mathrm{th}$
\end_inset

 vector 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

.
 Expect loss function for SVM can be written as 
\begin_inset Formula 
\begin{equation}
\rho(\mathbf{x}_{i},\mathbf{w},b)=\frac{1}{N}\sum_{i=1}^{N}\max(0,1-y_{i}(\mathbf{w}^{T}\mathbf{x}_{i}+b)).\label{eq:expected_loss_SVM}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Decision Theory and Algorithm Based on Neural Network Function 
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_nn"

\end_inset


\end_layout

\begin_layout Subsubsection
Neural Network 
\end_layout

\begin_layout Standard
Neural Network (NN) is a mapping that has the instance 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 on its input predicted value of the label 
\begin_inset Formula $\mathbf{\hat{y}}$
\end_inset

 on the output.
 In this part of the work, our prior interest is focused around Feed Forward
 Neural Network algorithm, that assigns input value to a specific class.
 
\end_layout

\begin_layout Standard
The first layer of NN is defined as
\begin_inset Formula 
\begin{equation}
\mathbf{a}_{1}=\mathbf{W}_{1}^{T}\mathbf{x}+\mathbf{b}_{1},\label{eq:Neuron}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{W}_{1}$
\end_inset

 is matrix of weights and 
\begin_inset Formula $\mathbf{b}_{1}$
\end_inset

 is a vector of bias values.
 The first layer is called the input layer.
 
\end_layout

\begin_layout Standard
Further layers of NN are formed as
\begin_inset Formula 
\begin{equation}
\mathbf{a}_{k}=\mathbf{W}_{k}^{T}f\big(\mathbf{a}_{k-1}\big)+\mathbf{b}_{k},\ k=\{2,...,K-1\}.\label{eq:second_layer_neron}
\end{equation}

\end_inset

As seen from equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:second_layer_neron"

\end_inset

), neurons from each layer (except input layer) take linear combination
 of the neurons from the previous layer.
 Function 
\begin_inset Formula $f$
\end_inset

 is an activation function.
 The activation function is defined as a non-decreasing, continuous function.
 The most commonly used activation functions are sigmoid, relu, elu, and
 hyperbolic tangence functions 
\begin_inset CommandInset citation
LatexCommand cite
key "bishop2006machine"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Output values are computed with
\begin_inset Formula 
\begin{equation}
\hat{\mathbf{y}}=f_{sm}\left(\mathbf{W}_{K}^{T}f\big(\mathbf{a}_{K-1}\big)+\mathbf{b}_{K}\right),\label{eq:Output_layer}
\end{equation}

\end_inset

where 
\begin_inset Formula $f_{sm}$
\end_inset

 is the softmax function, that is typically used for classification problems.
 The softmax function is defined as 
\begin_inset Formula 
\[
f_{sm,i}=\frac{\exp(\mathbf{z}_{i})}{\sum_{i=1}^{2}\exp(\mathbf{z}_{i})},
\]

\end_inset

where 
\begin_inset Formula 
\[
\mathbf{z}=\mathbf{W}_{K}^{T}f\big(\mathbf{a}_{K-1}\big)+\mathbf{b}_{K}
\]

\end_inset

 is an output vector before the activation function is applied.
 The output vector has the same size as label 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Decision Theory
\end_layout

\begin_layout Standard
Decision theory construction for the algorithm, based on a neural network
 function, is mostly the same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 However, in this case, our decision is to find estimate 
\begin_inset Formula $\hat{\mathbf{y}}=\hat{\mathbf{y}}(\mathbf{x},\Omega)$
\end_inset

 of the probability density function 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is the input data, 
\begin_inset Formula $\Omega=(\mathbf{W}_{1},...\mathbf{W}_{K},\mathbf{b}_{1}...,\mathbf{b}_{K})$
\end_inset

 is a collection of all neural network function parameters and biases.
 Action space 
\begin_inset Formula $\mathcal{A}$
\end_inset

 will be the parameters' and biases' space of 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

.
 Same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can define 
\begin_inset Formula $(\mathbf{x},\mathbf{y})$
\end_inset

 are parameters of the loss function and 
\begin_inset Formula ${\cal X}\times{\cal Y}$
\end_inset

 is a parameters' space.
 Another example of loss functions that we will use is the cross entropy
 loss function, which is defined as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\Omega)=-y_{1}\ln\big(\hat{y}_{1}(\mathbf{x},\Omega)\big)-y_{2}\ln\big((\hat{y}_{2}(\mathbf{x},\Omega)\big),\label{eq:cross_entropy_loss}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{y}=[y_{1},y_{2}]^{T}$
\end_inset

 and 
\begin_inset Formula $\hat{\mathbf{y}}=[\hat{y}_{1},\hat{y}_{2}]^{T}$
\end_inset

.
 With the usage of the given dataset, where 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{(\tilde{\mathbf{X}}\subset{\cal X},\tilde{\mathbf{Y}}\subset{\cal Y})}$
\end_inset

 are independent identically distributed, we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Applying definition (
\begin_inset CommandInset ref
LatexCommand ref
reference "def:expected_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

), expected loss for the algorithm based on a neural network function is
 evaluated as 
\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},y,\Omega)p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & =-\int_{\mathbf{{\cal X}}\times{\cal Y}}\Big(y_{1}\ln\big(\hat{y}_{1}(\mathbf{x},\Omega)\big)+y_{2}\ln\big((\hat{y}_{2}(\mathbf{x},\Omega)\big)\Big)\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & =-\frac{1}{N}\sum_{i=1}^{N}\Big(y_{1}\ln\big(\hat{y}_{1}(\mathbf{x},\Omega)\big)+y_{2}\ln\big((\hat{y}_{2}(\mathbf{x},\Omega)\big)\Big),\label{eq:expected_loss_cross_entropy}
\end{align}

\end_inset

where 
\begin_inset Formula $\mathbf{y}=[y_{1},y_{2}]^{T}$
\end_inset

 and 
\begin_inset Formula $\hat{\mathbf{y}}=[\hat{y}_{1},\hat{y}_{2}]^{T}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Parameters Estimation
\end_layout

\begin_layout Standard
In further sections, we are going to introduce more methods based on Neural
 Networks, which will slightly differ between each other.
 Thus, we would like to cover more theory around parameters estimation.
 The very simple, but efficient method is Gradient Descent.
 This method is based on equation
\begin_inset Formula 
\begin{equation}
\hat{\Omega}_{n+1}=\hat{\Omega}_{n}-\eta_{n}\nabla L(\tilde{\mathbf{X}},\tilde{\mathbf{Y}},\hat{\Omega}_{n},Z_{n}),\label{eq:Gradient-Descent}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{\mathbf{\Omega}}_{n}$
\end_inset

 is the 
\begin_inset Formula $n-$
\end_inset

th iteration value of gradient descent of NN weights and its biases that
 converges to 
\begin_inset Formula $\hat{\Omega}$
\end_inset

.
 Value of 
\begin_inset Formula $\nabla L(\mathbf{X},\mathbf{Y},\hat{\Omega}_{n},Z_{n})$
\end_inset

 is a gradient of a loss function and 
\begin_inset Formula $\eta_{n}$
\end_inset

 is the 
\begin_inset Formula $n-$
\end_inset

th iteration of value, that in terms of NN is defined as learning rate with
 a decay.
 Term 
\begin_inset Formula $Z_{n}$
\end_inset

 represents indices from 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

, that are used in the 
\begin_inset Formula $n-$
\end_inset

th iteration of a loss function.
 Learning rate decay is not an obligatory feature.
 It can be constant as well.
 However, the best performance is obtained when the learning rate is estimated
 from the data, which is the main idea of adaptive methods, such as RMSProp
 or ADAM 
\begin_inset CommandInset citation
LatexCommand cite
key "kingma2014adam"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The usual gradient descent is an efficient method for complex 
\begin_inset Formula $\hat{\mathbf{y}}(\mathbf{x},\Omega)$
\end_inset

 but may struggle with local minima.
 In this case, the algorithm may stop iterating even in a very shallow local
 minimum.
 Due to the complex functions 
\begin_inset Formula $\hat{\mathbf{y}}(\mathbf{x},\Omega)$
\end_inset

, the loss function of its approximation will be non-convex with a large
 amount of local minima and maxima.
 Multiple solutions to this problem have been proposed in the form of Gradient
 Descent with momentum, or Stochastic Gradient Descent (SGD) 
\begin_inset CommandInset citation
LatexCommand cite
key "bishop2006machine"
literal "false"

\end_inset

.
 In comparison to standard Gradient Descent, the key difference is that
 SGD allows us to use only a small subset of all data points (minibatch)
 from the full training dataset in order to calculate the step 
\begin_inset CommandInset citation
LatexCommand cite
key "bishop2006machine"
literal "false"

\end_inset

.
 The minibatch is represented with value 
\begin_inset Formula $Z_{n}$
\end_inset

 that says which training indices to use.
 The data samples (minibatch) are picked randomly at each step.
 
\end_layout

\begin_layout Standard
In this work, we are using the ADAM optimization 
\begin_inset CommandInset citation
LatexCommand cite
key "kingma2014adam"
literal "false"

\end_inset

, which is a stochastic gradient optimization, including an adaptation of
 the learning rate as well as the coefficient of the momentum.
 
\end_layout

\begin_layout Subsection
Decision Theory and Naive Bayes Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:decision_theory_nb"

\end_inset


\end_layout

\begin_layout Standard
Naive Bayes algorithm is a bit different from the algorithm based on neural
 networks and SVM.
 In the case of Naive Bayes, we want to estimate 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

, where 
\begin_inset Formula $\mathbf{W}=(\mathbf{w}_{1},\mathbf{w}_{2},...,\mathbf{w}_{n})\subset{\cal W}$
\end_inset

 is a collection of parameters of individual classifiers, and will be used
 as an action (
\begin_inset Formula $a=\mathbf{W},\ a\in\mathcal{A}$
\end_inset

) it the following decision task.
 The reason why we look for an estimate of the 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 but not 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{W})$
\end_inset

 is due to 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{W})$
\end_inset

 normalization constant.
 The normalization constant would be dependent on the set of parameters
 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 That fact would make our computations very complicated.
 In order to proceed with the loss function construction, we would like
 to go through Naive Bayes (NB) method.
\end_layout

\begin_layout Subsubsection
Naive Bayes
\end_layout

\begin_layout Standard
Consider a binary classification problem.
 With the usage of the Bayes rule, we can rewrite
\series bold
 
\series default

\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 as follows 
\begin_inset Formula 
\begin{equation}
p(\mathbf{W}|\mathbf{x},\mathbf{y})=\frac{p(\mathbf{y})p(\mathbf{x}|\mathbf{y},\mathbf{W})p(\mathbf{W})}{\int_{\mathbf{{\cal W}}}p(\mathbf{x},\mathbf{y}|\mathbf{W})p(\mathbf{W})d\mathbf{W}},\label{eq:bayes_rule}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\cal W}$
\end_inset

 is the space of possible values of 
\begin_inset Formula $\mathbf{W}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Naive Bayes method introduces a very strong assumption in equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes_rule"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 This assumption says that features of vector 
\begin_inset Formula $\mathbf{x}=[x_{1},x_{2},...,x_{n}]^{T}$
\end_inset

 are conditionally independent.
 As a result, the estimation of 
\begin_inset Formula $p(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

 can be written as
\begin_inset Formula 
\begin{equation}
\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y})=\frac{1}{Z}p(\mathbf{y})p(\mathbf{W})\prod_{i=1}^{n}\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big),\label{eq:naive_bayes_eq}
\end{equation}

\end_inset

where 
\series bold

\begin_inset Formula $\mathbf{y}=[y_{1},y_{2}]$
\end_inset


\series default
, 
\begin_inset Formula $\mathbf{W}=\{\mathbf{w}_{1},\mathbf{w}_{2},...,\mathbf{w}_{n}\}$
\end_inset

, and 
\begin_inset Formula $Z$
\end_inset

 is a normalizing constant.
 
\end_layout

\begin_layout Subsubsection
Decision Theory
\end_layout

\begin_layout Standard
We want to maximize probability 
\begin_inset Formula $\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y})$
\end_inset

.
 As a result, using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:naive_bayes_eq"
plural "false"
caps "false"
noprefix "false"

\end_inset

) loss function 
\begin_inset Formula $L$
\end_inset

 will be represented as 
\begin_inset Formula 
\begin{align}
L(\mathbf{y},\mathbf{x},\mathbf{w}) & =-\log\big(\tilde{p}(\mathbf{W}|\mathbf{x},\mathbf{y}),\label{eq:nb_loss}\\
 & =\log(Z)-\log\big(p(\mathbf{y})\big)-\log\big(p(\mathbf{W})\big)-\sum_{i=1}^{n}\log\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big).
\end{align}

\end_inset

 Same as in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_svm"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:decision_theory_nn"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we will assume that we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 From this moment, everything is ready for expected loss function derivation.
 The expected loss function for Naive Bayes method is derived as 
\series bold

\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi^{*}}L & =\int_{\mathbf{{\cal X}}\times\mathbf{{\cal Y}}}L(\mathbf{x},\mathbf{y},\mathbf{w})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & \int_{\mathbf{{\cal X}}\times\mathbf{{\cal Y}}}\Big(\xi(\mathbf{W},\mathbf{y})-\sum_{i=1}^{n}\log\big(p(x_{i}|y_{1},\mathbf{w}_{i})^{y_{1}}p(x_{i}|y_{2},\mathbf{w}_{i})^{y_{2}}\big)\frac{1}{N}\sum_{j=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{j},\mathbf{y}-\mathbf{y}_{j})d(\mathbf{x},\mathbf{y}),\nonumber \\
 & \frac{1}{N}\sum_{j=1}^{N}\Big(\xi_{j}(\mathbf{W},\mathbf{y}_{j})-\sum_{i=1}^{n}\log\big(p(x_{i,j}|y_{1,j},\mathbf{w}_{i})^{y_{1,j}}p(x_{i,j}|y_{2,j},\mathbf{w}_{i})^{y_{2,j}}\big)\Big),\label{eq:expected_loss_mle_or_map}
\end{align}

\end_inset


\series default
where 
\begin_inset Formula $\xi(\mathbf{W},\text{y})=\log(Z)-\log\big(p(\mathbf{y})\big)-\log\big(p(\mathbf{W})\big)$
\end_inset

 and 
\begin_inset Formula $\xi_{j}(\mathbf{W},\mathbf{y}_{j})=\log(Z)-\log\big(p(\mathbf{y}_{j})-\log\big(p(\mathbf{W})\big)$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Decision Theory and Random Forest Algorithm
\begin_inset CommandInset label
LatexCommand label
name "subsec:random_forest_supervised_learning"

\end_inset


\end_layout

\begin_layout Standard
In order to work with random forests, we must precisely define decision
 trees and only then construct a random forest theory.
\end_layout

\begin_layout Subsubsection
Decision Tree
\end_layout

\begin_layout Standard
In this section, we expect our decision tree to give us an estimate 
\begin_inset Formula $\hat{\mathbf{y}}(\mathbf{x},\mathbf{w})\in\{[0,1]^{T},[1,0]^{T}\}$
\end_inset

, where 
\begin_inset Formula $\mathbf{w}$
\end_inset

 is a vector that describes tree (depth, branches, etc.), for each 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

.
 It is very important to mention that for different trees 
\begin_inset Formula $\mathbf{w}$
\end_inset

 can have different dimensionality.
 Thus, for consistency, we will assume that for all 
\begin_inset Formula $\mathbf{w}\in{\cal W}$
\end_inset

 exist a single upper bound, where 
\begin_inset Formula ${\cal W}$
\end_inset

 is redefined as a space of tree parameters.
 As a result, we will make all 
\begin_inset Formula $\mathbf{w}$
\end_inset

 to have the same length.
 If 
\begin_inset Formula $\mathbf{w}$
\end_inset

 has spare elements, they will be filled with zeros.
 Decision tree parameters space is 
\begin_inset Formula ${\cal W}$
\end_inset

.
 The loss function action 
\begin_inset Formula $a\in{\cal A}$
\end_inset

 will be represented as elements of the action space 
\begin_inset Formula ${\cal A}={\cal W}$
\end_inset

.
 The loss function of a decision tree is a zero-one loss function, defined
 as 
\begin_inset Formula 
\begin{equation}
L(\mathbf{x},\mathbf{y},\mathbf{w})=\begin{cases}
1, & \mathbf{y\neq\hat{y}}(\mathbf{x},\mathbf{w})\\
0, & \mathbf{y=\hat{y}}(\mathbf{x},\mathbf{w})
\end{cases}.\label{eq:zero-one_loss_function}
\end{equation}

\end_inset

With the use of the given data, where 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed, we can approximate 
\begin_inset Formula $p(\mathbf{x},\mathbf{y})$
\end_inset

 as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_of_joint_dist-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 As a result, the expected loss function for a decision tree can be derived
 as 
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi^{*}}L & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w})p(\mathbf{x},\mathbf{y})d(\mathbf{x},\mathbf{y}),\\
 & =\int_{{\cal X}\times{\cal Y}}L(\mathbf{x},\mathbf{y},\mathbf{w})\frac{1}{N}\sum_{i=1}^{N}\delta(\mathbf{x}-\mathbf{x}_{i},\mathbf{y}-\mathbf{y}_{i})d(\mathbf{x},\mathbf{y}),\\
 & =\frac{1}{N}\sum_{i=1}^{N}L(\mathbf{x}_{i},\mathbf{y}_{i},\mathbf{w}),
\end{align*}

\end_inset

where 
\begin_inset Formula $\sum_{i=1}^{N}L(\mathbf{x}_{i},\mathbf{y}_{i},\mathbf{w})$
\end_inset

 is (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:zero-one_loss_function"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
The conventional optimization procedure for decision trees is a heuristic,
 that works as follows.
 In the process of constructing a decision tree, we choose such feature
 
\begin_inset Formula $x_{i}\in[x_{1},...,x_{n}]^{T}=\mathbf{x}$
\end_inset

 that will bring the highest information about the system (e.g.
 highest entropy of the features).
 This feature will form the first layer on which a classification rule is
 designed.
 Then we add another feature with the highest information gain and construct
 the second layer.
 Using this method, we construct nodes and add more and more layers (branches).
 However, such a procedure can be sub-optimal, and improvement was achieved
 using a set of decision trees.
 For this purpose, we will define our decision tree 
\begin_inset Formula $T(\mathbf{x},\mathbf{w}_{l})$
\end_inset

, which has a one-hot encoded classification, i.e.
 two-dimensional vector 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

 for the binary classification problem.
 Index 
\begin_inset Formula $l$
\end_inset

 represents the set of parameters for the 
\begin_inset Formula $l$
\end_inset

-th three.
 
\end_layout

\begin_layout Subsubsection
Random Forest
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y\in}{\cal Y}$
\end_inset

 as random variables with joint probability density function 
\begin_inset Formula $p({\bf x},\mathbf{y})$
\end_inset

.
 We will also assume that 
\begin_inset Formula $\forall i\in\{1,..,N\},\ (\mathbf{x}_{i},\mathbf{y}_{i})\in\mathbf{\tilde{X}\times\tilde{Y}}$
\end_inset

 are independent identically distributed.
 
\end_layout

\begin_layout Standard
We will use the training set (
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

), as a set from which we sample 
\begin_inset Formula $V\in\mathbb{N}$
\end_inset

 sets, 
\begin_inset Formula $\tilde{\mathbf{X}}_{v}\subset\tilde{\mathbf{X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}_{v}\subset\tilde{{\bf Y}}$
\end_inset

, 
\begin_inset Formula $\forall v\in\{1,...,V\}$
\end_inset

.
 The data 
\begin_inset Formula $\tilde{\mathbf{X}}_{v}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}_{v}$
\end_inset

 are created with random uniform sampling of indices of instances in 
\begin_inset Formula $\tilde{{\bf X}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

 without repetition.
 We, also, want each subset to contain strictly 
\begin_inset Formula $80\%$
\end_inset

 of the data from 
\begin_inset Formula $\mathbf{\tilde{{\bf X}}}$
\end_inset

 and 
\begin_inset Formula $\tilde{{\bf Y}}$
\end_inset

.
 As a result, the parameters space for random forests will form tuples of
 sets 
\begin_inset Formula $(\tilde{{\bf X}}_{v},\tilde{{\bf Y}}_{v})$
\end_inset

.
 Using this theory, we will construct 
\begin_inset Formula $V$
\end_inset

 decision trees 
\begin_inset Formula $\hat{\mathbf{y}}_{v}=T(\mathbf{x},\mathbf{w}_{v})$
\end_inset

.
 As a result, the expected loss is 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi^{*}}L=\frac{1}{V}\sum_{v=1}^{V}\frac{1}{N_{v}}\sum_{i=1}^{N_{v}}L(\mathbf{x}_{i,v},\mathbf{y}_{i,v},\mathbf{w}_{v})\delta(\mathbf{x}-\mathbf{x}_{i,v},\mathbf{y}-\mathbf{y}_{i,v}),\label{eq:decision_tree_for_random_forest}
\end{equation}

\end_inset

where 
\begin_inset Formula $(\mathbf{x}_{i,v},\mathbf{y}_{i,v})\in(\tilde{{\bf X}}_{v},\tilde{{\bf Y}}_{v})$
\end_inset

 and 
\begin_inset Formula $N_{v}$
\end_inset

 is the number of the data samples in 
\begin_inset Formula $\tilde{{\bf X}}_{v}$
\end_inset

 and 
\begin_inset Formula $\tilde{\mathbf{Y}}_{v}$
\end_inset

.
 If we assume 
\begin_inset Formula $\mathbf{w}_{v}$
\end_inset

 to be a random variable, then 
\begin_inset Formula $V$
\end_inset

 decision trees form samples from the probability density function 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x},\mathbf{w})$
\end_inset

.
 In other words 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x},\mathbf{w}_{v})=\hat{y}_{v,1}^{y_{1}}\hat{y}_{v,2}^{y_{2}},\label{eq:decision_tree_as_random_variable}
\end{equation}

\end_inset

where label 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is written as a one-hot representation, and the estimate is in the same
 form 
\begin_inset Formula $\hat{\mathbf{y}}_{v}=[\hat{y}_{v,1},\hat{y}_{v,2}]^{T}$
\end_inset

.
 Thus, we can say that classification probability 
\begin_inset Formula $p(\text{\textbf{y}}|\mathbf{x})$
\end_inset

 can be written as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x})=\int_{\mathbf{w}\in\mathbf{{\cal A}}}p(\mathbf{y}|\mathbf{x},\mathbf{w})p(\mathbf{w})d\mathbf{w},\label{eq:classification_prob_for_random_forest}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\cal A}$
\end_inset

 is an action space.
 With the usage of samples 
\begin_inset Formula $\mathbf{w}_{v}$
\end_inset

 we can approximate 
\begin_inset Formula $p(\mathbf{y}|\mathbf{x})$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
p(\mathbf{y}|\mathbf{x})=\frac{1}{V}\sum_{l=1}^{V}\hat{y}_{v,1}^{y_{1}}\hat{y}_{v,2}^{y_{2}},\label{eq:approx_class_prob_for_random_forest}
\end{equation}

\end_inset

where each decision tree 
\begin_inset Formula $T(\mathbf{x},\mathbf{w}_{v})$
\end_inset

 is constructed with the use of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:decision_tree_for_random_forest"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
In order to proceed with further sections we define the output of Random
 Forest as 
\begin_inset Formula $\hat{\mathbf{y}}=\hat{\mathbf{y}}(\mathbf{x},\mathbf{W})$
\end_inset

, where 
\begin_inset Formula $\mathbf{W}=\{\mathbf{w}_{1},...,\mathbf{w}_{V}\}\in\mathcal{W}$
\end_inset

 is a set of parameters of specific Random Forest algorithm.
 We define vector 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\hat{\mathbf{y}}=\frac{1}{V}\sum_{l=1}^{V}\hat{\mathbf{y}}_{v}.\label{eq:Random_forest_output}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Decision Theory for Active Learning 
\end_layout

\begin_layout Standard
As mentioned in previous sections, the training set 
\begin_inset Formula $\tilde{\mathbf{X}}\times\mathbf{\tilde{Y}}$
\end_inset

 is only a subset of all available data.
 It is important that each point 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in the training set has a label 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 The labels are expensive to obtain in many situations, and the number 
\begin_inset Formula $N$
\end_inset

 of all available samples 
\begin_inset Formula $\mathbf{x}\in\mathbf{X}$
\end_inset

 is large, while labels are available only for the initial subset 
\begin_inset Formula $J_{0}=\{1,\ldots,N_{0}\},N_{0}\ll N$
\end_inset

.
 This problem is known as semi-supervised learning.
\end_layout

\begin_layout Standard
We consider a setup in which we can ask for a label for an arbitrary 
\begin_inset Formula $\mathbf{x}$
\end_inset

, that can be provided for example by a human (annotator).
 We assume that getting labels needs some time and is very expensive.
 The task is to choose which sample we will ask to label.
 
\end_layout

\begin_layout Standard
The active learning problem is defined as a sequence of supervised learning
 problems.
 Specifically, we assume that the initial sets for supervised learning are
 
\begin_inset Formula $\mathbf{X}_{0}=\{\mathbf{x}_{i}\}_{i\in J_{0}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Y}_{0}=\{\mathbf{y}_{i}\}_{i\in J_{0}}$
\end_inset

.
 We consider a sequence of 
\begin_inset Formula $U$
\end_inset

 questions 
\begin_inset Formula $u=\{1,\ldots,U\}$
\end_inset

, in each question we select an index 
\begin_inset Formula $j_{u}$
\end_inset

 and ask to obtain the label 
\begin_inset Formula $\mathbf{y}_{j_{u}}$
\end_inset

 for data record 
\begin_inset Formula $\mathbf{x}_{j_{u}}$
\end_inset

.
 The index set and the data sets are extended as follows
\begin_inset Formula 
\begin{align*}
J_{u} & =\{J_{u-1},j_{u}\}, & \mathbf{X}_{u} & =\{\mathbf{X}_{u-1},\mathbf{x}_{j_{u}}\}, & \mathbf{Y}_{u} & =\{\mathbf{Y}_{u-1},\mathbf{y}_{j_{u}}\}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The task of active learning is to optimize the selection of indices 
\begin_inset Formula $j_{u}$
\end_inset

 to reach as good classification metrics with as low number of questions
 as possible.
 As a result, we have to define the expected loss for each question 
\begin_inset Formula $u$
\end_inset

 that will be dependent on the action and parameter spaces.
 In this case, we can define our action space as a space of the data indices
 of the unlabeled data 
\begin_inset Formula $\mathbf{x}$
\end_inset

, 
\begin_inset Formula $a=j_{u}\in\mathcal{A}_{u}=J\backslash J_{u}$
\end_inset

.
 The uncertainty of the decision task, if the parameter space 
\begin_inset Formula $\varTheta$
\end_inset

 of the used classifiers, i.e.
 
\begin_inset Formula $\theta=(\mathbf{w},\mathbf{b})$
\end_inset

 for SVM, 
\begin_inset Formula $\theta=$
\end_inset


\begin_inset Formula $\Omega$
\end_inset

 for NN, 
\begin_inset Formula $\theta=\mathbf{W}$
\end_inset

 for RF and NB.
 It is very vital to understand, that point estimate of the parameters that
 was designed in supervised learning is not sufficient for this task.
 We need full distribution on the parameters because we will integrate over
 the parameters space.
 As an example, if we talk about the SVM method, then parameters space for
 active learning problem will be defined as a set of weights that form a
 hyperplane.
 If we talk about the algorithm that is based on a neural network function,
 then parameters space of the active learning problem will form weights
 from neurons.
 We wanted to highlight that parameters space will be different for each
 problem but the idea for each algorithm is the same.
\end_layout

\begin_layout Standard
The decision task for this particular problem can be written as 
\begin_inset Formula 
\begin{equation}
j_{u}^{*}=\argmin_{j_{u}\in J\backslash J_{u}}(\mathbb{E}_{\pi_{u}^{*}}L^{*}),\label{eq:index_selection_active_learning}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbb{E}_{\pi_{u}^{*}}L^{*}$
\end_inset

 is the expected loss that is dependent on an action given question 
\begin_inset Formula $u$
\end_inset

, and 
\begin_inset Formula $J$
\end_inset

 is the space of all indices.
 The expected loss for the active learning problem is defined as 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{\pi_{u}^{*}}L^{*}=\int_{\varTheta}L^{*}(j_{u},\theta)\pi_{u}^{*}d\theta,\label{eq:Expected_loss_active_learning}
\end{equation}

\end_inset

where 
\begin_inset Formula $j_{u}\in J\backslash J_{u}$
\end_inset

, 
\begin_inset Formula $\theta\in\varTheta$
\end_inset

 and 
\begin_inset Formula $L^{*}$
\end_inset

 is a loss function for the active learning problem.
 Character 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $*$
\end_inset


\begin_inset Quotes erd
\end_inset

 is used only for distinguishing active learning loss from the loss function
 which is used for different models.
 
\end_layout

\begin_layout Standard
Using this approach, we will be able sequentially select indices from 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and ask for a label from 
\begin_inset Formula $\mathbf{Y}$
\end_inset

, that will help us to get higher scores faster than in the case of a random
 choice of indices.
 
\end_layout

\begin_layout Subsection
Bayesian Approach of Classifiers' Parameters Sampling
\end_layout

\begin_layout Standard
Consider that 
\begin_inset Formula $\mathbf{y}\in{\cal Y}$
\end_inset

.
 Let 
\begin_inset Formula $\hat{\mathbf{y}}=\hat{\mathbf{y}}(\mathbf{x}_{j_{u}},\theta_{u})$
\end_inset

 is an estimate of 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 However, in this case output estimate 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

 is represented as a vector of probabilities that 
\begin_inset Formula $\mathbf{x}_{j_{u}}$
\end_inset

 is assigned to different classes.
 As an example for a well trained binary classifier, for specific 
\begin_inset Formula $\mathbf{x}$
\end_inset

 that is assigned to 
\begin_inset Formula $\mathbf{y}=[1,0]^{T},$
\end_inset

 classifiers estimate of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can be 
\begin_inset Formula $\mathbf{\hat{y}}=[0.95,0.05]^{T}$
\end_inset

.
 It is interesting that before we can solve the optimization problem with
 choosing the index 
\begin_inset Formula $j_{u}$
\end_inset

, we have to solve the optimization problem of finding 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

.
 This leads us to supervised learning models that we have discussed in previous
 sections.
\end_layout

\begin_layout Standard
In this section, we would like to construct a theory around 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 from equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Expected_loss_active_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The mentioned distribution is a distribution of the models' parameters,
 given the training data that can be written as 
\begin_inset Formula 
\begin{equation}
\pi_{u}^{*}=p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u}).\label{eq:prior_distribution_for_expected_entropy_loss}
\end{equation}

\end_inset

We do not have explicit form of the pdf.
 However, we assume that we have 
\begin_inset Formula $Q_{u}$
\end_inset

 samples 
\begin_inset Formula $\theta_{u,q}\in\{1_{u},...,Q_{u}\}$
\end_inset

 from 
\begin_inset Formula $p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

.
 As a result 
\begin_inset Formula $p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

 can be approximated as 
\begin_inset Formula 
\begin{equation}
\pi_{u}^{*}=\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\delta(\theta_{u}-\theta_{u}^{(q)}),\label{eq:prior_distribution_for_exp_loss_estimate}
\end{equation}

\end_inset

where 
\begin_inset Formula $\delta(\theta_{u}-\theta_{u}^{(q)})$
\end_inset

 is Dirac delta function centered in 
\begin_inset Formula $\theta_{u}^{(q)}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Parameters Sampling Based on Training Data Subsets
\end_layout

\begin_layout Standard
This method is quite general and can be applied to all types of classifiers
 in this work (Random Forest, SVM, neural network).
 The idea is very simple.
 We consider that some data samples in training dataset 
\begin_inset Formula $\tilde{\mathbf{X}}\times\tilde{\mathbf{Y}}$
\end_inset

 are noise corrupted.
 Thus, it is obvious that we do not want our models to learn from noise
 corrupted data.
 As a result, we would like to randomly sample 
\begin_inset Formula $Q_{u}$
\end_inset

 subsets from 
\begin_inset Formula $\tilde{\mathbf{X}}$
\end_inset

 with their labels from 
\begin_inset Formula $\tilde{\mathbf{Y}}$
\end_inset

.
 Let us rewrite it in a more mathematical form.
 
\end_layout

\begin_layout Standard
Assume 
\begin_inset Formula $N_{u}$
\end_inset

 is the number of samples in 
\begin_inset Formula $\mathbf{X}_{u}$
\end_inset

.
 Let 
\begin_inset Formula $Z_{u}=\{z_{1},...,z_{N_{u}^{sub}}\}\subset J_{u}$
\end_inset

, where 
\begin_inset Formula $N_{u}^{sub}<N_{u}$
\end_inset

 .
 Let 
\begin_inset Formula $p(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u},Z_{u})$
\end_inset

 be a probability of model parameters 
\begin_inset Formula $\theta_{u}$
\end_inset

 given 
\begin_inset Formula $\mathbf{X}_{u},\mathbf{Y}_{u}$
\end_inset

 and 
\begin_inset Formula $Z_{u}$
\end_inset

.
 The conditioning in the pdf is defined as a restriction of sets 
\begin_inset Formula $\mathbf{X}_{u},\ \mathbf{Y}_{u}$
\end_inset

 on indices from 
\begin_inset Formula $Z_{u}$
\end_inset

.
 As a result, we can approximate (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

) as 
\begin_inset Formula 
\begin{align}
\pi_{u}^{*} & =p(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})\label{eq:prior_distribution_approximation_training_set_sampling}\\
 & =\int p(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u},Z_{u})p(Z_{u})dZ_{u}\\
 & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}p(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u},Z_{u}^{(q)})\\
 & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\delta(\theta_{u}-\theta_{u}^{(q)}).\label{eq:prior_disctribution_approximation_training_set_sampling_final}
\end{align}

\end_inset

Sampling from 
\begin_inset Formula $p(Z_{u})$
\end_inset

 is very simple.
 The only thing that must be predefined is 
\begin_inset Formula $N_{u}^{sub}$
\end_inset

.
 After training the model using 
\begin_inset Formula $\mathbf{X}_{u}$
\end_inset

 and 
\begin_inset Formula $Y_{u}$
\end_inset

 under restriction 
\begin_inset Formula $Z_{u}$
\end_inset

, the vector of model parameters will represent a single sample from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
SGLD
\end_layout

\begin_layout Standard
Unlike the previous section method, SGLD sampling is designed only for neural
 network based classifiers.
 SGLD modifies neural network learning algorithm by adding noise in Stochastic
 Gradient Descent.
 The conventional stochastic gradient descent of Feed Forward Neural Network
 was extended in 
\begin_inset CommandInset citation
LatexCommand cite
key "welling2011bayesian"
literal "false"

\end_inset

 by an additive white noise, which is know as the Stochastic Gradient Langevin
 Dynamics (SGLD) algorithm.
 This algorithm provides Bayesian estimate of Neural Network parameters
 
\begin_inset CommandInset citation
LatexCommand cite
key "welling2011bayesian"
literal "false"

\end_inset

.
 In essence, when the algorithm is almost trained, the additional noise
 samples i.i.d parameter values in a neighborhood of the minimum.
 Using the Bayes rule we can rewrite (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

) as 
\begin_inset Formula 
\begin{align}
\pi_{u}^{*} & =p(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})\nonumber \\
 & \propto p(\mathbf{X}_{u},\mathbf{Y}_{u}|\theta_{u})p(\theta_{u}).\label{eq:prior_distribution_approximation_sgld}
\end{align}

\end_inset

Next, we can approximate (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_approximation_sgld"
plural "false"
caps "false"
noprefix "false"

\end_inset

) as 
\begin_inset Formula 
\begin{align}
\pi_{u}^{*} & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}p(\mathbf{X}_{u},\mathbf{Y}_{u}|\theta_{u}^{(q)})
\end{align}

\end_inset

that results in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_disctribution_approximation_training_set_sampling_final"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 However, for this case, 
\begin_inset Formula $\theta$
\end_inset

 must meet some constraints.
 If we want to sample parameters from its distribution, 
\begin_inset Formula $\theta$
\end_inset

 must be independent identically distributed.
 Based on 
\begin_inset CommandInset citation
LatexCommand cite
key "welling2011bayesian"
literal "false"

\end_inset

, the updated gradient for SGLD can be written as 
\begin_inset Formula 
\begin{align}
\hat{\Omega}_{n+1} & =\hat{\Omega}_{n}-\frac{N}{N_{minibatch}}\frac{\eta_{n}}{2}\Big(\nabla L(\tilde{\mathbf{X}},\tilde{\mathbf{Y}},\hat{\Omega}_{n},Z_{n})\Big)+\epsilon_{n},\label{eq:SGLD_gradient_descent}\\
\epsilon_{n} & \sim{\cal N}(0,\eta_{n}I),
\end{align}

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of training data, 
\begin_inset Formula $N_{minibatch}$
\end_inset

 is the number of samples in the minibatch and 
\begin_inset Formula $\eta_{n}$
\end_inset

 is the learning rate in 
\begin_inset Formula $n-$
\end_inset

th iteration of the algorithm.
\end_layout

\begin_layout Subsubsection
Dropout
\end_layout

\begin_layout Standard
Dropout is another technique similar to SGLD designed to sample from a neural
 network parameters distribution 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

.
 The idea is randomly turn off some neurons while training.
 Thus, an estimate of 
\begin_inset Formula $\pi_{u}^{*}=p(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

 is the Dirac function as well 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_exp_loss_estimate"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 When the algorithm is almost trained, we are able to start sampling the
 i.i.d.
 parameter masks.
 Samples 
\begin_inset Formula $\theta^{(q)}$
\end_inset

 are generated after each step of the SGD, where the different mask of neural
 weight and biases is sampled for each 
\begin_inset Formula $q$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Deep Ensemble Filter
\end_layout

\begin_layout Standard
Deep Ensemble Filter (DENFI) algorithm is an extension of deep ensembles
 
\begin_inset CommandInset citation
LatexCommand cite
key "lakshminarayanan2017simple"
literal "false"

\end_inset

.
 In case of DENFI algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "ulrych2020denfi"
literal "false"

\end_inset

, we can approximate (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

) with the usage of equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_approximation_sgld"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_disctribution_approximation_training_set_sampling_final"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The equations are the same but a sampling from 
\begin_inset Formula $p(\theta_{u})$
\end_inset

 is different.
 In this work, we propose a modification of the initial DENFI algorithm.
 The idea of this algorithm is to train an ensemble of 
\begin_inset Formula $Q_{u}$
\end_inset

 Feed Forward Neural Networks using Stochastic Gradient Descent.
 In theory, each neural network will find a different local minimum due
 to different initial weights of neurons and Stochastic Gradient Descent.
 The beauty of this algorithm is in further training iterations that will
 be described and shown in further sections.
 For now, we are only interested in parameters sampling from 
\begin_inset Formula $p(\theta_{u})$
\end_inset

, that has been already covered and described.
\end_layout

\begin_layout Subsection
Active Learning Loss Function
\begin_inset CommandInset label
LatexCommand label
name "subsec:Active-Learning-Loss"

\end_inset


\end_layout

\begin_layout Standard
The acquisition function is a function that helps us to decide which data
 sample is the best for model learning given the model's uncertainty.
 A wide overview of different acquisition functions is shown in 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

 e.g.
 entropy, information, or mean standard deviation maximization.
 However, in our work, we decided to use only the entropy loss.
\end_layout

\begin_layout Subsubsection
Entropy Based Active Learning Loss
\end_layout

\begin_layout Standard
The first approach of defining the Active Learning loss function is negative
 entropy.
 Basing on the formal entropy definition 
\begin_inset CommandInset citation
LatexCommand cite
key "shannon1948mathematical"
literal "false"

\end_inset

, we can write it as 
\begin_inset Formula 
\begin{equation}
-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u})=\sum_{r=1}^{R}\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u})\log\big(\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u})\big),\label{eq:entropy}
\end{equation}

\end_inset

where 
\begin_inset Formula $\hat{y}_{r}$
\end_inset

 is 
\begin_inset Formula $r-$
\end_inset

th element of the output estimate 
\begin_inset Formula $\mathbf{\hat{y}}$
\end_inset

, and 
\begin_inset Formula $\theta$
\end_inset

 is a vector of parameters for specific model.
 As done in Passive Learning sections we want to find expected loss based
 on entropy function.
 
\end_layout

\begin_layout Standard
With the usage of previous knowledge, we can derive expected entropy loss
 as 
\begin_inset Formula 
\begin{align}
\mathbb{E}_{\pi_{u}^{*}}L^{*} & =\int_{\varTheta_{u}}-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u})p_{u}(\theta_{u}|\mathbf{X}_{u},\mathbf{Y}_{u})d\theta_{u}\nonumber \\
 & =\int_{\varTheta_{u}}-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u})\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\delta(\theta_{u}-\theta_{u,q})d\theta_{u}\nonumber \\
 & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}-H(\hat{\mathbf{y}}|\mathbf{x}_{j_{u}},\theta_{u,q}))\nonumber \\
 & =\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\sum_{r=1}^{R}\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u,q})\log\big(\hat{y}_{r}(\mathbf{x}_{j_{u}},\theta_{u,q})\big).\label{eq:expected_entropy_loss}
\end{align}

\end_inset

As a result, the minimization of the given expected loss will lead us to
 a sample with the highest entropy.
 Thus, we are seeking for the index of the data instance that has maximum
 predictive entropy.
 In other words 
\begin_inset Formula $j_{u}^{*}=\argmin_{j\in J\backslash J_{u}}(\mathbb{E}_{\pi_{u}^{*}}L^{*})$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Active Learning
\end_layout

\begin_layout Standard
We would like to generalize the active learning part for all described algorithm
s, in order to estimates 
\begin_inset Formula $p({\bf y}|\mathbf{x},\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

 basing on samples from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 In the Supervised Learning section we have derived estimate 
\begin_inset Formula $\hat{\mathbf{y}}$
\end_inset

 of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 for SVMs, Random Forests and Feed Forward Neural Networks.
 Active Learning algorithm requires distribution over the parameters of
 the algorithms.
 We will solve this problem the way that we will get samples from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

, and then approximate probability distribution as (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:prior_distribution_for_expected_entropy_loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\end_layout

\begin_layout Standard
In order to estimate 
\begin_inset Formula $p({\bf y}|\mathbf{x},\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

, we define Generalized Ensembles Algorithm.
 SGLD and DENFI can be also represented as generalized ensembles models
 because parameters sampling (neuron weights sampling) represents different
 configurations of neural networks.
 Thus, each sample from 
\begin_inset Formula $\pi_{u}^{*}$
\end_inset

 can be assumed as i.i.d.
 ensemble.
 As a result, we will use 
\begin_inset Formula $Q_{u}$
\end_inset

 ensembles in each step of Active Learning algorithm.
 Therefore, basing on the previous theory we can approximate 
\begin_inset Formula $p({\bf y}|\mathbf{x},\mathbf{X}_{u},\mathbf{Y}_{u})$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
p({\bf y}|\mathbf{x},\mathbf{X}_{u},\mathbf{Y}_{u})=\frac{1}{Q_{u}}\sum_{q=1}^{Q_{u}}\hat{\mathbf{y}}_{q,u}.\label{eq:active_learning_output_estimate}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
In this section, we have covered the decision theory for both passive and
 active learning with respect to different algorithms and ensemble approaches.
 Passive Learning section showed how it is possible to represent SVM, Random
 Forest, and neural networks in terms of decision theory problem setup.
 In addition to this, the Active Learning section showed how to represent
 the uncertainty of the model and parameters sampling for ensembles representati
on.
\end_layout

\begin_layout Chapter
Natural Language Processing Theory
\begin_inset CommandInset label
LatexCommand label
name "chap:Natural_Language_Processing"

\end_inset


\end_layout

\begin_layout Section
Text Representation
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "liddy2001natural"
literal "false"

\end_inset

, Natural Language Processing (NLP) is a theoretically motivated range of
 computational techniques for analyzing and representing naturally occurring
 texts at one or more levels of linguistic analysis for the purpose of achieving
 human-like language processing for a range of tasks or applications.
 
\end_layout

\begin_layout Standard
In this work we are focused on two techniques, such as TF-IDF 
\begin_inset CommandInset citation
LatexCommand cite
key "rajaraman2011mining"
literal "false"

\end_inset

 and Fast Text Word Embeddings 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2018advances"
literal "false"

\end_inset

.
 These methods are used for representation of text in a mathematical form
 (vectors, matrices).
 Even though TF-IDF is quite old method for text representation, it is still
 widely used.
 However, primary method, that is used in the thesis is the Fast Text Word
 Embeddings.
 In this project we are working with text documents (articles and tweets)
 and their labels.
 In the beginning of chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Introduction-to-Decision"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we defined value 
\begin_inset Formula $\mathbf{x}\in{\cal X}$
\end_inset

 as text features vector.
 By features vector we mean any kind of text encoding (TF-IDF, Fast Text
 Word Embedding, etc..).
 
\end_layout

\begin_layout Subsection
TF-IDF 
\end_layout

\begin_layout Standard
Term Frequency - Inverse Document Frequency (TF-IDF) is extremely powerful
 tool.
 This text encoding tool is quite simple and efficient.
 Method's advantage is its popularity.
 Plenty of packages in different programming languages have implementations
 of this algorithm.
 As mentioned in the name of this method, it is composed of two parts as
 Term Frequency and Inverse Document Frequency.
 Term Frequency is defined as 
\begin_inset Formula 
\[
TF(t,d)=\frac{f_{t,d}}{\sum_{t^{\prime}}f_{t^{\prime},d},},
\]

\end_inset

where 
\begin_inset Formula $f_{t,d}$
\end_inset

 is number of times of word 
\begin_inset Formula $t$
\end_inset

 in a document 
\begin_inset Formula $d$
\end_inset

.
 Inverse Document Frequency is defined as 
\begin_inset Formula 
\[
IDF(t,d)=\log\frac{|D|}{|\{d\in D:t\in d\}|},
\]

\end_inset

where numerator stand for total number of documents in the corpus and denominato
r is number of documents where the term 
\begin_inset Formula $t$
\end_inset

 appears.
 We assume words from corpus 
\begin_inset Formula $D$
\end_inset

.
 Thus, the denominator is always greater than zero.
 
\end_layout

\begin_layout Standard
Finally, 
\begin_inset Formula 
\[
TF-IDF(t,d)=TF(t,d)\cdot IDF(t,d).
\]

\end_inset


\end_layout

\begin_layout Subsubsection
TF-IDF and Information Theory
\end_layout

\begin_layout Standard
In this part is shown the connection of TF-IDF to information theory 
\begin_inset CommandInset citation
LatexCommand cite
key "aizawa2003information"
literal "false"

\end_inset

.
 Let us first take a look on documents' entropy given word 
\begin_inset Formula $t$
\end_inset

,
\begin_inset Formula 
\begin{align}
H(D|T=t) & =-\sum_{d}p(d|t)\log p(d|t)\nonumber \\
 & =\log\frac{1}{|\{d\in D:t\in D\}|}\nonumber \\
 & =-\log\frac{|\{d\in D:t\in D\}|}{D|}+\log|D|\nonumber \\
 & =-IDF(t,d)+\log|D|,\label{eq:Documents_entropy_given_word}
\end{align}

\end_inset

where 
\begin_inset Formula $D$
\end_inset

 is a documents' random variable and 
\begin_inset Formula $T$
\end_inset

 is words' random variable.
 Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Documents_entropy_given_word"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is correct under the condition that we have no duplicate documents in
 the text corpus.
 Next step is to derive an equation of mutual information of documents and
 words as follows 
\begin_inset Formula 
\begin{align}
M(D,T) & =H(D)-H(D|T)\nonumber \\
 & =-\sum_{d}p(d)\log p(d)-\sum H(D|T=t)\cdot p(t)_{t}\nonumber \\
 & =\sum_{t}p(t)\cdot\Big(\log\frac{1}{|D|}+IDF(t,d)-\log|D|\Big)\nonumber \\
 & =\sum_{t}p(t)\cdot IDF(t,d)\nonumber \\
 & =\sum_{t,d}p(t|d)\cdot p(d)\cdot IDF(t,d)\nonumber \\
 & =\frac{1}{|D|}\sum_{t,d}TF(t,d)\cdot IDF(t,d).\label{eq:tf_idf_mutual_information}
\end{align}

\end_inset

As seen from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:tf_idf_mutual_information"
plural "false"
caps "false"
noprefix "false"

\end_inset

) TF-IDF has really good explanatory definition based on information theory.
 As a result, it is one more advantage of this method usage.
 However, here is one big disadvantage that can be very crucial.
 The higher amount of words is, the bigger and sparser the vectors that
 represent each document, will be.
\end_layout

\begin_layout Subsection
Fast Text and CBOW Word Embeddings
\end_layout

\begin_layout Standard
Term 
\begin_inset Quotes sld
\end_inset

embedding
\begin_inset Quotes srd
\end_inset

 means a set of language modeling and feature learning techniques in natural
 language processing, where words or phrases from the vocabulary are mapped
 to vectors of real numbers.
 Plenty of word embedding methods based on neural networks and co-occurrence
 matrices exist nowadays.
 Word embeddings are used as pretrained models.
 Words' encoding is used to encode the text, and then text encoding is used
 for different purposes, such as classification, clustering, etc..
 
\end_layout

\begin_layout Standard
The principle of word embeddings based on neural networks is explained in
 this section.
 We decided to describe Continuous Bag of Words Model (CBOW) because Fast
 Text word embeddings model is a modification of this method, and CBOW covers
 all main theoretical aspects.
\end_layout

\begin_layout Subsubsection
CBOW Word Embeddings
\end_layout

\begin_layout Standard
The CBOW embeddings have been introduced in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"
literal "false"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
check.
 
\end_layout

\end_inset

 Consider the sentence 
\begin_inset Quotes sld
\end_inset

A beautiful cat jumped over a puddle
\begin_inset Quotes srd
\end_inset

.
 We choose the window of 
\begin_inset Formula $2m+1,\ m\in\mathbb{N}$
\end_inset

 words and try to predict the word with index 
\begin_inset Formula $m+1$
\end_inset

, basing on the context of size 
\begin_inset Formula $2m$
\end_inset

, where 
\begin_inset Formula $m$
\end_inset

 words are before the prediction word and 
\begin_inset Formula $m$
\end_inset

 words are after the prediction word.
 We would like to treat tuple ("a", 
\begin_inset Quotes sld
\end_inset

beautiful
\begin_inset Quotes srd
\end_inset

, "cat", ’over", "a’, "puddle") as a context, and based on the context we
 would like to predict or generate the word "jumped".
 This type of model is the Continuous Bag of Words (CBOW) model.
 Let the known parameters in our model be the contexts, represented with
 one-hot encoded word vectors.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Introduce vocabulary (as a set?).
\end_layout

\end_inset

 The input one hot encoded word vector is denoted as 
\begin_inset Formula ${\bf c}^{(c)}$
\end_inset

, where 
\begin_inset Formula $c$
\end_inset

 is the position of the word in the context.
 The predicted context word is defined as 
\begin_inset Formula ${\bf c}_{\mathrm{pred}}$
\end_inset

.
 We create two matrices, 
\begin_inset Formula ${\bf E}\in\mathbb{R}^{E×|\mathbf{v}|}$
\end_inset

 and 
\begin_inset Formula $\mathbf{U}\in\mathbb{R}^{|\mathbf{v}|×E}$
\end_inset

.
 Where 
\begin_inset Formula $E$
\end_inset

 is an arbitrary size which defines the size of our embedding space and
 
\begin_inset Formula $|\mathbf{v}|$
\end_inset

 is a vocabulary size of all words from all contexts.
 
\begin_inset Formula ${\bf E}$
\end_inset

 is the input word matrix, such that the 
\begin_inset Formula $i$
\end_inset

-th column of 
\begin_inset Formula ${\bf E}$
\end_inset

 is the 
\begin_inset Formula $E$
\end_inset

-dimensional embedded vector for word 
\begin_inset Formula $\mathbf{c}_{i}$
\end_inset

.
 We denote this 
\begin_inset Formula $E×1$
\end_inset

 vector as 
\begin_inset Formula ${\bf e}_{i}$
\end_inset

.
 Similarly, 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is the output word matrix.
 The 
\begin_inset Formula $k-$
\end_inset

th row of 
\begin_inset Formula $\mathbf{U}$
\end_inset

 is an 
\begin_inset Formula $E$
\end_inset

-dimensional embedded vector for word 
\begin_inset Formula $\mathbf{c}_{\mathrm{pred},k}$
\end_inset

 when it is an output of the model.
 We denote this row of 
\begin_inset Formula $\mathbf{U}$
\end_inset

 as 
\begin_inset Formula ${\bf u}_{k}$
\end_inset

.
\end_layout

\begin_layout Standard
The matrices 
\begin_inset Formula $\mathbf{E}$
\end_inset

 and 
\begin_inset Formula $\mathbf{U}$
\end_inset

 are obtained by the following sequence of actions:
\end_layout

\begin_layout Itemize
We generate our one hot word vectors 
\begin_inset Formula $({\bf {\bf c}}^{(c−m)},...,{\bf {\bf c}}^{(c)},...,{\bf {\bf c}}^{(c+m)})$
\end_inset

 for the input context of size 
\begin_inset Formula $2m$
\end_inset


\end_layout

\begin_layout Itemize
We get our embedded word vectors for the context 
\begin_inset Formula $({\bf e}_{c−m}=\mathbf{E}{\bf c}^{(c−m)},{\bf e}_{c−m+1}=\mathbf{E}{\bf c}^{(c−m+1)},...,{\bf e}_{c+m}=\mathbf{E}{\bf c}^{(c+m)})$
\end_inset


\end_layout

\begin_layout Itemize
Average these vectors to get 
\begin_inset Formula $\tilde{{\bf e}}=\frac{{\bf e}_{c\text{−}m}+{\bf e}_{c\text{−}m+1}+...+{\bf e}_{c\text{+}m}}{2m}$
\end_inset


\end_layout

\begin_layout Itemize
Generate a score vector 
\begin_inset Formula ${\bf z}=\mathbf{U}\tilde{{\bf e}}$
\end_inset


\end_layout

\begin_layout Itemize
Turn the scores into probabilities 
\begin_inset Formula 
\begin{equation}
\hat{{\bf c}}_{\mathrm{pred}}=\mathrm{softmax}({\bf \mathbf{z}})\label{eq:softmax_cbow}
\end{equation}

\end_inset


\end_layout

\begin_layout Itemize
We optimize 
\begin_inset Formula $\mathbf{E}$
\end_inset

 and 
\begin_inset Formula $\mathbf{U}$
\end_inset

 to maximize the match between the predicted vector 
\begin_inset Formula $\hat{{\bf \mathbf{c}}}_{\mathrm{pred}}$
\end_inset

, and the true vector 
\begin_inset Formula ${\bf c}_{\mathrm{pred}}$
\end_inset

, which also happens to be the one hot vector of the actual word.
\end_layout

\begin_layout Standard
The described method can be interpreted as a Feed Forward Neural Network
 with only input and output layers.
 The optimization is provided with respect to cross entropy loss function
 
\begin_inset Formula 
\begin{equation}
L=\sum_{i=1}^{|\mathbf{v}|}\mathbf{c}_{\mathrm{pred},i}\log({\bf \hat{\mathbf{c}}}_{\mathrm{pred},i}),\label{eq:cross_entropy_cbow}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{\hat{c}}_{\mathrm{pred}}$
\end_inset

 is softmax (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:softmax_cbow"
plural "false"
caps "false"
noprefix "false"

\end_inset

) function.
 When the weights in a neural network are trained, matrix 
\begin_inset Formula $\mathbf{E}$
\end_inset

 represents 
\begin_inset Formula $|\mathbf{v}|$
\end_inset

 word embeddings where 
\begin_inset Formula $i$
\end_inset

-th word from vocabulary 
\begin_inset Formula $\mathbf{v}$
\end_inset

 is 
\begin_inset Formula $i$
\end_inset

-th row in 
\begin_inset Formula $\mathbf{E}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Previously, we mentioned that all the methods take an instance (text) 
\begin_inset Formula $\mathbf{x}$
\end_inset

 as an input value and predict its class.
 Instance 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is calculated as a mean value from all words embeddings in the text
\begin_inset Formula 
\[
\mathbf{x}_{i}=\frac{1}{|\mathcal{D}_{i}|}\sum_{j\in\mathcal{D}_{i}}\mathbf{E}\mathbf{c}^{(j)},
\]

\end_inset

where 
\begin_inset Formula $\mathcal{D}_{i}$
\end_inset

 is the set of indices of all words in 
\begin_inset Formula $i$
\end_inset

-th document in the common vocabulary.
\end_layout

\begin_layout Subsubsection
Fast Text Word Embeddings
\end_layout

\begin_layout Standard
As mentioned previously, the Fast Text method is a CBOW modification.
 The main modification is that Fast Text is taking into account not only
 words but also suffixes of words.
 The words are split into suffixes and as a result, they can handle understandin
g of the context better.
\end_layout

\begin_layout Standard
In this thesis, we used pretrained Fast Text models 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2018advances"
literal "false"

\end_inset

 consisting of 1 million word vectors trained on Wikipedia 2017, UMBC web
 base corpus and statmt.org news dataset (16B tokens).
\end_layout

\begin_layout Chapter
Data and Evaluation Metrics
\end_layout

\begin_layout Section
Evaluation Metrics
\end_layout

\begin_layout Standard
When the models are implemented and trained we have to compare them.
 This part is very important because we want to define such metrics that
 will not be biased and which will have high discriminability.
 In this project, experiments are separated into two parts.
 The first part is a supervised classification with a big amount of the
 training data.
 This is done for classifiers' maximal upper bound understating.
 The upper bounds are used as maxima to which our active learning algorithms
 should be converging.
 
\end_layout

\begin_layout Subsection
Receiver Operating Characteristic Metric
\end_layout

\begin_layout Standard
In section 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Introduction-to-Decision"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we mentioned that we are limiting our problem only on binary classification.
 Plenty of metrics, such as recall, accuracy, precision, etc.
 exists for binary classification scoring.
 However, we decided to find a metric that is able to unify all metrics
 discussed before and do not underperform each of them.
 For these purposes, we chose the Receiver Operating Characteristic (ROC)
 metric.
 ROC visualizes the tradeoff between the true positive rate (TPR) 
\begin_inset Formula 
\[
\mathrm{TPR}=\frac{\mathrm{true\ positive}}{\mathrm{true\ positive+false\ negative}}
\]

\end_inset

 and the false positive rate (FPR) 
\begin_inset Formula 
\[
\mathrm{FPR}=\frac{\mathrm{false\ positive}}{\mathrm{false\ positive+true\ negative}},
\]

\end_inset

where terminology true/false positive/negative refers to assigned classification
 being correct or incorrect with respect to positive or negative category
 
\begin_inset CommandInset citation
LatexCommand cite
key "fawcett2006introduction"
literal "false"

\end_inset

.
 This means that for every threshold we are able to calculate TPR and FPR,
 and plot it in one figure.
\end_layout

\begin_layout Standard
We are working with balanced datasets.
 Thus, there is no problem with using ROC metric.
 ROC metric is also quite efficient when we care equally about positive
 and negative classes.
 Another advantage is that if we notice small changes in ROC, it will not
 result in big changes in other binary classification metrics.
\end_layout

\begin_layout Standard
The metric results should be aggregated into a single number in order to
 obtain a unique comparison value.
 This is typically done as calculation of the area under the ROC, which
 is known as the area under the curve (AUC) metric.
 The probabilistic interpretation of ROC score means that if a positive
 case and a negative case are chosen randomly, the probability that the
 positive case outranks the negative case, according to the classifier,
 is given by the AUC 
\begin_inset CommandInset citation
LatexCommand cite
key "fawcett2006introduction"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Supervised Learning Results Validation
\end_layout

\begin_layout Standard
As mentioned above, supervised learning results are used as a maximal upper
 bound of the specific classifier.
 In order to make results statistically valid, we used k-fold cross validation.
 For each batch from k-fold cross validation we calculated both ROC and
 AUC.
 As an output result of a classifier performance we calculated mean value
 over all ROC and AUC scores.
 All results are calculated with respect to balanced data classes.
\end_layout

\begin_layout Subsection
Active Learning Results Validation
\end_layout

\begin_layout Standard
Active learning model evolution is based on supervised learning algorithms
 that are sequentially retrained.
 Thus, we are not able to display ROC for active learning algorithms because
 the amount of results is too big.
 We decided to aggregate results and display the evolution of AUC metric
 for each step of the active learning sequence.
 AUC sequences can be well compared over different classifiers.
 Another aspect of data validation is making the results statistically significa
nt.
 We are not able to use k-fold cross validation for active learning algorithms.
 Therefore, we run the active learning algorithm 
\begin_inset Formula $H\in\mathbb{N}$
\end_inset

 times with different random selection of the initial training set.
 Due to the random initializations, we are able to determine uncertainty
 bounds that are calculated as standard deviations from the mean values.
 
\end_layout

\begin_layout Section
Data 
\begin_inset CommandInset label
LatexCommand label
name "chap:Data"

\end_inset


\end_layout

\begin_layout Standard
This chapter is dedicated to the dataset description.
 We used two datasets for algorithms training and testing.
 We consider these datasets big and diverse enough for getting unbiased
 results.
 We took into account the size of texts (articles and tweets), diversity
 and at the same time, similarity of topics.
 
\end_layout

\begin_layout Subsection
HuffPost 200k Articles Dataset
\end_layout

\begin_layout Standard
HuffPost 200k Articles Dataset is publicly available at Kaggle competition
 webpage and can be found as News Category Dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "dataset"
literal "false"

\end_inset

 here 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

https://www.kaggle.com/rmisra/news-category-dataset
\end_layout

\end_inset

.
 The described dataset contains around 200k news headlines from the year
 2012 to 2018 obtained from HuffPost web journal.
 The following dataset includes a url address and a label for each article.
 HuffPost dataset has 200k articles assigned to 41 categories.
 We used only 10 categories.
 We are interested in binary classification, as a result we have to make
 pairs from chosen categories.
 These categories and pairs are listed in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:HuffPost_Dataset_Categories"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Tuple Id
\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Category Pairs
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Crime
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Good News
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $2$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Sports
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Comedy
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Politics
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Business
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $4$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Science
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Tech
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $5$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Education
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
College
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:HuffPost_Dataset_Categories"

\end_inset

HuffPost Dataset Categories which were chosen for algorithms' training and
 testing
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Due to the fact that there is no raw article included in the dataset, we
 used url links in order to find the articles.
 For each category, we scraped 500 original articles from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

www.huffpost.com
\end_layout

\end_inset

.
 Listed categories are chosen with respect to diversity and classification
 complexity.
 We sorted the categories in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:HuffPost_Dataset_Categories"
plural "false"
caps "false"
noprefix "false"

\end_inset

 with respect to the ascending classification complexity order.
 By classification complexity we mean two sets intersection in feature space.
 If the classification complexity is high, the majority of feature space
 dimensions have intersections between two datasets.
 Thus, it is harder to find such set of features that can be used for high
 classification performance.
 
\end_layout

\begin_layout Subsection
1600k Tweets Dataset
\end_layout

\begin_layout Standard
Another dataset that is used in this work is 1600k tweets dataset, which
 is publicly available and is also taken from Kaggle competition webpage.
 The dataset can be found as sentiment140 dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "tweets_dataset"
literal "false"

\end_inset

 at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://www.kaggle.com/kazanova/sentiment140
\end_layout

\end_inset

.
 This dataset contains 1,600,000 tweets extracted using Twitter API.
 The tweets have been annotated as negative (
\begin_inset Formula $0$
\end_inset

), positive (
\begin_inset Formula $4$
\end_inset

) and they are used for sentiment detection.
 Same as with HuffPost dataset we used 500 records for each category, for
 training and testing purposes.
 The reason of choosing this dataset is that we wanted to show how our algorithm
s can handle data that consist of little texts.
 
\end_layout

\begin_layout Chapter
Project Implementation and Architecture
\end_layout

\begin_layout Standard
Even though this work is quite theoretical with experiments that prove theoretic
al concepts, we consider the implementational part interesting as well.
 In this chapter, we show the architecture of the project and explain how
 different dependencies cooperate with each other.
 The codebase of this project can be easily found at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/sahanmar/Peony
\end_layout

\end_inset


\shape italic
.

\shape default
 We expect this project to grow continuously and be used not only in terms
 of master thesis.
 
\end_layout

\begin_layout Standard
This project was written in Python 3.7 programming language with the usage
 of Conda environment.
 The project combines a lot of different tools and programs such as Docker,
 Docker-Compose, MongoDb, Jupyter, etc..
\end_layout

\begin_layout Standard
In this thesis, we used two main components that represent the database
 and a computational part.
 We tried to unify all methods as much as possible and make the utilization
 process very easy.
\end_layout

\begin_layout Section
Database
\end_layout

\begin_layout Standard
In order to make everything consistent and let the models work with the
 same input and output format we decided to create a database that will
 store all the data in JSON format.
 This unification lets us connect the database to machine learning and visualiza
tion components.
 In this project we decided to work with NoSQL database.
 Our choice was MongoDb.
 The reason why we have chosen MongoDb is because of its simplicity and
 possibility of maintaining through Docker.
 Since Docker and MongoDb is a perfect combination, the database can be
 deployed with two lines of code through Docker-Compose as explained in
 documentation on GitHub.
 Of course, it is easier to use MongoDb without Docker but our motivation
 was measured on the simplicity of creating and working with the database.
 All experiments were run on Google Cloud Platform virtual machine instance.
 Thus, we could start working with the models right away without any complicatio
ns with the installation.
 
\end_layout

\begin_layout Subsection
MongoDb Data Format
\end_layout

\begin_layout Standard
MongoDb represents the data in BSON format behind the scenes but we insert
 and get the data in JSON format.
 Despite the fact that we are having different text datasets which we store
 in the database, we decided to create a unified JSON scheme that will let
 us preserve the structure of the data stored in MongoDb.
 JSON schema of how the data are stored and what a user will get as an output
 from a database is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MongoDb_JSON_schema"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Deeper explanation of JSON schema format can be found at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://json-schema.org/understanding-json-schema/
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset include
LatexCommand lstinputlisting
filename "MongoDb_structure.json"
lstparams "breaklines={true //--> breaks lines to margin},basicstyle={\\small}"

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MongoDb_JSON_schema"

\end_inset

MongoDb JSON schema visualization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Computations
\end_layout

\begin_layout Standard
All computations were done with the usage of virtual instance on Google
 Cloud Platform.
 We used configuration with 
\begin_inset Formula $2$
\end_inset

 CPU, 
\begin_inset Formula $7.5$
\end_inset

Gb memory that was running on Debian GNU/Linux 10.
 All versions of python, python packages, docker, mongo, etc.
 can be found in .yml files in GitHub folder with the project.
 
\end_layout

\begin_layout Section
Machine Learning Component
\begin_inset CommandInset label
LatexCommand label
name "sec:Machine-Learning-Component"

\end_inset


\end_layout

\begin_layout Standard
Machine Learning (ML) Component is fully implemented in Python with the
 usage of open source libraries.
 In order to understand how to use the models, it is possible to find the
 code and its usage in Jupyter notebook that is stored in the showcase folder.
 Showcase folder has four Jupyter notebooks that show both how to get the
 data for the models from the database and how to start using the models.
 
\end_layout

\begin_layout Subsection
Data Transformers
\end_layout

\begin_layout Standard
Before models training and testing, the user has to fit the data transformer
 that transforms text into tensors form.
 Tensors are used as input values for models.
 As mentioned in chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Natural_Language_Processing"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we are working only with Fast Text and TF-IDF text encodings.
 Both TF-IDF and Fast Text models are fitted from the documents that are
 given to the transformer.
 
\end_layout

\begin_layout Subsubsection
TF-IDF Transformer
\end_layout

\begin_layout Standard
Basic concept is that TF-IDF transformer represents one article as a vector.
 TF-IDF encoding for a single document is calculated on the basis of all
 extracted words that exist in the vocabulary.
 As a result, if we make TF-IDF encoding of a set of articles, we will get
 a matrix where each row represents a specific document and each column
 represents a word from a dictionary.
\end_layout

\begin_layout Subsubsection
Fast Text Transformer
\end_layout

\begin_layout Standard
Fast Text model is a pretrained model that consists of one million words
 mapped to vectors.
 These words are stored in MongoDb.
 When a user starts to use the Fast Text model, ML component creates a words'
 vocabulary from the texts taken for model training/testing.
 This vocabulary is created in the form of a hash map (word -> vector) where
 word embeddings are downloaded from MongoDb.
 It is important to remember that Fast Text encoding represents each word
 as a vector with predefined number of components.
 We are using word embeddings that represent each word with 
\begin_inset Formula $300$
\end_inset

 float values.
 We introduce article encoding as a mean value through all words from a
 text that is given for encoding.
 Thus, if we make Fast Text encoding of a set of articles, we will get a
 matrix where each row represents a specific document.
 A huge advantage of this method in comparison to TF-IDF is that we are
 working only with 
\begin_inset Formula $300$
\end_inset

 float values than with huge vocabulary (all unique words from a text corpus).
 Therefore, we get both lower features dimensionality and better context
 understanding.
\end_layout

\begin_layout Subsection
Machine Learning
\end_layout

\begin_layout Standard
In this work, we created the Generalized Model that unifies all models.
 Generalized Model allows us to work with each Machine Learning algorithm
 in the same way.
 Generalized Model is able to take a data transformer as an input argument.
 This feature makes it easier to work with models.
 In the first chapter of this thesis, we introduced our models the way that
 we want to sample from their parameters' distributions.
 In other words, we defined ensembles models.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Machine_Learning_Workflow_diagram"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is shown a generalized diagram of a machine learning structure.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Model_diagram.png
	scale 35

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Machine_Learning_Workflow_diagram"

\end_inset

Machine Learning Workflow
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We used scikit-learn 
\begin_inset CommandInset citation
LatexCommand cite
key "scikit-learn"
literal "false"

\end_inset

 for basic algorithms such as Random Forests and SVMs.
 However, the core of this project is constructed around neural networks.
 We used PyTorch 
\begin_inset CommandInset citation
LatexCommand cite
key "NEURIPS2019_9015"
literal "false"

\end_inset

 as a neural networks framework.
 In this work, we have implemented and tested five classification algorithms.
 Three of them, such as SVM, Random Forest, and Feed Forward Neural Network
 ensembles are trained on randomly chosen subsets from the training data.
 For each ensemble are randomly chosen 
\begin_inset Formula $80\%$
\end_inset

 from training data that are used for training.
 Two algorithms such as SGLD and DENFI are using a full training dataset
 for their ensembles.
 The variability in SGLD and DENFI ensembles is reached through adding Gaussian
 noise while ensembles training.
 We hardcoded amount of ensembles for all models to 
\begin_inset Formula $10$
\end_inset

, except SGLD where are used 50 ensembles.
\end_layout

\begin_layout Subsubsection
SVM and Random Forest Ensemble Setup
\end_layout

\begin_layout Standard
Both SVM and Random Forest models were taken and used out of the box.
 We created 
\begin_inset Formula $10$
\end_inset

 SVM and 
\begin_inset Formula $10$
\end_inset

 Random Forest ensembles with default scikit-learn setting.
 No modifications were provided.
\end_layout

\begin_layout Subsubsection
Feed Forward Neural Network
\end_layout

\begin_layout Standard
Despite the fact that we used very simple neural network architecture it
 showed great results.
 We implemented Feed Forward Neural Network with the usage of PyTorch python
 package with only an input layer that consists of 
\begin_inset Formula $100$
\end_inset

 neurons with a sigmoid activation function.
 We chose the softmax activation function for an output layer.
 
\end_layout

\begin_layout Subsubsection
SGLD
\end_layout

\begin_layout Standard
For SGLD we used the same configuration as for Feed Forward Neural Network.
 One significant difference is that we used the whole training dataset and
 were adding Gaussian noise to a Gradient Descent 
\begin_inset CommandInset citation
LatexCommand cite
key "welling2011bayesian"
literal "false"

\end_inset

 as shown in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SGLD_gradient_descent"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 We used 
\begin_inset Formula $2000$
\end_inset

 epochs in order to train the model.
 Next, we started a sampling procedure.
 Each sample was generated with a 
\begin_inset Formula $50$
\end_inset

 epochs interval.
 The precise configuration of SGLD can be found in GitHub project here 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

https://github.com/sahanmar/Peony/blob/master/Peony_project/Peony_box/src/peony_a
djusted_models/sgld_nn.py
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Details: how many epochs, how many steps between samples? Done
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
DENFI
\end_layout

\begin_layout Standard
In this work, we have simplified the original DENFI algorithm.
 The pseudocode of this algorithm is shown in algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DENFI_modification_algorithm"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The main idea is that the algorithm finds different local minimums due
 to the random weights initialization in the first active learning iteration.
 We used 
\begin_inset Formula $2000$
\end_inset

 epochs in order to train the model.
 When the training is finished, Gaussian noise is added to the output weights
 in order to increase the variability.
 In further active learning iterations, weights from previous iterations
 with extended training dataset are used.
 After the training, we also add Gaussian noise to the weights.
 We have empirically discovered that the model shows the best performance
 with 
\begin_inset Formula $0.1$
\end_inset

 variance of an additive noise.
 We have also tried different noise configurations with 
\begin_inset Formula $0.2$
\end_inset

 and 
\begin_inset Formula $0.3$
\end_inset

 variance.
 However, Gaussian noise with 
\begin_inset Formula $0.1$
\end_inset

 variance has shown the best scores.
 The amount of training epochs in hot start loop equals to 
\begin_inset Formula $500$
\end_inset

 epochs.
 When the learning iterations are done, we can use the algorithm for predicting.
 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

def active_learning_iteration_training(all_ensembles, training_data):
\end_layout

\begin_layout Plain Layout

  for ensemble in all_ensembles:
\end_layout

\begin_layout Plain Layout

    if first_active_learning_iteration is False:
\end_layout

\begin_layout Plain Layout

      ensemble.weights = ensemble.weights_prev_iteration 
\end_layout

\begin_layout Plain Layout

      training_epochs = 500
\end_layout

\begin_layout Plain Layout

    else:
\end_layout

\begin_layout Plain Layout

      ensemble.weights = gaussian_initialization(mean=0, var=0.1)
\end_layout

\begin_layout Plain Layout

      training_epochs = 2000
\end_layout

\begin_layout Plain Layout

    ensemble.train(training_data)
\end_layout

\begin_layout Plain Layout

	ensemble.weights = ensemble.weights 
\end_layout

\begin_layout Plain Layout

                        + gaussian_noise(mean=0, var=0.1)
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

		
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "alg:DENFI_modification_algorithm"

\end_inset

DENFI modification algorithm pseudocode
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Details: how many epochs, what variance? Done
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Passive Learning Classification
\end_layout

\begin_layout Standard
Before we can start the active learning part, we have to understand if the
 implemented algorithms are capable to solve the classification task.
 Thus, we decided to test the models on Sports and Comedy categories from
 the HuffPost Dataset and on tweets from the Tweets Dataset.
 The classification was done both for the TF-IDF and Fast Text encodings.
 We separated experiments with respect to the dataset types.
\end_layout

\begin_layout Section
Passive Learning HuffPost Dataset
\end_layout

\begin_layout Standard
In this section, we are illustrating ROC and AUC metrics with respect to
 
\begin_inset Formula $10$
\end_inset

-fold cross validation and 
\begin_inset Formula $500$
\end_inset

 Sports, 
\begin_inset Formula $500$
\end_inset

 Comedy articles.
 Vocabulary, that is created from 
\begin_inset Formula $1000$
\end_inset

 articles corpus consists of 
\begin_inset Formula $20$
\end_inset

 thousand unique words.
 We would like to mention that all algorithms are trained and tested with
 respect to 
\begin_inset Formula $10$
\end_inset

 ensembles
\begin_inset Note Note
status open

\begin_layout Plain Layout
proc? Done
\end_layout

\end_inset

.
 We decided to test the ensemble models on a passive learning problem in
 order to see the limit that an active learning strategy will converge to.
 Moreover, the ratio of randomly chosen training data for ensembles (SMV,
 Random Forest, Feed Forward NN ensembles) is set to 
\begin_inset Formula $80\%$
\end_inset

.
 
\end_layout

\begin_layout Subsection
SVM Ensembles
\end_layout

\begin_layout Standard
Results for SVM Ensembles are shown in figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_SVM_passive_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SVM_passive_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 These are results both for TF-IDF and Fast Text encodings.
 As seen on these plots, ROC and AUC values of 
\begin_inset Formula $10$
\end_inset

-fold cross validation are very high.
 This means that our model works very good.
 Another interesting point is that the standard deviation with respect to
 all runs is very low.
 It means that SVM ensembles could linearly separate Sports and Comedy sets
 with an acceptable classification error.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Passive Learning/SVM_tfidf_ROC.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:TF-IDF_SVM_passive_learning"

\end_inset

TF-IDF ROC and AUC for 10 SVM ensembles trained and tested on Sports and
 Comedy data where each ensemble is trained on 
\begin_inset Formula $80\%$
\end_inset

 of randomly chosen training data 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Passive Learning/SVM_fast_text_ROC.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SVM_passive_learning"

\end_inset

Fast Text ROC and AUC for 10 SVM ensembles trained and tested on Sports
 and Comedy data where each ensemble is trained on 
\begin_inset Formula $80\%$
\end_inset

 of randomly chosen training data 
\end_layout

\end_inset


\end_layout

\end_inset

It is also seen that ROC and AUC metrics are almost the same for TF-IDF
 and Fast Text encodings.
 However, there is one significant difference.
 The difference is in computational time because Fast Text encoded text
 document consists of 
\begin_inset Formula $300$
\end_inset

 float components and the TF-IDF encoded text document consists of 
\begin_inset Formula $20$
\end_inset

 thousand float components.
 Even though we are using algorithms for sparse matrix computations for
 the TF-IDF method, computations for Fast Text encoding are approximately
 five times faster.
 The higher amount of unique words is, the higher elapsed time per article
 for TF-IDF based encoding algorithm will be.
 Another good aspect of SVM is the algorithm's simplicity.
 SVM is trained much faster than neural network based models.
 
\end_layout

\begin_layout Standard
It is possible to conclude that the model achives good results for this
 classification problem and can be used for active learning experiments.
 
\end_layout

\begin_layout Subsection
Random Forest Ensembles
\end_layout

\begin_layout Standard
Results for the Random Forest ensembles are shown in figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_RF_passive_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_RF_passive_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The displayed results are both for the TF-IDF and the Fast Text encodings.
 Same as in the SVM section, ROC and AUC values of 
\begin_inset Formula $10$
\end_inset

-fold cross validation for Random Forest are high as well.
 This means that our model works well.
 ThesStandard deviation with respect to all runs is also low.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Passive Learning/RF_tfidf_ROC.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:TF-IDF_RF_passive_learning"

\end_inset

TF-IDF ROC and AUC for 10 Random Forest ensembles trained and tested on
 Sports and Comedy data where each ensemble is trained on 
\begin_inset Formula $80\%$
\end_inset

 of randomly chosen training data 
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Passive Learning/RF_fast_text_ROC.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_RF_passive_learning"

\end_inset

Fast Text ROC and AUC for 10 Random Forest ensembles trained and tested
 on Sports and Comedy data where each ensemble is trained on 
\begin_inset Formula $80\%$
\end_inset

 of randomly chosen training data 
\end_layout

\end_inset


\end_layout

\end_inset

If we compare the results for TF-IDF and Fast Text encodings, it is seen
 that the Fast Text based model outperforms the TF-IDF encoding model.
 ThemMean AUC value for the Fast Text article encoding model is higher by
 
\begin_inset Formula $5\%$
\end_inset

.
 It is interesting that the TF-IDF sparse matrix algorithm application in
 Random Forest ensembles model results in almost the same computational
 time for Fast Text and TF-IDF encoding algorithms.
 This observation was made on the basis of processing 
\begin_inset Formula $1000$
\end_inset

 articles from the Comedy and Sports categories.
 As mentioned in the SVM section, we can also say that the Random Forest
 algorithm is trained much faster than the neural network based models.
 
\end_layout

\begin_layout Standard
It is possible to conclude that the model shows good results for this classifica
tion problem and can be used for active learning experiments.
 
\end_layout

\begin_layout Subsection
Feed Forward Neural Network Ensembles
\end_layout

\begin_layout Standard
In this work, we have implemented three algorithms based on neural networks,
 such as neural networks ensembles, SGLD, and DENFI algorithms.
 The difference between these methods is in parameters sampling.
 Neural networks ensembles take a randomly selected subset from the training
 data for each ensemble.
 On the other hand, SGLD and DENFI require special training.
 Thus, we decided not to show SGLD and DENFI results in this section.
 Consequently, if neural networks ensembles achieve good results, we assume
 that SGLD and DENFI will also perform well for the passive learning scenario.
\end_layout

\begin_layout Standard
Results for the neural networks ensembles are shown in figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_NN_passive_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_NN_passive_learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The displayed results are both for the TF-IDF and Fast Text encodings.
 As seen in the previous section, ROC and AUC values of 
\begin_inset Formula $10$
\end_inset

-fold cross validation for neural networks are high as well.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Passive Learning/NN_tfidf_ROC.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:TF-IDF_NN_passive_learning"

\end_inset

TF-IDF ROC and AUC for 10 neural networks ensembles trained and tested on
 Sports and Comedy data where each ensemble is trained on 
\begin_inset Formula $80\%$
\end_inset

 of randomly chosen training data 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Passive Learning/NN_fast_text_ROC.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_NN_passive_learning"

\end_inset

Fast Text ROC and AUC for 10 neural networks ensembles trained and tested
 on Sports and Comedy data where each ensemble is trained on 
\begin_inset Formula $80\%$
\end_inset

 of randomly chosen training data 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Comparing the results between the TF-IDF and Fast Text encoding based models
 we can observe that the Fast Text based model gives slightly better results.
 We have already discussed the fastness of algorithm training with respect
 to different embedding models in the Random Forest section.
 In the case of neural networks, this difference is even more significant.
 The model that is based on the Fast Text word embeddings takes approximately
 
\begin_inset Formula $20$
\end_inset

 times less computational time than the TF-IDF encoding based model.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
All of the tested models achieved really good results in the task of classificat
ion documents into the Sports and Comedy categories.
 We can not rule out any of them as irrelevant and we will test these algorithms
 in the active learning section.
 We would like to highlight that the Fast Text encoding based algorithms
 showed a bit better results than TF-IDF.
 Fast Text based algorithms also showed significant improvement in the evaluatio
n speed of algorithm training.
\end_layout

\begin_layout Chapter
Active Learning Classification
\end_layout

\begin_layout Standard
In the Passive Learning section, we showed that all implemented algorithms
 are achieve good results for solving passive text classification tasks.
 The results of Active Learning are the main contribution of this thesis.
 Therefore, we tested all five algorithms on the data mentioned in the Data
 section.
 The results are shown and described in further subsections.
 However, before starting with text classification results, we would like
 to show how our models represent uncertainty.
 As written in the theoretical introduction to active learning, we use models'
 uncertainty for an active learning loop.
 
\end_layout

\begin_layout Section
Active Learning Models' Uncertainty
\end_layout

\begin_layout Standard
Due to the fact that dimensionality of the feature space of both text encoding
 approaches is extremely high, we introduce a 
\begin_inset Formula $2$
\end_inset

-dimensional toy problem for uncertainty visualization.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Toy_problem_dataset"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is shown a conventional two-moon dataset that is used for the toy problem
 classification.
 We generated this dataset by adding Gaussian noise in order to make the
 task similar to real-world problems.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/Toy_problem_dataset.png
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Toy_problem_dataset"

\end_inset

Visualization of the toy problem dataset.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We generated 
\begin_inset Formula $1000$
\end_inset

 data points where 
\begin_inset Formula $500$
\end_inset

 are assigned to class 
\begin_inset Formula $0$
\end_inset

 and another 
\begin_inset Formula $500$
\end_inset

 points are assigned to class 
\begin_inset Formula $1$
\end_inset

.
 We used 
\begin_inset Formula $50\%$
\end_inset

 randomly chosen data samples as the training dataset.
 The next step was creating a two-dimensional grid that will be used for
 model predictions.
 We assign data sample to class 
\begin_inset Formula $1$
\end_inset

 if the prediction value is higher than 
\begin_inset Formula $0.5$
\end_inset

.
 If the prediction value is lower than 
\begin_inset Formula $0.5$
\end_inset

, then the value is assigned to class 
\begin_inset Formula $0$
\end_inset

.
 We consider that the model is uncertain about a specific data sample if
 its prediction values is close to 
\begin_inset Formula $0.5$
\end_inset

.
 As a result, sampling values from the maximal uncertainty region will bring
 maximum information about the dataset.
 Setup of models for the toy problem is the same as it is defined in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Machine-Learning-Component"
plural "false"
caps "false"
noprefix "false"

\end_inset

 .
\end_layout

\begin_layout Subsection
SVM Ensembles
\end_layout

\begin_layout Standard
Uncertainty for SVM ensembles model is visualized in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SVM_Ensembles_posterior_toy_problem"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Uncertainty bounds are linear and quite narrow.
 The linearity of uncertainty bounds is explained with the fact that we
 are using SVMs with a linear kernel.
 Narrowness can be explained with the richness of the training dataset and
 linear limitations of the SVM decision boundary.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/SVM_toy_problem_uncertainty.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SVM_Ensembles_posterior_toy_problem"

\end_inset

Mean value of posterior prediction of class 
\begin_inset Formula $1$
\end_inset

 for SVM ensembles.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Random Forest Ensembles
\end_layout

\begin_layout Standard
Model uncertainty of the Random Forest ensembles is visualized in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RF_Ensembles_posterior_toy_problem"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In the case of Random Forest ensembles, we can see that uncertainty bounds
 are not linear and lay near the region where the two classes are split.
 We see that the uncertainty region is becoming wider near the places where
 datapoint of two different classes lay close to each other.
 This is a behavior that we wanted to observe.
 
\end_layout

\begin_layout Standard
It is also seen that the curve is not smooth enough.
 This is caused by the shape of the Random Forest decision boundary.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/RF_toy_problem_uncertainty.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:RF_Ensembles_posterior_toy_problem"

\end_inset

Mean value of posterior prediction of class 1 for Random Forest ensembles.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Neural Network Ensembles
\end_layout

\begin_layout Standard
Model uncertainty of the neural network ensembles is visualized in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NN_Ensembles_posterior_toy_problem"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In comparison to the methods that were shown above, uncertainty bounds
 are quite smooth.
 We can also observe that uncertainty bounds become wider when they go further
 away from the data points.
 This behavior can be explained by the fact that in these places the algorithm
 did not get any training data samples.
 We could also see this behavior in SVM ensembles, but the neural network
 models represent the uncertainties much better.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/NN_toy_problem_uncertainty.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:NN_Ensembles_posterior_toy_problem"

\end_inset

Neural network ensembles posterior predictive mean probability of class
 
\begin_inset Formula $1$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
SGLD
\end_layout

\begin_layout Standard
Model uncertainty of neural network estimated by SGLD is visualized in In
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SGLD_Ensembles_posterior_toy_problem"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We see that SGLD algorithm has similar behavior to neural network ensembles.
 The difference is that all uncertainty bound curves have similar curvature.
 This is happening due to the fact that SGLD finds a loss function minimum
 and then samples parameters' values in a neighborhood of the minimum.
 As a result, we expect decision bound for each parameters sample to be
 quite similar to each other.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/SGLD_toy_problem_uncertainty.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SGLD_Ensembles_posterior_toy_problem"

\end_inset

SGLD posterior predictive mean probability of class 
\begin_inset Formula $1$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
DENFI
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DENFI_Ensembles_posterior_toy_problem"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is visualized DENFI model uncertainty.
 We see that uncertainty bounds are very similar to neural network ensembles
 algorithms but still a bit different.
 As told in pseudocode 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:DENFI_modification_algorithm"
plural "false"
caps "false"
noprefix "false"

\end_inset

, algorithm founds different local minimums and then we add some Gaussian
 noise to parameters values.
 In further learning iterations the algorithm continues training using the
 weights from the previous step.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/DENFI_toy_problem_uncertainty.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DENFI_Ensembles_posterior_toy_problem"

\end_inset

DENFI posterior predictive mean probability of class 
\begin_inset Formula $1$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

The last 
\begin_inset Formula $100$
\end_inset

 loss function values with respect to each DENFI ensemble are shown in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:DENFI_Ensembles_100_loss_values"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 It is seen that each ensemble found its own local minimum.
 Additional Gaussian noise adds more variability to the algorithm that makes
 samples more diverse.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/Toy_problem/DENFI_toy_ensemble_losses.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:DENFI_Ensembles_100_loss_values"

\end_inset

The last 
\begin_inset Formula $100$
\end_inset

 loss function values with respect to each DENFI ensemble
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\begin_layout Standard
To conclude, we can say that the best variability representation was seen
 in neural network models.
 Obviously, we could test SVM algorithm with different kernels but our prior
 interest was concentrated on neural networks due to their scalability to
 larger datasets.
 We have shown that all models are able to represent variability and can
 be used for further tests.
\end_layout

\begin_layout Section
Active Learning Simulation Set Up
\begin_inset CommandInset label
LatexCommand label
name "sec:Active_Learning_Simulation_set_up"

\end_inset


\end_layout

\begin_layout Subsection
Simulation Loop
\end_layout

\begin_layout Standard
The active learning simulation is designed to compare two strategies: i)
 random, and ii) active (smart) selection of text documents.
 We randomly choose an initial training set that has 
\begin_inset Formula $10$
\end_inset

 samples.
 These 
\begin_inset Formula $10$
\end_inset

 random samples are chosen from 
\begin_inset Formula $1000$
\end_inset

 text documents (
\begin_inset Formula $500$
\end_inset

 text documents per category).
 We reduce the size of all the above-mentioned datasets to 
\begin_inset Formula $1000$
\end_inset

 documents.
 Each dataset is split into the testing and training data.
 
\end_layout

\begin_layout Standard
Next, we initialize two runs.
 The first run is based on a random selection of the text documents and
 the second run is based on acquisition function selection of the documents.
 Both runs start with the same training dataset, and then they choose additional
 training documents based on their strategies.
 We consider continuous new data selection from 1000 documents dataset and
 imitating the annotators labeling process.
 All in all, we repeat selection of text samples 200 times (
\begin_inset Formula $U=200$
\end_inset

).
 We select 
\begin_inset Formula $10$
\end_inset

 random samples in the beginning, train our model, and make a prediction
 on the complement to the training dataset (
\begin_inset Formula $990$
\end_inset

 text documents from validation set).
 As a further step, we select 
\begin_inset Formula $1$
\end_inset

 new sample from the set on which the prediction was done.
 A new text sample for labeling is chosen with respect to acquisition function
 or random choice.
 Before we extend our training dataset with a new labeled sample, we calculate
 the AUC metrics on the complement to the dataset (
\begin_inset Formula $990$
\end_inset

 text documents).
 Thus, by the end of the simulation, our training set will have 
\begin_inset Formula $210$
\end_inset

 text documents and the testing set (complement to a training set) will
 have 
\begin_inset Formula $790$
\end_inset

 data samples.
 In order to make our results statistically valid, we repeat the described
 simulation loop 
\begin_inset Formula $10$
\end_inset

 times.
\end_layout

\begin_layout Subsection
Epsilon Greedy Strategy
\end_layout

\begin_layout Standard
In this work, we decided that we do not want to sample with respect to an
 acquisition function from the beginning but follow the epsilon greedy strategy
 
\begin_inset CommandInset citation
LatexCommand cite
key "watkins1989learning"
literal "false"

\end_inset

.
 We decided that both active learning and random strategy are going to start
 with random sampling.
 The algorithms have a coefficient 
\begin_inset Formula $\epsilon\in[0,1)$
\end_inset

 that represents a probability of the data sampling with respect to the
 acquisition function.
 In this work, we define 
\begin_inset Formula $\epsilon$
\end_inset

 as 
\begin_inset Formula 
\begin{equation}
\epsilon=\begin{cases}
\frac{\exp(u-40)}{\exp(u-40)+1}, & u\in\{1,...,U\}\\
0, & u=0,
\end{cases}\label{eq:alpha_greedy_function}
\end{equation}

\end_inset

where 
\begin_inset Formula $u=\{0,1,...,U\}$
\end_inset

 is set of questions that results in the number of text documents which
 will be labeled by an annotator.
 As mentioned in the previous section, the number of iterations (questions
 
\begin_inset Formula $U$
\end_inset

) is set to 
\begin_inset Formula $200$
\end_inset

.
 From the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:alpha_greedy_function"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is seen that when we reach 
\begin_inset Formula $40-$
\end_inset

th document, the probability of random sampling is 
\begin_inset Formula $50\%$
\end_inset

.
 This strategy provides good results which will be shown in further sections.
\end_layout

\begin_layout Section
Active Learning on Texts with TF-IDF Encoding Based Models
\begin_inset CommandInset label
LatexCommand label
name "sec:Active_Learning_on_TF-IDF_models"

\end_inset


\end_layout

\begin_layout Standard
We decided to show the results for the TF-IDF encoding based model because
 this text encoding provides high discriminability and it is relatively
 simple at the same time.
 However, as mentioned in the passive learning section, models that are
 using this type of encoding are harder to train due to the high dimensionality
 of features.
 We were not able to train neural network ensembles because of the high
 dimensional feature space.
 Thus, we decided to show the results only with respect to SVM, Random Forest
 and Sports, Comedy categories.
 Therefore, there is no need to split further section for different datasets
 because all the experiments based on TF-IDF encoding are assumed to be
 done only for the Sports and Comedy categories.
\end_layout

\begin_layout Subsection
SVM Ensembles
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for SVM ensembles with
 TF-IDF encoding.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/TF-IDF/TF-IDF_SVM_active_learning.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:TF-IDF_SVM_ensembles"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SVM ensembles algorithm with TF-IDF encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the results for entropy are not better than for random selection.
 This can be explained due to the low variability of SVM ensembles.
 In this case, we can conclude that there is no significant difference between
 entropy and random data selection strategy.
 The first and the second strategy has almost the same uncertainty bounds
 and converges to the same AUC results.
 
\end_layout

\begin_layout Subsection
Random Forest Ensembles
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_Random_Forest_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the Random Forest ensembles
 with TF-IDF encoding.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/TF-IDF/TF-IDF_Random_Forest_active_learning.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:TF-IDF_Random_Forest_ensembles"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the Random Forest ensembles algorithm with TF-IDF encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In comparison to SVM ensembles, the active learning strategy for Random
 Forest ensembles shows really poor results.
 Random data selection overcomes the entropy selection, and it can be said
 that the active learning strategy is not working at all for this algorithm.
 The reason of such behavior can be explained due to the little amount of
 training data in the beginning.
 This fact may make the algorithm to start selecting the data which have
 high entropy but are close to each other.
 As a result, it will not lead to high-performance results.
 
\end_layout

\begin_layout Subsection
Conclusion
\end_layout

\begin_layout Standard
Even though we were able to see extremely good results for TF-IDF encoding
 in the Passive Learning section, we were not able to train neural networks
 models and test active learning there, due to the high dimensionality of
 the feature space.
 Moreover, the results of the SVM and Random Forest ensembles were unsatisfying,
 and the active learning strategy did not show good results in comparison
 to random sampling.
\end_layout

\begin_layout Section
Active Learning on Texts with Fast Text Encoding Based Models
\end_layout

\begin_layout Standard
In this section we show results of all models because the Fast Text encoding
 maps the texts to a 
\begin_inset Formula $300$
\end_inset

-dimensional feature space.
 This is much lower than in the case of the TF-IDF encoding.
 Thus, we were able to provide computations with respect to all models.
\end_layout

\begin_layout Subsection
SVM Ensemble
\end_layout

\begin_layout Subsubsection
Sports and Comedy Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the SVM ensembles with
 the Fast Text encoding.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SVM_active_learning.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SVM_ensembles"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SVM ensembles algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The behavior which we observe in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is similar to figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:TF-IDF_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We can see that the active learning strategy is not working for the case
 of the Fast Text encoding as well.
 
\end_layout

\begin_layout Subsubsection
Conclusion
\end_layout

\begin_layout Standard
We can conclude that even despite good performance in the Passive Learning
 section, the active learning algorithm is not working as expected.
 Thus, we will not continue testing this algorithm on other datasets.
 
\end_layout

\begin_layout Subsection
Random Forest Ensembles
\end_layout

\begin_layout Subsubsection
Sports and Comedy Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_Random_Forest_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the Random Forest ensembles
 with the Fast Text encoding.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_Random_Forest_active_learning.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_Random_Forest_ensembles"

\end_inset

.Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the Random Forest ensembles algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As illustrated in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see the strategies results are similar to each other but in active
 learning case, the entropy based sampling is outperforming the random sampling
 strategy.
 We can also see that the uncertainty bounds are much more narrow in the
 active learning case .
 
\end_layout

\begin_layout Subsubsection
Conclusion
\end_layout

\begin_layout Standard
Even though we could observe that the active learning strategy achieves
 better results than the random sampling strategy, we do not continue with
 further experiments because the results are not significant enough.
\end_layout

\begin_layout Subsection
Neural Network Ensembles
\end_layout

\begin_layout Subsubsection
Sports and Comedy Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_Feed_Forward_NN_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for neural networks ensembles
 with the Fast Text encoding.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_Feed_Forward_NN_active_learning.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_Feed_Forward_NN_ensembles"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the Feed Forward Neural Network ensembles algorithm with Fast Text
 encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
If we compare output metrics from figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_Feed_Forward_NN_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SVM_ensembles"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see a significant difference in performance of the algorithms.
 In comparison to previous methods, the algorithm based on neural networks
 achieves the same AUC at 
\begin_inset Formula $50$
\end_inset

-th iteration.
 It means that the probability of using non-random acquisition function
 equals to 
\begin_inset Formula $50\%$
\end_inset

.
 We are able to observe significant improvement of the entropy based sampling
 over the random sampling strategy.
 We can also see, that the random sampling strategy is converging very slow
 to the results of the active learning strategy.
 Moreover, uncertainty bounds of the active learning strategy are much more
 narrow than the uncertainty bounds of the random sampling algorithm.
 
\end_layout

\begin_layout Subsubsection
Conclusion
\end_layout

\begin_layout Standard
One significant disadvantage of this method is that it is very slow and
 hard to train.
 Each time, we have to train 
\begin_inset Formula $10$
\end_inset

 neural networks with a cold start.
 That means training neural networks with random weights initialization
 and no prior information from the previous training.
 Despite the fact, that the results are really good, we decided not to continue
 with the algorithm testing because it is very slow and computationally
 costly.
 In the next sections we introduce results based on neural networks but
 with a different way of uncertainty representation.
\end_layout

\begin_layout Subsection
SGLD
\end_layout

\begin_layout Standard
Stochastic Gradient Langevin Dynamics algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "welling2011bayesian"
literal "false"

\end_inset

 is one of the two algorithms that are assumed to improve the ratio of training
 time consumption and performance.
 As mentioned previously, Gaussian noise is added to the gradient while
 training.
 This approach helps us to sample different parameters vectors around the
 loss minimum by simply continuing training.
 In comparison to neural networks ensembles, we do not have to train neural
 network ensembles separately, but train only one neural network with some
 additional training epochs.
 
\end_layout

\begin_layout Subsubsection
Sports and Comedy Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results for active learning for SGLD with the Fast
 Text encoding and Sport vs.
 Comedy categories dataset.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SGLD_NN_active_learning_Sports_Comedy.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SGLD_NN"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SGLD algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can see that in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN"
plural "false"
caps "false"
noprefix "false"

\end_inset

 both mean and uncertainty bound curves are not smooth enough in comparison
 to neural network ensembles.
 This behavior can be explained with a low number of samples from the SGLD
 training.
 However, we are able to observe the same analogy as with neural network
 ensembles.
 The active learning strategy improves over the random sampling and has
 narrower uncertainty bounds.
 Moreover, the SGLD was trained almost three times faster than the neural
 network ensembles algorithm.
 
\end_layout

\begin_layout Subsubsection
Crime and Good News Categories
\begin_inset CommandInset label
LatexCommand label
name "subsec:SGLD_Crime_and_Good_news_categories"

\end_inset


\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Crime_Good_news"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for SGLD with the Fast
 Text encoding and the Crime vs.
 Good News datasets.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SGLD_NN_active_learning_Crime_Good_news.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SGLD_NN_Crime_Good_news"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SGLD algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Crime vs.
 Good News categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We expect the Crime and Good News categories to have only a small intersection.
 As seen from 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Crime_Good_news"
plural "false"
caps "false"
noprefix "false"

\end_inset

 it is really true, because the first iteration of the simulation starts
 at around 
\begin_inset Formula $85\%-90\%$
\end_inset

 AUC.
 It means that only 
\begin_inset Formula $10$
\end_inset

 training data samples algorithm could reach high-performance results.
 The evolution of the two strategies is quite the same as in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We would like to pay attention to the place where the upper uncertainty
 bound reaches the value that is greater than one.
 It is not possible for the AUC to be greater than one because 
\begin_inset Formula $\mathrm{AUC}\in[0,1]$
\end_inset

.
 However, as mentioned previously, we construct uncertainty bounds as one
 standard deviation from the mean value.
 In this case, it is possible that the upper uncertainty bound is greater
 than one.
 Thus, while interpreting the results, a reader has to keep in mind that
 AUC metric must be truncated to one.
\end_layout

\begin_layout Subsubsection
Politics and Business Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Politics_Business"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results for active learning for SGLD with the Fast
 Text encoding and the Politics vs.
 Business datasets.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SGLD_NN_active_learning_Politics_Business.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SGLD_NN_Politics_Business"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SGLD algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Politics vs.
 Business categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
These two categories have a larger intersection, what means that it is a
 harder classification task.
 The results show again that the active learning strategy easily exceeds
 the random sampling strategy.
 Even though the topics are harder to distinguish, the active learning strategy
 finds the patterns in the data and shows much better results with narrower
 uncertainty bounds.
 
\end_layout

\begin_layout Subsubsection
Tech and Science Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Tech_Science"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results for active learning for SGLD with the Fast
 Text encoding and Tech vs.
 Science categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SGLD_NN_active_learning_Science_Tech.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SGLD_NN_Tech_Science"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SGLD algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Tech vs.
 Science categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We observe once again that the active learning strategy is better even though
 the categories are quite similar.
 As seen from the plot, the upper uncertainty bound reaches the values which
 is greater than one.
 This is exactly the case which was already covered in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:SGLD_Crime_and_Good_news_categories"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
College and Education Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_College_Education"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for SGLD, with the Fast
 Text encoding and the College vs.
 Education categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SGLD_NN_active_learning_College_Education.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SGLD_NN_College_Education"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SGLD algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the College vs.
 Education categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The collected results based on the College and Education category do not
 differ from the results from the previous experiment with the SGLD algorithm.
 We can still see that active learning strategy outperforms the random sampling,
 even despite the fact that these categories are very similar.
 
\end_layout

\begin_layout Subsubsection
Positive and Negative Tweets Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Tweets"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for SGLD with the Fast
 Text encoding and the Positive vs.
 Negative tweets categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_SGLD_NN_active_learning_Tweets_categories.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_SGLD_NN_Tweets"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the SGLD algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Positive vs.
 Negative tweet categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In previous sections, we tested the SGLD algorithm on the data from the
 HuffPost dataset.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Tweets"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are illustrated results with respect to the tweets dataset.
 Due to the fact that the tweets are really short, it is quite hard to find
 the patterns that can be used for high performance separation of categories.
 Thus, we can see that our classification results (AUC) are not as high
 as they were in previous datasets.
 Moreover, we observed that for this dataset there is no difference between
 active learning and random sampling strategy.
 
\end_layout

\begin_layout Subsubsection
Conclusion
\end_layout

\begin_layout Standard
To sum up, we can say that the SGLD algorithm exhibit really impressive
 results and proved that the active learning strategy outperforms the random
 sampling.
 However, there are several disadvantages.
 The first disadvantage is that the resulted curves were not smooth.
 We think that this problem can be eliminated by sampling more parameters
 vectors while training the model.
 Another problem is that the algorithm does not show better active learning
 results when it is tested on short and quite general text data.
 We believe that this problem can be solved with a different encoding approach.
\end_layout

\begin_layout Subsection
Deep Ensemble Filter 
\end_layout

\begin_layout Standard
Deep Ensemble Filter (DENFI) algorithm is the second algorithm which is
 not partitioning the training dataset.
 In addition to this, we use a modification of the DENFI algorithm.
 A huge advantage of this algorithm is that it uses prior information from
 the previous training round.
 In the first training round, the DENFI algorithm follows almost the same
 strategy as the neural network ensembles method.
 It trains 
\begin_inset Formula $10$
\end_inset

 ensembles with respect to all training data.
 The variability in ensembles is reached with different weights initialization.
 After the training is finished, we add some Gaussian noise in order to
 increase the variability.
 When we add some training samples, we continue training using the weights
 from the previous iteration with the addition of Gaussian noise in the
 end.
 Moreover, we are using a lower number of epochs.
 The hot start approach gives sufficient variability that is manifested
 in the quality of results in the subsequent sections.
 
\end_layout

\begin_layout Subsubsection
Sports and Comedy Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_SPORTS_COMEDY"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for DENFI with the Fast
 Text encoding and the Sport vs.
 Comedy categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_DENFI_NN_active_learning_Sports_Comedy.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_DENFI_NN_SPORTS_COMEDY"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the DENFI algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Sports vs.
 Comedy categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The active learning result, displayed in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_SPORTS_COMEDY"
plural "false"
caps "false"
noprefix "false"

\end_inset

, outperforms all the algorithms that were tested before.
 We observe that the uncertainty bounds are extremely narrow.
 In addition to this, we see that the active learning strategy has much
 higher AUC metrics and the curves are quite smooth.
 Moreover, due to the hot start training, the time needed to fit the algorithm
 is much lower in comparison to neural network ensembles.
 
\end_layout

\begin_layout Subsubsection
Crime and Good News Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_Crime_Good_news"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the DENFI with Fast
 Text encoding and the Crime vs.
 Good News categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_DENFI_NN_active_learning_Crime_Good_news.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_DENFI_NN_Crime_Good_news"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the DENFI algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Crime vs.
 Good News categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We have already discussed in the SGLD section that the Crime and Good news
 categories can be well separated from each other.
 In this case, we see the same behavior for active learning strategy as
 in the previous section.
 The uncertainty bounds are narrow and the AUC scores are very high in compariso
n to the random sampling strategy.
\end_layout

\begin_layout Subsubsection
Politics and Business Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_Politics_Business"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the DENFI with Fast
 Text encoding and the Politics vs.
 Business categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_DENFI_NN_active_learning_Politics_Business.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_DENFI_NN_Politics_Business"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the DENFI algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Politics vs.
 Business categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The behavior, observed in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_Politics_Business"
plural "false"
caps "false"
noprefix "false"

\end_inset

, is the same as in the previous DENFI results.
 Moreover, we see good active learning performance, even though it is not
 easy to separate these categories.
 If we compare the results in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_Politics_Business"
plural "false"
caps "false"
noprefix "false"

\end_inset

 to the SGLD results in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Politics_Business"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see the DENFI is better not only because its less corrupted with
 noise but also in terms of higher AUC scores.
 
\end_layout

\begin_layout Subsubsection
Tech and Science Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_Tech_Science"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the DENFI with Fast
 Text encoding and the Tech vs.
 Science categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_DENFI_NN_active_learning_Science_Tech.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_DENFI_NN_Tech_Science"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the DENFI algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Tech vs.
 Science categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The active learning strategy results for the DENFI algorithm are much better
 in comparison to the SGLD method that are shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_SGLD_NN_Tech_Science"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We see that for the active learning strategy, the DENFI easily found the
 patterns which helped it to learn faster and show better scores with narrower
 uncertainty bounds.
 
\end_layout

\begin_layout Subsubsection
College and Education Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_College_Education"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the DENFI with Fast
 Text encoding and the College vs.
 Education categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_DENFI_NN_active_learning_College_Education.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_DENFI_NN_College_Education"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the DENFI algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the College vs.
 Education categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Last but not least of the Huffpost dataset, results for the College and
 Education categories also prove that success of the DENFI active learning
 strategy is not dependent on size of the intersection between the categories.
 The uncertainty bounds for the active learning strategy are still narrow
 and the AUC score is much higher than for the random sampling case.
\end_layout

\begin_layout Subsubsection
Positive and Negative Tweets Categories
\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Fast_Text_DENFI_NN_Tweets"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are shown simulation results of active learning for the DENFI with Fast
 Text encoding and the Positive vs.
 Negative tweets categories.
 The active learning strategy is based on the entropy acquisition function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Active Learning/FastText/Fast_Text_DENFI_NN_active_learning_Tweets_categories.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Fast_Text_DENFI_NN_Tweets"

\end_inset

Comparison of the random and active learning strategies for semi-supervised
 learning in terms of the mean and one standard deviation of 
\begin_inset Formula $10$
\end_inset

 runs of the DENFI algorithm with Fast Text encoding.
 The initial training set contains 
\begin_inset Formula $10$
\end_inset

 samples from the Positive vs.
 Negative tweets categories 
\begin_inset Formula $200$
\end_inset

 queries for data annotation were performed
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In comparison to SGLD, the DENFI algorithm shows better active learning
 performance for the tweets dataset.
 The DENFI active learning strategy is also working on tweets whereas the
 SGLD active learning strategy was not able to overcome the random sampling.
 We can also see that the DENFI AUC scores for active learning are higher
 with narrower uncertainty bounds than those for thes SGLD.
\end_layout

\begin_layout Subsubsection
Conclusion
\end_layout

\begin_layout Standard
To conclude we can say that the DENFI algorithm achieved by far the best
 results for all problems which it was tested on.
 In all cases, the active learning strategy outperformed the random sampling.
 The algorithm was tested both on the data that are easy and hard to separate.
 In addition to this, the AUC scores for all active learning results with
 the DENFI were higher than for all other algorithms.
 That makes the DENFI algorithm the best one, with the highest performance
 and the lowest uncertainty bounds, from all approaches tested within this
 project.
\end_layout

\begin_layout Chapter*
Conclusion
\end_layout

\begin_layout Standard
This project demonstrate how an active learning strategy of querying unlabeled
 text documents for further labeling and training can beat the random selection
 strategy.
 We have provided not only a high-level theoretical description of the problem
 but also testing results that cover different scenarios and text document
 categories.
 Github link for Python implementation is also available in this project.
 
\end_layout

\begin_layout Standard
Based on the achieved results that were gathered from testing on 12 different
 categories, we were able to see that a modification of the DENFI algorithm
 shows excellent performance and overcomes other algorithms in all aspects.
 The DENFI algorithm outperforms all other models, both in higher AUC scores
 and narrower uncertainty bounds.
 Another considerable advantage of DENFI is that it is the fastest neural
 network model that was implemented and tested in this thesis.
\end_layout

\begin_layout Standard
We see a plenty of further opportunities how to improve the algorithm, starting
 from a better text representation and ending using other modification of
 the DENFI algorithm.
 To conclude, we would like to say that the ensembles showed their power
 in solving an active learning problem, and in our opinion, it is a good
 field for continuing the research.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
