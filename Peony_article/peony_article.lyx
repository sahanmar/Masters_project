#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #718c00
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Articles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Ensembles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
 (Balaji Lakshminarayanan Alexander Pritzel Charles Blundell) - Nice approach
 of adversarial component in training ensembles.
 They showed how their approach of ensembles outperforms dropout technique.
 The results were shown on images MNIST, SVHN and ImageNet and toy problems.
\end_layout

\begin_layout Plain Layout
2.
 Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty
 Under Dataset Shift (Yaniv Ovadia Balaji Lakshminarayanan Sebastian Nowozin)
 - Different methods for uncertainty representation are compared between
 each other.
 All these methods were testes on text data and image processing data.
 It turned out that ensembles showed the best performace with repect to
 all other methods.
 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Active Learning
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Active Anomaly Detection via Ensembles (Shubhomoy Das, Md Rakibul Islam,
 Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa) - Anomaly detection
 use case.
 They showed how it is possible to apply the weights for each ensemble where
 the weights are based on a feedback from an annotator.
 Nice approach of active learning on rescaling weights of each ensemble.
\end_layout

\begin_layout Plain Layout
2.
 Deep Bayesian Active Learning with Image Data (Yarin Gal, Riashat Islam,
 Zoubin Ghahramani ) - Active learning classification on images with respect
 to different acquisition functions.
 Good examples of which acquisition function can be used.
\end_layout

\begin_layout Plain Layout
3.
 ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH
 (Ozan Sener, Silvio Savarese) - Core set approach.
 They try to cover training dataset with some multidimentional spheres in
 ordet to find the best learning subset that covers as much dataset as possible.
 Tested on Image Recognition and convolutional neural networks.
 
\end_layout

\begin_layout Plain Layout
4.
 Learning Loss for Active Learning (Donggeun Yoo, In So Kweon) - Another
 approach with learning and predicting loss function.
 They do tell that they dont compare their method because for really large
 datasets dropout technique in too much computationally costly.
 Their algorithm works nicely for 1-10k image datasets.
 
\end_layout

\begin_layout Plain Layout
5.
 Bayesian learning via stochastic gradient Langevin dynamics (Welling, Max
 and Teh, Yee W) - Another uncertainty representation for neural networks.
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout

\series bold
Active Learning with texts
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Semi-Supervised Bayesian Active Learning for Text Classification (Sophie
 Burkhardt, Julia Siekiera, Stefan Kramer) - They are doing exactly the
 same thing that I do but with the usage of drop out based approach and
 ayes-by-Backprop (BBB) algorithm.
 The results are not cool at all.
 Mine are better
\end_layout

\begin_layout Plain Layout
2.
 Practical Obstacles to Deploying Active Learning (David Lowell, Zachary
 C.
 Lipton, Byron C.
 Wallace) - Nice try with active learning.
 Very poor results.
 They used LSTM, SVM and CNN for active learning.
 They also used dropout in their work.
\end_layout

\begin_layout Plain Layout
3.
 Deep active learning for named entity recognition (Yanyao Shen, Hyokun
 Yun, Zachary C.
 Lipton, Yakov Kronrod, Animashree Anandkumar) - Dropout based active learning.
 They are also trying to reduce amount of the training data for NER models.
 They also consider sampling according to the measure of uncertainty proposed
 by Gal et al.
 (2017).
\end_layout

\begin_layout Plain Layout
4.
 Support Vector Machine Active Learning with Applications to Text Classification
 (Simon Tong, Daphne Koller) - The approach of active learning method for
 text classification that comes from 2001.
 They go through three techniques that show different queuing strategy.
 The results are better than in case of random sampling.
 However, no uncertainty was measured there.
 (Non bayesian way of querying).
\end_layout

\begin_layout Plain Layout
5.
 Deep Active Learning for Text Classification (Bang An, Wenjun Wu, Huimin
 Han) - SVM and RNN (LSTM) multiclass text classification.
 No bayesian approach of neural networks.
 Trying to sample not 1 sample but batch.
 Their approach is only based on acquisition function.
 The uncertainty is measured only through output labels based on one set
 of parameters (point-wise estimate) 
\end_layout

\begin_layout Plain Layout
5.
 DEEP ACTIVE LEARNING FOR NAMED ENTITY RECOGNITION (Kashyap Chitta, Jose
 M.
 Alvarez, Adam Lesnikowski) - Active learning for NER.
 Same task as ours.
 They compare different models for example BALD, LC and so on.
 The active learning results are almost same.
 No significant difference seen there.
 
\end_layout

\begin_layout Plain Layout
6.
 (NON-RELEVANT) Active Deep Networks for Semi-Supervised Sentiment Classificatio
n (Shusen Zhou, Qingcai Chen and Xiaolong Wang) - Very poor approach without
 a comparison to random selection.
 They represent an uncertainty as a min distance from a decision boundary.
 Nothing special about the article.
 Year 2010 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Techniques 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Shannon, Claude Elwood.
 A mathematical theory of com- munication.
 Bell System Technical Journal, 27(3):379– 423, 1948.
 - Citation to entropy
\end_layout

\begin_layout Plain Layout
2.
 Advances in Pre-Training Distributed Word Representations (Tomas Mikolov,
 Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin) - FastText
 pretrained models
\end_layout

\begin_layout Plain Layout
3.
 Efficient Estimation of Word Representations in Vector Space (Tomas Mikolov,
 Kai Chen, Greg Corrado, Jeffrey Dean) - CBOW (ancestor of FastText)
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Active learning for texts - why ()
\end_layout

\begin_layout Standard
Classical approaches - SVM, etc.
 ()
\end_layout

\begin_layout Standard
Deep networks & embeddings ()
\end_layout

\begin_layout Standard
Uncertainty in deep networks (SGLD, Dropout, DEnFi)
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Subsection
Embedding
\end_layout

\begin_layout Subsection
Bayesian neural networks
\end_layout

\begin_layout Subsection
Active Learning
\end_layout

\begin_layout Standard
Acquisition function...
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Advantage of Neural networks
\end_layout

\begin_layout Standard
NN much better than SVM (bag-of-words)
\end_layout

\begin_layout Subsection
Impact of uncertainty represantation
\end_layout

\begin_layout Description
Dropout (hot, cold start)
\end_layout

\begin_layout Description
Denfi (variance sensitivity)
\end_layout

\begin_layout Section
Conclusion
\end_layout

\end_body
\end_document
